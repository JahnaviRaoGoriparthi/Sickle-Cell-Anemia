{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3e08a84-0b73-4ea1-9097-457553a3bc41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Proportions:\n",
      "Negatives: 1000 Positives: 1000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import imgaug.augmenters as iaa\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define constants\n",
    "data_path = \"/Users/yanthraa/Desktop/PROJECT/NEW DATASET/archive\"\n",
    "target_size = (224, 224)\n",
    "target_count = 1000  # Desired number of images per class\n",
    "output_path = \"/Users/yanthraa/Desktop/PROJECT/NEW DATASET/Augmented_Dataset\"  # Output directory\n",
    "\n",
    "# Ensure output directories exist\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "for class_name in ['Negatives', 'Positives']:\n",
    "    os.makedirs(os.path.join(output_path, class_name), exist_ok=True)\n",
    "\n",
    "# Define augmentation sequence\n",
    "augmenters = iaa.Sequential([\n",
    "    iaa.Fliplr(0.5),  # Horizontal flip\n",
    "    iaa.Affine(rotate=(-15, 15)),  # Rotate between -15 and 15 degrees\n",
    "    iaa.GaussianBlur(sigma=(0.0, 1.0)),  # Apply Gaussian blur\n",
    "    iaa.AdditiveGaussianNoise(scale=(0, 0.05*255)),  # Add noise\n",
    "    iaa.Multiply((0.8, 1.2))  # Brightness adjustments\n",
    "])\n",
    "\n",
    "# Load and preprocess data\n",
    "images = []\n",
    "labels = []\n",
    "\n",
    "for class_name in ['Negatives', 'Positives']:\n",
    "    class_path = os.path.join(data_path, class_name)\n",
    "    class_images = []\n",
    "    label = 0 if class_name == 'Negatives' else 1\n",
    "    \n",
    "    # Load original images\n",
    "    for image_name in os.listdir(class_path):\n",
    "        image_path = os.path.join(class_path, image_name)\n",
    "        img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "        img = cv2.resize(img, target_size)\n",
    "        img = img.astype(np.float32) / 255.0\n",
    "        class_images.append(img)\n",
    "    \n",
    "    # Augment images until target count is reached\n",
    "    while len(class_images) < target_count:\n",
    "        augmented_images = augmenters(images=class_images[:min(len(class_images), target_count - len(class_images))])\n",
    "        class_images.extend(augmented_images)\n",
    "    \n",
    "    class_images = class_images[:target_count]  # Trim excess\n",
    "    images.extend(class_images)\n",
    "    labels.extend([label] * target_count)\n",
    "    \n",
    "    # Save augmented images\n",
    "    for idx, img in enumerate(class_images):\n",
    "        save_path = os.path.join(output_path, class_name, f\"aug_{idx}.png\")\n",
    "        cv2.imwrite(save_path, (img * 255).astype(np.uint8))\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "images = np.array(images)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Print class proportions\n",
    "print(\"Class Proportions:\")\n",
    "print(\"Negatives:\", np.sum(labels == 0), \"Positives:\", np.sum(labels == 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e10a5879-c1b2-43a6-bc97-8b99c4642c6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced Images: (1000, 224, 224)\n",
      "Balanced labels: (1000,)\n",
      "Negatives_images: (500, 224, 224)\n",
      "Positives_images: (500, 224, 224)\n",
      "X_train shape: (720, 224, 224)\n",
      "X_test shape: (200, 224, 224)\n",
      "y_train shape: (720,)\n",
      "y_test shape: (200,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Separate data for Negatives and Positives classes\n",
    "Negatives_images = images[labels == 0][:500]\n",
    "Positives_images = images[labels == 1][:500]\n",
    "Negatives_labels = labels[labels == 0][:500]\n",
    "Positives_labels = labels[labels == 1][:500]\n",
    "\n",
    "# Concatenate the data back together\n",
    "balanced_images = np.concatenate([Negatives_images, Positives_images])\n",
    "balanced_labels = np.concatenate([Negatives_labels, Positives_labels])\n",
    "\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(balanced_images, balanced_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Split the training data into training and validation sets\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full, test_size=0.1, random_state=42)\n",
    "\n",
    "#print the balanced data\n",
    "print(\"Balanced Images:\",balanced_images.shape)\n",
    "print(\"Balanced labels:\",balanced_labels.shape)\n",
    "\n",
    "#print the data seperately of each class\n",
    "print(\"Negatives_images:\",Negatives_images.shape)\n",
    "print(\"Positives_images:\",Positives_images.shape)\n",
    "\n",
    "\n",
    "# Print the shapes of the training and testing sets\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edddc453-e8cf-4706-a2e2-5c424825d88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "# from tensorflow.keras.optimizers import Optimizer\n",
    "# from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "# from sklearn.model_selection import train_test_split, KFold\n",
    "# from sklearn.metrics import (confusion_matrix, classification_report, roc_curve, \n",
    "#                            roc_auc_score, precision_recall_curve, auc, precision_score, \n",
    "#                            recall_score, f1_score, cohen_kappa_score, matthews_corrcoef)\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# import time\n",
    "# import os\n",
    "\n",
    "# # Create directory for results if it doesn't exist\n",
    "# os.makedirs('results', exist_ok=True)\n",
    "\n",
    "# # Custom Optimizers\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.optimizers import Optimizer\n",
    "# import tensorflow.keras.backend as K\n",
    "# class HGSOptimizer(tf.keras.optimizers.Optimizer):\n",
    "#     def __init__(self, \n",
    "#                  learning_rate=0.001,\n",
    "#                  population_size=30,\n",
    "#                  hunger_rate=0.1,\n",
    "#                  name=\"HGSOptimizer\",\n",
    "#                  **kwargs):\n",
    "#         super().__init__(name=name, learning_rate=learning_rate, **kwargs)\n",
    "#         self._population_size = population_size\n",
    "#         self._hunger_rate = hunger_rate\n",
    "    \n",
    "#     def update_step(self, gradient, variable, learning_rate):\n",
    "#         \"\"\"Update step for variables.\"\"\"\n",
    "#         hunger_factor = tf.random.uniform([], dtype=variable.dtype) * self._hunger_rate\n",
    "#         survival_factor = 1.0 - hunger_factor\n",
    "        \n",
    "#         # Apply the update\n",
    "#         variable.assign_sub(learning_rate * gradient * survival_factor)\n",
    "\n",
    "# class MPAOptimizer(tf.keras.optimizers.Optimizer):\n",
    "#     def __init__(self,\n",
    "#                  learning_rate=0.001,\n",
    "#                  elite_factor=0.2,\n",
    "#                  prey_factor=0.1,\n",
    "#                  name=\"MPAOptimizer\",\n",
    "#                  **kwargs):\n",
    "#         super().__init__(name=name, learning_rate=learning_rate, **kwargs)\n",
    "#         self._elite_factor = elite_factor\n",
    "#         self._prey_factor = prey_factor\n",
    "    \n",
    "#     def update_step(self, gradient, variable, learning_rate):\n",
    "#         \"\"\"Update step for variables.\"\"\"\n",
    "#         elite_influence = tf.random.uniform([], dtype=variable.dtype) * self._elite_factor\n",
    "#         prey_influence = tf.random.uniform([], dtype=variable.dtype) * self._prey_factor\n",
    "        \n",
    "#         # Apply the update\n",
    "#         variable.assign_sub(learning_rate * gradient * (1 + elite_influence - prey_influence))\n",
    "\n",
    "# class PathwaysOptimizer(tf.keras.optimizers.Optimizer):\n",
    "#     def __init__(self,\n",
    "#                  learning_rate=0.001,\n",
    "#                  pathway_strength=0.3,\n",
    "#                  adaptation_rate=0.1,\n",
    "#                  name=\"PathwaysOptimizer\",\n",
    "#                  **kwargs):\n",
    "#         super().__init__(name=name, learning_rate=learning_rate, **kwargs)\n",
    "#         self._pathway_strength = pathway_strength\n",
    "#         self._adaptation_rate = adaptation_rate\n",
    "    \n",
    "#     def update_step(self, gradient, variable, learning_rate):\n",
    "#         \"\"\"Update step for variables.\"\"\"\n",
    "#         pathway_factor = tf.random.uniform([], dtype=variable.dtype) * self._pathway_strength\n",
    "#         adapt_factor = tf.random.uniform([], dtype=variable.dtype) * self._adaptation_rate\n",
    "        \n",
    "#         # Apply the update\n",
    "#         variable.assign_sub(learning_rate * gradient * (1 + pathway_factor * adapt_factor))\n",
    "\n",
    "#     def get_config(self):\n",
    "#         config = super().get_config()\n",
    "#         config.update({\n",
    "#             \"pathway_strength\": self._pathway_strength,\n",
    "#             \"adaptation_rate\": self._adaptation_rate\n",
    "#         })\n",
    "#         return config\n",
    "\n",
    "# def create_model(optimizer, input_shape=(224, 224, 3)):\n",
    "#     model = Sequential([\n",
    "#         Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=input_shape),\n",
    "#         MaxPooling2D((2, 2)),\n",
    "#         Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "#         MaxPooling2D((2, 2)),\n",
    "#         Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "#         MaxPooling2D((2, 2)),\n",
    "#         Flatten(),\n",
    "#         Dense(128, activation='relu'),\n",
    "#         Dropout(0.5),\n",
    "#         Dense(1, activation='sigmoid')\n",
    "#     ])\n",
    "    \n",
    "#     model.compile(\n",
    "#         optimizer=optimizer,\n",
    "#         loss='binary_crossentropy',\n",
    "#         metrics=['accuracy', \n",
    "#                 tf.keras.metrics.Precision(),\n",
    "#                 tf.keras.metrics.Recall(),\n",
    "#                 tf.keras.metrics.AUC()]\n",
    "#     )\n",
    "#     return model\n",
    "\n",
    "# def save_metrics_to_csv(metrics_dict, optimizer_name, fold, k):\n",
    "#     \"\"\"Save metrics for a single run to CSV\"\"\"\n",
    "#     df = pd.DataFrame({\n",
    "#         'Test Accuracy': [metrics_dict['Test Accuracy']],\n",
    "#         'Train Accuracy': [metrics_dict['Train Accuracy']],\n",
    "#         'Precision': [metrics_dict['Precision']],\n",
    "#         'Recall': [metrics_dict['Recall']],\n",
    "#         'AUC-ROC': [metrics_dict['AUC-ROC']],\n",
    "#         'AUC-PR': [metrics_dict['AUC-PR']],\n",
    "#         'TN': [metrics_dict['TN']],\n",
    "#         'FP': [metrics_dict['FP']],\n",
    "#         'FN': [metrics_dict['FN']],\n",
    "#         'TP': [metrics_dict['TP']],\n",
    "#         'F1 Score': [metrics_dict['F1 Score']],\n",
    "#         'Cohen\\'s Kappa Coefficient': [metrics_dict['Cohen\\'s Kappa']],\n",
    "#         'Matthews Correlation Coefficient': [metrics_dict['Matthews Correlation Coefficient']],\n",
    "#         'Training Loss': [metrics_dict['Training Loss']],\n",
    "#         'Testing Loss': [metrics_dict['Testing Loss']],\n",
    "#         'Time Taken (seconds)': [metrics_dict['Time Taken (seconds)']],\n",
    "#         'Fold': [fold],\n",
    "#         'K': [k]\n",
    "#     })\n",
    "    \n",
    "#     filename = f'results/{optimizer_name}_metrics_K{k}_Fold{fold}.csv'\n",
    "#     df.to_csv(filename, index=False)\n",
    "#     return filename\n",
    "\n",
    "# def plot_training_curves(history, optimizer_name, k, fold):\n",
    "#     \"\"\"Plot and save training curves\"\"\"\n",
    "#     plt.figure(figsize=(12, 4))\n",
    "    \n",
    "#     # Accuracy plot\n",
    "#     plt.subplot(1, 2, 1)\n",
    "#     plt.plot(history.history['accuracy'], label='train')\n",
    "#     plt.plot(history.history['val_accuracy'], label='validation')\n",
    "#     plt.title(f'{optimizer_name} - Accuracy (K={k}, Fold={fold})')\n",
    "#     plt.xlabel('Epoch')\n",
    "#     plt.ylabel('Accuracy')\n",
    "#     plt.legend()\n",
    "    \n",
    "#     # Loss plot\n",
    "#     plt.subplot(1, 2, 2)\n",
    "#     plt.plot(history.history['loss'], label='train')\n",
    "#     plt.plot(history.history['val_loss'], label='validation')\n",
    "#     plt.title(f'{optimizer_name} - Loss (K={k}, Fold={fold})')\n",
    "#     plt.xlabel('Epoch')\n",
    "#     plt.ylabel('Loss')\n",
    "#     plt.legend()\n",
    "    \n",
    "#     plt.tight_layout()\n",
    "#     plt.savefig(f'results/{optimizer_name}_training_curves_K{k}_Fold{fold}.png')\n",
    "#     plt.close()\n",
    "\n",
    "# def plot_comparative_metrics(all_summary_dfs):\n",
    "#     \"\"\"Create comparative visualizations for all optimizers\"\"\"\n",
    "#     plt.figure(figsize=(15, 10))\n",
    "    \n",
    "#     metrics_to_plot = ['Test Accuracy', 'F1 Score', 'AUC-ROC', 'Training Loss']\n",
    "#     for i, metric in enumerate(metrics_to_plot, 1):\n",
    "#         plt.subplot(2, 2, i)\n",
    "#         data = pd.concat(all_summary_dfs)\n",
    "#         sns.boxplot(data=data, x='K', y=metric, hue='Optimizer')\n",
    "#         plt.title(f'{metric} by K-Fold and Optimizer')\n",
    "#         plt.xticks(rotation=45)\n",
    "    \n",
    "#     plt.tight_layout()\n",
    "#     plt.savefig('results/comparative_metrics.png')\n",
    "#     plt.close()\n",
    "\n",
    "\n",
    "# def preprocess_data(X_train, X_test, y_train, y_test):\n",
    "#     \"\"\"Preprocess the data to ensure correct shapes and normalization\"\"\"\n",
    "#     # Add channel dimension if not present\n",
    "#     if len(X_train.shape) == 3:\n",
    "#         X_train = X_train[..., np.newaxis]\n",
    "#     if len(X_test.shape) == 3:\n",
    "#         X_test = X_test[..., np.newaxis]\n",
    "    \n",
    "#     # Convert to RGB by repeating the channel 3 times\n",
    "#     X_train = np.repeat(X_train, 3, axis=-1)\n",
    "#     X_test = np.repeat(X_test, 3, axis=-1)\n",
    "    \n",
    "#     # Normalize pixel values\n",
    "#     X_train = X_train.astype('float32') / 255.0\n",
    "#     X_test = X_test.astype('float32') / 255.0\n",
    "    \n",
    "#     # Convert labels to numpy arrays if they aren't already\n",
    "#     y_train = np.array(y_train)\n",
    "#     y_test = np.array(y_test)\n",
    "    \n",
    "#     print(\"Preprocessed shapes:\")\n",
    "#     print(f\"X_train: {X_train.shape}\")\n",
    "#     print(f\"X_test: {X_test.shape}\")\n",
    "#     print(f\"y_train: {y_train.shape}\")\n",
    "#     print(f\"y_test: {y_test.shape}\")\n",
    "    \n",
    "#     return X_train, X_test, y_train, y_test\n",
    "\n",
    "# def create_model(optimizer, input_shape):\n",
    "#     \"\"\"Create model with specified input shape\"\"\"\n",
    "#     model = Sequential([\n",
    "#         Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=input_shape),\n",
    "#         MaxPooling2D((2, 2)),\n",
    "#         Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "#         MaxPooling2D((2, 2)),\n",
    "#         Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "#         MaxPooling2D((2, 2)),\n",
    "#         Flatten(),\n",
    "#         Dense(128, activation='relu'),\n",
    "#         Dropout(0.5),\n",
    "#         Dense(1, activation='sigmoid')\n",
    "#     ])\n",
    "    \n",
    "#     model.compile(\n",
    "#         optimizer=optimizer,\n",
    "#         loss='binary_crossentropy',\n",
    "#         metrics=['accuracy', \n",
    "#                 tf.keras.metrics.Precision(),\n",
    "#                 tf.keras.metrics.Recall(),\n",
    "#                 tf.keras.metrics.AUC()]\n",
    "#     )\n",
    "#     return model\n",
    "# def train_and_evaluate_single_optimizer(X_train, y_train, X_test, y_test, optimizer_name, k_folds=[5, 10, 15, 20]):\n",
    "#     \"\"\"Train and evaluate a single optimizer with detailed metrics saving\"\"\"\n",
    "    \n",
    "#     all_files = []\n",
    "#     summary_metrics = []\n",
    "    \n",
    "#     # Ensure input shape is correct\n",
    "#     input_shape = X_train.shape[1:]  # Should be (224, 224, 3)\n",
    "#     print(f\"Model input shape: {input_shape}\")\n",
    "    \n",
    "#     for k in k_folds:\n",
    "#         print(f'\\nTraining with k={k} folds')\n",
    "#         kfold = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "        \n",
    "#         for fold, (train_ids, val_ids) in enumerate(kfold.split(X_train)):\n",
    "#             print(f'Training fold {fold + 1}/{k}')\n",
    "            \n",
    "#             try:\n",
    "#                 # Split data\n",
    "#                 X_train_fold = X_train[train_ids]\n",
    "#                 y_train_fold = y_train[train_ids]\n",
    "#                 X_val_fold = X_train[val_ids]\n",
    "#                 y_val_fold = y_train[val_ids]\n",
    "                \n",
    "#                 print(f\"Fold data shapes:\")\n",
    "#                 print(f\"X_train_fold: {X_train_fold.shape}\")\n",
    "#                 print(f\"y_train_fold: {y_train_fold.shape}\")\n",
    "#                 print(f\"X_val_fold: {X_val_fold.shape}\")\n",
    "#                 print(f\"y_val_fold: {y_val_fold.shape}\")\n",
    "                \n",
    "#                 # Create optimizer\n",
    "#                 if optimizer_name == \"HGS\":\n",
    "#                     optimizer = HGSOptimizer(learning_rate=0.001, hunger_rate=0.1)\n",
    "#                 elif optimizer_name == \"MPA\":\n",
    "#                     optimizer = MPAOptimizer(learning_rate=0.001, elite_factor=0.2)\n",
    "#                 else:  # Pathways\n",
    "#                     optimizer = PathwaysOptimizer(learning_rate=0.001, pathway_strength=0.3)\n",
    "                \n",
    "#                 # Create model with correct input shape\n",
    "#                 model = create_model(optimizer, input_shape)\n",
    "                \n",
    "#                 early_stopping = EarlyStopping(\n",
    "#                     monitor='val_loss',\n",
    "#                     patience=5,\n",
    "#                     restore_best_weights=True\n",
    "#                 )\n",
    "                \n",
    "#                 checkpoint = ModelCheckpoint(\n",
    "#                     f'results/{optimizer_name}_model_K{k}_Fold{fold}.h5',\n",
    "#                     monitor='val_loss',\n",
    "#                     save_best_only=True\n",
    "#                 )\n",
    "                \n",
    "#                 # Start timing\n",
    "#                 start_time = time.time()\n",
    "                \n",
    "#                 # Train the model\n",
    "#                 history = model.fit(\n",
    "#                     X_train_fold, y_train_fold,\n",
    "#                     epochs=50,\n",
    "#                     batch_size=32,\n",
    "#                     validation_data=(X_val_fold, y_val_fold),\n",
    "#                     callbacks=[early_stopping, checkpoint],\n",
    "#                     verbose=1\n",
    "#                 )\n",
    "                \n",
    "#                 # End timing\n",
    "#                 time_taken = time.time() - start_time\n",
    "                \n",
    "#                 # Evaluate model\n",
    "#                 test_results = model.evaluate(X_test, y_test, verbose=0)\n",
    "#                 y_pred = model.predict(X_test)\n",
    "#                 y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "                \n",
    "#                 # Calculate confusion matrix metrics\n",
    "#                 tn, fp, fn, tp = confusion_matrix(y_test, y_pred_binary).ravel()\n",
    "                \n",
    "#                 # Calculate precision-recall curve for AUC-PR\n",
    "#                 precision, recall, _ = precision_recall_curve(y_test, y_pred)\n",
    "#                 pr_auc = auc(recall, precision)\n",
    "                \n",
    "#                 # Create metrics dictionary\n",
    "#                 metrics = {\n",
    "#                     'Test Accuracy': test_results[1],\n",
    "#                     'Train Accuracy': history.history['accuracy'][-1],\n",
    "#                     'Precision': precision_score(y_test, y_pred_binary),\n",
    "#                     'Recall': recall_score(y_test, y_pred_binary),\n",
    "#                     'AUC-ROC': roc_auc_score(y_test, y_pred),\n",
    "#                     'AUC-PR': pr_auc,\n",
    "#                     'TN': tn,\n",
    "#                     'FP': fp,\n",
    "#                     'FN': fn,\n",
    "#                     'TP': tp,\n",
    "#                     'F1 Score': f1_score(y_test, y_pred_binary),\n",
    "#                     'Cohen\\'s Kappa': cohen_kappa_score(y_test, y_pred_binary),\n",
    "#                     'Matthews Correlation Coefficient': matthews_corrcoef(y_test, y_pred_binary),\n",
    "#                     'Training Loss': history.history['loss'][-1],\n",
    "#                     'Testing Loss': test_results[0],\n",
    "#                     'Time Taken (seconds)': time_taken\n",
    "#                 }\n",
    "                \n",
    "#                 # Save metrics to CSV\n",
    "#                 csv_file = save_metrics_to_csv(metrics, optimizer_name, fold + 1, k)\n",
    "#                 all_files.append(csv_file)\n",
    "                \n",
    "#                 # Plot and save training curves\n",
    "#                 plot_training_curves(history, optimizer_name, k, fold + 1)\n",
    "                \n",
    "#                 # Store summary metrics\n",
    "#                 summary_metrics.append({\n",
    "#                     'Optimizer': optimizer_name,\n",
    "#                     'K': k,\n",
    "#                     'Fold': fold + 1,\n",
    "#                     **metrics\n",
    "#                 })\n",
    "                \n",
    "#                 print(f\"\\nFold {fold + 1} Results:\")\n",
    "#                 print(f\"Test Accuracy: {metrics['Test Accuracy']:.4f}\")\n",
    "#                 print(f\"F1 Score: {metrics['F1 Score']:.4f}\")\n",
    "#                 print(f\"AUC-ROC: {metrics['AUC-ROC']:.4f}\")\n",
    "#                 print(f\"Training Time: {metrics['Time Taken (seconds)']:.2f} seconds\")\n",
    "                \n",
    "#             except Exception as e:\n",
    "#                 print(f\"Error in fold {fold + 1}: {str(e)}\")\n",
    "#                 continue\n",
    "    \n",
    "#     # Create and save summary DataFrame for this optimizer\n",
    "#     if summary_metrics:\n",
    "#         summary_df = pd.DataFrame(summary_metrics)\n",
    "#         summary_df.to_csv(f'results/{optimizer_name}_summary_metrics.csv', index=False)\n",
    "        \n",
    "#         # Print overall results for this optimizer\n",
    "#         print(f\"\\nOverall Results for {optimizer_name}:\")\n",
    "#         print(f\"Average Test Accuracy: {summary_df['Test Accuracy'].mean():.4f} ± {summary_df['Test Accuracy'].std():.4f}\")\n",
    "#         print(f\"Average F1 Score: {summary_df['F1 Score'].mean():.4f} ± {summary_df['F1 Score'].std():.4f}\")\n",
    "#         print(f\"Average AUC-ROC: {summary_df['AUC-ROC'].mean():.4f} ± {summary_df['AUC-ROC'].std():.4f}\")\n",
    "#         print(f\"Average Training Time: {summary_df['Time Taken (seconds)'].mean():.2f} ± {summary_df['Time Taken (seconds)'].std():.2f} seconds\")\n",
    "#     else:\n",
    "#         summary_df = pd.DataFrame()  # Empty DataFrame if no metrics were collected\n",
    "    \n",
    "#     return summary_df, all_files\n",
    "\n",
    "# # Main execution\n",
    "# if __name__ == \"__main__\":\n",
    "#     # First preprocess the data\n",
    "#     X_train, X_test, y_train, y_test = preprocess_data(X_train, X_test, y_train, y_test)\n",
    "    \n",
    "#     # Define optimizers to test\n",
    "#     optimizers = [\"HGS\", \"MPA\", \"Pathways\"]\n",
    "#     all_summary_dfs = []\n",
    "#     all_generated_files = []\n",
    "    \n",
    "#     # Train and evaluate each optimizer\n",
    "#     for opt in optimizers:\n",
    "#         print(f\"\\nStarting training with {opt} optimizer\")\n",
    "#         try:\n",
    "#             summary_df, files = train_and_evaluate_single_optimizer(\n",
    "#                 X_train, y_train, X_test, y_test, opt\n",
    "#             )\n",
    "#             all_summary_dfs.append(summary_df)\n",
    "#             all_generated_files.extend(files)\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error training {opt} optimizer: {str(e)}\")\n",
    "#             continue\n",
    "    \n",
    "#     # Create comparative visualizations if we have results\n",
    "#     if all_summary_dfs:\n",
    "#         plot_comparative_metrics(all_summary_dfs)\n",
    "    \n",
    "#     # Print summary of generated files\n",
    "#     print(\"\\nGenerated Files:\")\n",
    "#     for file in all_generated_files:\n",
    "#         print(f\"- {file}\")\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbe5f9c-0d6e-49f5-a30f-e6702b51e216",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Optimizer\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import (confusion_matrix, classification_report, roc_curve, \n",
    "                           roc_auc_score, precision_recall_curve, auc, precision_score, \n",
    "                           recall_score, f1_score, cohen_kappa_score, matthews_corrcoef)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Create directory for results if it doesn't exist\n",
    "os.makedirs('results', exist_ok=True)\n",
    "\n",
    "# Custom Optimizers\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Optimizer\n",
    "import tensorflow.keras.backend as K\n",
    "class HGSOptimizer(tf.keras.optimizers.Optimizer):\n",
    "    def __init__(self, \n",
    "                 learning_rate=0.001,\n",
    "                 population_size=30,\n",
    "                 hunger_rate=0.1,\n",
    "                 name=\"HGSOptimizer\",\n",
    "                 **kwargs):\n",
    "        super().__init__(name=name, learning_rate=learning_rate, **kwargs)\n",
    "        self._population_size = population_size\n",
    "        self._hunger_rate = hunger_rate\n",
    "    \n",
    "    def update_step(self, gradient, variable, learning_rate):\n",
    "        \"\"\"Update step for variables.\"\"\"\n",
    "        hunger_factor = tf.random.uniform([], dtype=variable.dtype) * self._hunger_rate\n",
    "        survival_factor = 1.0 - hunger_factor\n",
    "        \n",
    "        # Apply the update\n",
    "        variable.assign_sub(learning_rate * gradient * survival_factor)\n",
    "\n",
    "\n",
    "def create_model(optimizer, input_shape=(224, 224, 3)):\n",
    "    model = Sequential([\n",
    "        Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=input_shape),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Flatten(),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy', \n",
    "                tf.keras.metrics.Precision(),\n",
    "                tf.keras.metrics.Recall(),\n",
    "                tf.keras.metrics.AUC()]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def save_metrics_to_csv(metrics_dict, optimizer_name, fold, k):\n",
    "    \"\"\"Save metrics for a single run to CSV\"\"\"\n",
    "    df = pd.DataFrame({\n",
    "        'Test Accuracy': [metrics_dict['Test Accuracy']],\n",
    "        'Train Accuracy': [metrics_dict['Train Accuracy']],\n",
    "        'Precision': [metrics_dict['Precision']],\n",
    "        'Recall': [metrics_dict['Recall']],\n",
    "        'AUC-ROC': [metrics_dict['AUC-ROC']],\n",
    "        'AUC-PR': [metrics_dict['AUC-PR']],\n",
    "        'TN': [metrics_dict['TN']],\n",
    "        'FP': [metrics_dict['FP']],\n",
    "        'FN': [metrics_dict['FN']],\n",
    "        'TP': [metrics_dict['TP']],\n",
    "        'F1 Score': [metrics_dict['F1 Score']],\n",
    "        'Cohen\\'s Kappa Coefficient': [metrics_dict['Cohen\\'s Kappa']],\n",
    "        'Matthews Correlation Coefficient': [metrics_dict['Matthews Correlation Coefficient']],\n",
    "        'Training Loss': [metrics_dict['Training Loss']],\n",
    "        'Testing Loss': [metrics_dict['Testing Loss']],\n",
    "        'Time Taken (seconds)': [metrics_dict['Time Taken (seconds)']],\n",
    "        'Fold': [fold],\n",
    "        'K': [k]\n",
    "    })\n",
    "    \n",
    "    filename = f'results/{optimizer_name}_metrics_K{k}_Fold{fold}.csv'\n",
    "    df.to_csv(filename, index=False)\n",
    "    return filename\n",
    "\n",
    "def plot_training_curves(history, optimizer_name, k, fold):\n",
    "    \"\"\"Plot and save training curves\"\"\"\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    # Accuracy plot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'], label='train')\n",
    "    plt.plot(history.history['val_accuracy'], label='validation')\n",
    "    plt.title(f'{optimizer_name} - Accuracy (K={k}, Fold={fold})')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Loss plot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'], label='train')\n",
    "    plt.plot(history.history['val_loss'], label='validation')\n",
    "    plt.title(f'{optimizer_name} - Loss (K={k}, Fold={fold})')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'results/{optimizer_name}_training_curves_K{k}_Fold{fold}.png')\n",
    "    plt.close()\n",
    "\n",
    "def plot_comparative_metrics(all_summary_dfs):\n",
    "    \"\"\"Create comparative visualizations for all optimizers\"\"\"\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    metrics_to_plot = ['Test Accuracy', 'F1 Score', 'AUC-ROC', 'Training Loss']\n",
    "    for i, metric in enumerate(metrics_to_plot, 1):\n",
    "        plt.subplot(2, 2, i)\n",
    "        data = pd.concat(all_summary_dfs)\n",
    "        sns.boxplot(data=data, x='K', y=metric, hue='Optimizer')\n",
    "        plt.title(f'{metric} by K-Fold and Optimizer')\n",
    "        plt.xticks(rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('results/comparative_metrics.png')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def preprocess_data(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"Preprocess the data to ensure correct shapes and normalization\"\"\"\n",
    "    # Add channel dimension if not present\n",
    "    if len(X_train.shape) == 3:\n",
    "        X_train = X_train[..., np.newaxis]\n",
    "    if len(X_test.shape) == 3:\n",
    "        X_test = X_test[..., np.newaxis]\n",
    "    \n",
    "    # Convert to RGB by repeating the channel 3 times\n",
    "    X_train = np.repeat(X_train, 3, axis=-1)\n",
    "    X_test = np.repeat(X_test, 3, axis=-1)\n",
    "    \n",
    "    # Normalize pixel values\n",
    "    X_train = X_train.astype('float32') / 255.0\n",
    "    X_test = X_test.astype('float32') / 255.0\n",
    "    \n",
    "    # Convert labels to numpy arrays if they aren't already\n",
    "    y_train = np.array(y_train)\n",
    "    y_test = np.array(y_test)\n",
    "    \n",
    "    print(\"Preprocessed shapes:\")\n",
    "    print(f\"X_train: {X_train.shape}\")\n",
    "    print(f\"X_test: {X_test.shape}\")\n",
    "    print(f\"y_train: {y_train.shape}\")\n",
    "    print(f\"y_test: {y_test.shape}\")\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def create_model(optimizer, input_shape):\n",
    "    \"\"\"Create model with specified input shape\"\"\"\n",
    "    model = Sequential([\n",
    "        Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=input_shape),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Flatten(),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy', \n",
    "                tf.keras.metrics.Precision(),\n",
    "                tf.keras.metrics.Recall(),\n",
    "                tf.keras.metrics.AUC()]\n",
    "    )\n",
    "    return model\n",
    "def train_and_evaluate_single_optimizer(X_train, y_train, X_test, y_test, optimizer_name, k_folds=[5, 10, 15, 20]):\n",
    "    \"\"\"Train and evaluate a single optimizer with detailed metrics saving\"\"\"\n",
    "    \n",
    "    all_files = []\n",
    "    summary_metrics = []\n",
    "    \n",
    "    # Ensure input shape is correct\n",
    "    input_shape = X_train.shape[1:]  # Should be (224, 224, 3)\n",
    "    print(f\"Model input shape: {input_shape}\")\n",
    "    \n",
    "    for k in k_folds:\n",
    "        print(f'\\nTraining with k={k} folds')\n",
    "        kfold = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "        \n",
    "        for fold, (train_ids, val_ids) in enumerate(kfold.split(X_train)):\n",
    "            print(f'Training fold {fold + 1}/{k}')\n",
    "            \n",
    "            try:\n",
    "                # Split data\n",
    "                X_train_fold = X_train[train_ids]\n",
    "                y_train_fold = y_train[train_ids]\n",
    "                X_val_fold = X_train[val_ids]\n",
    "                y_val_fold = y_train[val_ids]\n",
    "                \n",
    "                print(f\"Fold data shapes:\")\n",
    "                print(f\"X_train_fold: {X_train_fold.shape}\")\n",
    "                print(f\"y_train_fold: {y_train_fold.shape}\")\n",
    "                print(f\"X_val_fold: {X_val_fold.shape}\")\n",
    "                print(f\"y_val_fold: {y_val_fold.shape}\")\n",
    "                \n",
    "                # Create optimizer\n",
    "                if optimizer_name == \"HGS\":\n",
    "                    optimizer = HGSOptimizer(learning_rate=0.001, hunger_rate=0.1)\n",
    "                elif optimizer_name == \"MPA\":\n",
    "                    optimizer = MPAOptimizer(learning_rate=0.001, elite_factor=0.2)\n",
    "                else:  # Pathways\n",
    "                    optimizer = PathwaysOptimizer(learning_rate=0.001, pathway_strength=0.3)\n",
    "                \n",
    "                # Create model with correct input shape\n",
    "                model = create_model(optimizer, input_shape)\n",
    "                \n",
    "                early_stopping = EarlyStopping(\n",
    "                    monitor='val_loss',\n",
    "                    patience=5,\n",
    "                    restore_best_weights=True\n",
    "                )\n",
    "                \n",
    "                checkpoint = ModelCheckpoint(\n",
    "                    f'results/{optimizer_name}_model_K{k}_Fold{fold}.h5',\n",
    "                    monitor='val_loss',\n",
    "                    save_best_only=True\n",
    "                )\n",
    "                \n",
    "                # Start timing\n",
    "                start_time = time.time()\n",
    "                \n",
    "                # Train the model\n",
    "                history = model.fit(\n",
    "                    X_train_fold, y_train_fold,\n",
    "                    epochs=50,\n",
    "                    batch_size=32,\n",
    "                    validation_data=(X_val_fold, y_val_fold),\n",
    "                    callbacks=[early_stopping, checkpoint],\n",
    "                    verbose=1\n",
    "                )\n",
    "                \n",
    "                # End timing\n",
    "                time_taken = time.time() - start_time\n",
    "                \n",
    "                # Evaluate model\n",
    "                test_results = model.evaluate(X_test, y_test, verbose=0)\n",
    "                y_pred = model.predict(X_test)\n",
    "                y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "                \n",
    "                # Calculate confusion matrix metrics\n",
    "                tn, fp, fn, tp = confusion_matrix(y_test, y_pred_binary).ravel()\n",
    "                \n",
    "                # Calculate precision-recall curve for AUC-PR\n",
    "                precision, recall, _ = precision_recall_curve(y_test, y_pred)\n",
    "                pr_auc = auc(recall, precision)\n",
    "                \n",
    "                # Create metrics dictionary\n",
    "                metrics = {\n",
    "                    'Test Accuracy': test_results[1],\n",
    "                    'Train Accuracy': history.history['accuracy'][-1],\n",
    "                    'Precision': precision_score(y_test, y_pred_binary),\n",
    "                    'Recall': recall_score(y_test, y_pred_binary),\n",
    "                    'AUC-ROC': roc_auc_score(y_test, y_pred),\n",
    "                    'AUC-PR': pr_auc,\n",
    "                    'TN': tn,\n",
    "                    'FP': fp,\n",
    "                    'FN': fn,\n",
    "                    'TP': tp,\n",
    "                    'F1 Score': f1_score(y_test, y_pred_binary),\n",
    "                    'Cohen\\'s Kappa': cohen_kappa_score(y_test, y_pred_binary),\n",
    "                    'Matthews Correlation Coefficient': matthews_corrcoef(y_test, y_pred_binary),\n",
    "                    'Training Loss': history.history['loss'][-1],\n",
    "                    'Testing Loss': test_results[0],\n",
    "                    'Time Taken (seconds)': time_taken\n",
    "                }\n",
    "                \n",
    "                # Save metrics to CSV\n",
    "                csv_file = save_metrics_to_csv(metrics, optimizer_name, fold + 1, k)\n",
    "                all_files.append(csv_file)\n",
    "                \n",
    "                # Plot and save training curves\n",
    "                plot_training_curves(history, optimizer_name, k, fold + 1)\n",
    "                \n",
    "                # Store summary metrics\n",
    "                summary_metrics.append({\n",
    "                    'Optimizer': optimizer_name,\n",
    "                    'K': k,\n",
    "                    'Fold': fold + 1,\n",
    "                    **metrics\n",
    "                })\n",
    "                \n",
    "                print(f\"\\nFold {fold + 1} Results:\")\n",
    "                print(f\"Test Accuracy: {metrics['Test Accuracy']:.4f}\")\n",
    "                print(f\"F1 Score: {metrics['F1 Score']:.4f}\")\n",
    "                print(f\"AUC-ROC: {metrics['AUC-ROC']:.4f}\")\n",
    "                print(f\"Training Time: {metrics['Time Taken (seconds)']:.2f} seconds\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error in fold {fold + 1}: {str(e)}\")\n",
    "                continue\n",
    "    \n",
    "    # Create and save summary DataFrame for this optimizer\n",
    "    if summary_metrics:\n",
    "        summary_df = pd.DataFrame(summary_metrics)\n",
    "        summary_df.to_csv(f'results/{optimizer_name}_summary_metrics.csv', index=False)\n",
    "        \n",
    "        # Print overall results for this optimizer\n",
    "        print(f\"\\nOverall Results for {optimizer_name}:\")\n",
    "        print(f\"Average Test Accuracy: {summary_df['Test Accuracy'].mean():.4f} ± {summary_df['Test Accuracy'].std():.4f}\")\n",
    "        print(f\"Average F1 Score: {summary_df['F1 Score'].mean():.4f} ± {summary_df['F1 Score'].std():.4f}\")\n",
    "        print(f\"Average AUC-ROC: {summary_df['AUC-ROC'].mean():.4f} ± {summary_df['AUC-ROC'].std():.4f}\")\n",
    "        print(f\"Average Training Time: {summary_df['Time Taken (seconds)'].mean():.2f} ± {summary_df['Time Taken (seconds)'].std():.2f} seconds\")\n",
    "    else:\n",
    "        summary_df = pd.DataFrame()  # Empty DataFrame if no metrics were collected\n",
    "    \n",
    "    return summary_df, all_files\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # First preprocess the data\n",
    "    X_train, X_test, y_train, y_test = preprocess_data(X_train, X_test, y_train, y_test)\n",
    "    \n",
    "    # Define optimizers to test\n",
    "    optimizers = [\"HGS\", \"MPA\", \"Pathways\"]\n",
    "    all_summary_dfs = []\n",
    "    all_generated_files = []\n",
    "    \n",
    "    # Train and evaluate each optimizer\n",
    "    for opt in optimizers:\n",
    "        print(f\"\\nStarting training with {opt} optimizer\")\n",
    "        try:\n",
    "            summary_df, files = train_and_evaluate_single_optimizer(\n",
    "                X_train, y_train, X_test, y_test, opt\n",
    "            )\n",
    "            all_summary_dfs.append(summary_df)\n",
    "            all_generated_files.extend(files)\n",
    "        except Exception as e:\n",
    "            print(f\"Error training {opt} optimizer: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # Create comparative visualizations if we have results\n",
    "    if all_summary_dfs:\n",
    "        plot_comparative_metrics(all_summary_dfs)\n",
    "    \n",
    "    # Print summary of generated files\n",
    "    print(\"\\nGenerated Files:\")\n",
    "    for file in all_generated_files:\n",
    "        print(f\"- {file}\")\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e1dba19-870b-4cab-8465-38cda29eb449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HGS Summary:\n",
      "\n",
      "HGS Generated Files:\n",
      "- results/HGS_metrics_K5_Fold1.csv\n",
      "- results/HGS_metrics_K5_Fold2.csv\n",
      "- results/HGS_metrics_K5_Fold3.csv\n",
      "- results/HGS_metrics_K5_Fold4.csv\n",
      "- results/HGS_metrics_K5_Fold5.csv\n",
      "- results/HGS_metrics_K10_Fold1.csv\n",
      "- results/HGS_metrics_K10_Fold2.csv\n",
      "- results/HGS_metrics_K10_Fold3.csv\n",
      "- results/HGS_metrics_K10_Fold4.csv\n",
      "- results/HGS_metrics_K10_Fold5.csv\n",
      "- results/HGS_metrics_K10_Fold6.csv\n",
      "- results/HGS_metrics_K10_Fold7.csv\n",
      "- results/HGS_metrics_K10_Fold8.csv\n",
      "- results/HGS_metrics_K10_Fold9.csv\n",
      "- results/HGS_metrics_K10_Fold10.csv\n",
      "- results/HGS_metrics_K15_Fold1.csv\n",
      "- results/HGS_metrics_K15_Fold2.csv\n",
      "- results/HGS_metrics_K15_Fold3.csv\n",
      "- results/HGS_metrics_K15_Fold4.csv\n",
      "- results/HGS_metrics_K15_Fold5.csv\n",
      "- results/HGS_metrics_K15_Fold6.csv\n",
      "- results/HGS_metrics_K15_Fold7.csv\n",
      "- results/HGS_metrics_K15_Fold8.csv\n",
      "- results/HGS_metrics_K15_Fold9.csv\n",
      "- results/HGS_metrics_K15_Fold10.csv\n",
      "- results/HGS_metrics_K15_Fold11.csv\n",
      "- results/HGS_metrics_K15_Fold12.csv\n",
      "- results/HGS_metrics_K15_Fold13.csv\n",
      "- results/HGS_metrics_K15_Fold14.csv\n",
      "- results/HGS_metrics_K15_Fold15.csv\n",
      "- results/HGS_metrics_K20_Fold1.csv\n",
      "- results/HGS_metrics_K20_Fold2.csv\n",
      "- results/HGS_metrics_K20_Fold3.csv\n",
      "- results/HGS_metrics_K20_Fold4.csv\n",
      "- results/HGS_metrics_K20_Fold5.csv\n",
      "- results/HGS_metrics_K20_Fold6.csv\n",
      "- results/HGS_metrics_K20_Fold7.csv\n",
      "- results/HGS_metrics_K20_Fold8.csv\n",
      "- results/HGS_metrics_K20_Fold9.csv\n",
      "- results/HGS_metrics_K20_Fold10.csv\n",
      "- results/HGS_metrics_K20_Fold11.csv\n",
      "- results/HGS_metrics_K20_Fold12.csv\n",
      "- results/HGS_metrics_K20_Fold13.csv\n",
      "- results/HGS_metrics_K20_Fold14.csv\n",
      "- results/HGS_metrics_K20_Fold15.csv\n",
      "- results/HGS_metrics_K20_Fold16.csv\n",
      "- results/HGS_metrics_K20_Fold17.csv\n",
      "- results/HGS_metrics_K20_Fold18.csv\n",
      "- results/HGS_metrics_K20_Fold19.csv\n",
      "- results/HGS_metrics_K20_Fold20.csv\n"
     ]
    }
   ],
   "source": [
    "# Extract results for HGS\n",
    "# hgs_summary_df = all_summary_dfs[0]  # HGS should be the first optimizer\n",
    "hgs_generated_files = all_generated_files[:len(files)]  # Files generated by HGS\n",
    "\n",
    "# Display summary\n",
    "print(\"HGS Summary:\")\n",
    "# print(hgs_summary_df)\n",
    "\n",
    "# List the generated files\n",
    "print(\"\\nHGS Generated Files:\")\n",
    "for file in hgs_generated_files:\n",
    "    print(f\"- {file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6f47219a-5e47-49d0-b3b7-35762776291b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged CSV saved as: results/HGS_merged_results.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# Path to all HGS result files\n",
    "file_paths = sorted(glob.glob(\"results/HGS_metrics_*.csv\"))  # Fetch all matching CSV files\n",
    "\n",
    "# Read and concatenate all files\n",
    "df_list = [pd.read_csv(file) for file in file_paths]\n",
    "merged_df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# Save to a single CSV\n",
    "merged_df.to_csv(\"results/HGS_merged_results.csv\", index=False)\n",
    "\n",
    "print(\"Merged CSV saved as: results/HGS_merged_results.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8cef8a-0bc4-4acf-9f71-147abedf3c6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "350b542e-4c8c-49c7-b778-1a21f9358d13",
   "metadata": {},
   "source": [
    "Lion Optimization Algorithm (LOA)\n",
    "\n",
    "Salp Swarm Algorithm (SSA)\n",
    "\n",
    "Grey Wolf Optimizer (GWO)\n",
    "\n",
    "AdamW\n",
    "\n",
    "Arithmetic Optimization Algorithm (AOA)\n",
    "\n",
    "Sharpness-Aware Minimization (SAM)\n",
    "\n",
    "Stochastic Weight Averaging (SWA)\n",
    "\n",
    "LAMB (Layer-wise Adaptive Moments optimizer for Batch training)\n",
    "\n",
    "Sine Cosine Algorithm (SCA) with Neural Extension Chaos Optimization Algorithms (COA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f5ce53-a764-4ddc-9adc-e2562bfe3c99",
   "metadata": {},
   "source": [
    "Hunger Games Search (HGS)\n",
    "\n",
    "Marine Predators Algorithm (MPA)\n",
    "\n",
    "Pathways Optimizer (2024)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfee4f7b-fb61-4845-915a-9a8ac7ed49ee",
   "metadata": {},
   "source": [
    "Hunger Games Search (HGS):\n",
    "\n",
    "\n",
    "Inspired by the survival competition concept\n",
    "Key features for image classification:\n",
    "\n",
    "Adaptive population size helps find optimal features\n",
    "Balance between exploration (searching new areas) and exploitation (refining existing solutions)\n",
    "Hunger mechanism helps escape local optima\n",
    "\n",
    "\n",
    "Significance for your data:\n",
    "\n",
    "Good for handling high-dimensional image data (224x224x3 = 150,528 dimensions)\n",
    "Can help find discriminative features between your binary classes\n",
    "The hunger mechanism can help prevent overfitting\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Marine Predators Algorithm (MPA):\n",
    "\n",
    "\n",
    "Based on marine predator foraging strategies\n",
    "Key features:\n",
    "\n",
    "Three phases of optimization: cruise, hunt, and chase\n",
    "Elite-based updating mechanism\n",
    "Adaptive movement patterns\n",
    "\n",
    "\n",
    "Significance for your data:\n",
    "\n",
    "Multi-phase search can help find subtle differences between classes\n",
    "Elite preservation helps maintain good features\n",
    "Particularly good at handling noisy data\n",
    "Adaptive step sizes can help with fine-grained feature learning\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Pathways Optimizer (2024):\n",
    "\n",
    "\n",
    "Inspired by neural pathway formation\n",
    "Key features:\n",
    "\n",
    "Dynamic pathway strength adaptation\n",
    "Parallel path exploration\n",
    "Learning rate adjustment based on pathway success\n",
    "\n",
    "\n",
    "Significance for your data:\n",
    "\n",
    "Well-suited for deep learning architectures\n",
    "Can adapt to different feature importances\n",
    "Good at handling the hierarchical nature of image features\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06358822-6d07-482e-821e-e4fc92dee1c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
