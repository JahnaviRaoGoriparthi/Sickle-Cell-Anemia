{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3e08a84-0b73-4ea1-9097-457553a3bc41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Proportions:\n",
      "Negatives: 1000 Positives: 1000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import imgaug.augmenters as iaa\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define constants\n",
    "data_path = \"/Users/yanthraa/Desktop/PROJECT/NEW DATASET/archive\"\n",
    "target_size = (224, 224)\n",
    "target_count = 1000  # Desired number of images per class\n",
    "output_path = \"/Users/yanthraa/Desktop/PROJECT/NEW DATASET/Augmented_Dataset\"  # Output directory\n",
    "\n",
    "# Ensure output directories exist\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "for class_name in ['Negatives', 'Positives']:\n",
    "    os.makedirs(os.path.join(output_path, class_name), exist_ok=True)\n",
    "\n",
    "# Define augmentation sequence\n",
    "augmenters = iaa.Sequential([\n",
    "    iaa.Fliplr(0.5),  # Horizontal flip\n",
    "    iaa.Affine(rotate=(-15, 15)),  # Rotate between -15 and 15 degrees\n",
    "    iaa.GaussianBlur(sigma=(0.0, 1.0)),  # Apply Gaussian blur\n",
    "    iaa.AdditiveGaussianNoise(scale=(0, 0.05*255)),  # Add noise\n",
    "    iaa.Multiply((0.8, 1.2))  # Brightness adjustments\n",
    "])\n",
    "\n",
    "# Load and preprocess data\n",
    "images = []\n",
    "labels = []\n",
    "\n",
    "for class_name in ['Negatives', 'Positives']:\n",
    "    class_path = os.path.join(data_path, class_name)\n",
    "    class_images = []\n",
    "    label = 0 if class_name == 'Negatives' else 1\n",
    "    \n",
    "    # Load original images\n",
    "    for image_name in os.listdir(class_path):\n",
    "        image_path = os.path.join(class_path, image_name)\n",
    "        img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "        img = cv2.resize(img, target_size)\n",
    "        img = img.astype(np.float32) / 255.0\n",
    "        class_images.append(img)\n",
    "    \n",
    "    # Augment images until target count is reached\n",
    "    while len(class_images) < target_count:\n",
    "        augmented_images = augmenters(images=class_images[:min(len(class_images), target_count - len(class_images))])\n",
    "        class_images.extend(augmented_images)\n",
    "    \n",
    "    class_images = class_images[:target_count]  # Trim excess\n",
    "    images.extend(class_images)\n",
    "    labels.extend([label] * target_count)\n",
    "    \n",
    "    # Save augmented images\n",
    "    for idx, img in enumerate(class_images):\n",
    "        save_path = os.path.join(output_path, class_name, f\"aug_{idx}.png\")\n",
    "        cv2.imwrite(save_path, (img * 255).astype(np.uint8))\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "images = np.array(images)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Print class proportions\n",
    "print(\"Class Proportions:\")\n",
    "print(\"Negatives:\", np.sum(labels == 0), \"Positives:\", np.sum(labels == 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e10a5879-c1b2-43a6-bc97-8b99c4642c6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced Images: (1000, 224, 224)\n",
      "Balanced labels: (1000,)\n",
      "Negatives_images: (500, 224, 224)\n",
      "Positives_images: (500, 224, 224)\n",
      "X_train shape: (720, 224, 224)\n",
      "X_test shape: (200, 224, 224)\n",
      "y_train shape: (720,)\n",
      "y_test shape: (200,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Separate data for Negatives and Positives classes\n",
    "Negatives_images = images[labels == 0][:500]\n",
    "Positives_images = images[labels == 1][:500]\n",
    "Negatives_labels = labels[labels == 0][:500]\n",
    "Positives_labels = labels[labels == 1][:500]\n",
    "\n",
    "# Concatenate the data back together\n",
    "balanced_images = np.concatenate([Negatives_images, Positives_images])\n",
    "balanced_labels = np.concatenate([Negatives_labels, Positives_labels])\n",
    "\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(balanced_images, balanced_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Split the training data into training and validation sets\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full, test_size=0.1, random_state=42)\n",
    "\n",
    "#print the balanced data\n",
    "print(\"Balanced Images:\",balanced_images.shape)\n",
    "print(\"Balanced labels:\",balanced_labels.shape)\n",
    "\n",
    "#print the data seperately of each class\n",
    "print(\"Negatives_images:\",Negatives_images.shape)\n",
    "print(\"Positives_images:\",Positives_images.shape)\n",
    "\n",
    "\n",
    "# Print the shapes of the training and testing sets\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22561f18-70be-4ae6-b41a-94d80411052e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/anaconda3/lib/python3.12/site-packages (4.49.0)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.29.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.5.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2025.1.31)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8928d69d-d37e-4279-994a-2fc651510d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tf-keras in /opt/anaconda3/lib/python3.12/site-packages (2.18.0)\n",
      "Requirement already satisfied: tensorflow<2.19,>=2.18 in /opt/anaconda3/lib/python3.12/site-packages (from tf-keras) (2.18.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (25.1.24)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (3.4.0)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (24.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (4.25.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (75.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (4.11.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (1.70.0)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (2.18.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (3.8.0)\n",
      "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (1.26.4)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (3.11.0)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (0.4.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/anaconda3/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow<2.19,>=2.18->tf-keras) (0.44.0)\n",
      "Requirement already satisfied: rich in /opt/anaconda3/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras) (13.7.1)\n",
      "Requirement already satisfied: namex in /opt/anaconda3/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras) (0.0.8)\n",
      "Requirement already satisfied: optree in /opt/anaconda3/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras) (0.14.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow<2.19,>=2.18->tf-keras) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow<2.19,>=2.18->tf-keras) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow<2.19,>=2.18->tf-keras) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow<2.19,>=2.18->tf-keras) (2025.1.31)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18->tf-keras) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18->tf-keras) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18->tf-keras) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18->tf-keras) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from rich->keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.12/site-packages (from rich->keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras) (0.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tf-keras"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9d52343f-d630-48cb-9401-54cc6dce3e6b",
   "metadata": {},
   "source": [
    "MBConv Block: Efficient bottleneck convolution with squeeze-excitation\n",
    "Window Attention: Local self-attention with relative position encoding\n",
    "Hybrid Architecture: Combines convolution and transformer blocks\n",
    "Optimizations:\n",
    "\n",
    "Uses AdamW optimizer with weight decay\n",
    "Properly handles padding for window attention\n",
    "Includes relative position bias\n",
    "Efficient memory usage with window partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8837836c-e83e-4731-9849-6ceaf115fb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model, Input\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import f1_score, confusion_matrix, roc_auc_score\n",
    "# Add these new imports:\n",
    "from sklearn.metrics import matthews_corrcoef, cohen_kappa_score, balanced_accuracy_score\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cee58050-7532-4235-ae2d-bce357eb3384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downsampled data from 224x224 to 74x74\n",
      "Downsampled data from 224x224 to 74x74\n",
      "\n",
      "=== Starting 5-fold cross-validation ===\n",
      "\n",
      "Fold 1/5\n",
      "Training data shape: (640, 74, 74, 3) (limited for CPU)\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/optimizers/base_optimizer.py:774: UserWarning: Gradients do not exist for variables ['max_vi_t_block/window_attention/sequential_1/conv2d_1/kernel', 'max_vi_t_block/window_attention/sequential_1/conv2d_1/bias', 'max_vi_t_block/window_attention/sequential_1/batch_normalization_1/gamma', 'max_vi_t_block/window_attention/sequential_1/batch_normalization_1/beta', 'max_vi_t_block_1/window_attention_1/sequential_3/conv2d_3/kernel', 'max_vi_t_block_1/window_attention_1/sequential_3/conv2d_3/bias', 'max_vi_t_block_1/window_attention_1/sequential_3/batch_normalization_3/gamma', 'max_vi_t_block_1/window_attention_1/sequential_3/batch_normalization_3/beta'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 117ms/step - accuracy: 0.6730 - loss: 0.6545 - val_accuracy: 0.5188 - val_loss: 0.7396 - learning_rate: 0.0050\n",
      "Epoch 2/50\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 102ms/step - accuracy: 0.7422 - loss: 0.5864 - val_accuracy: 0.5188 - val_loss: 0.7470 - learning_rate: 0.0050\n",
      "Epoch 3/50\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 104ms/step - accuracy: 0.7636 - loss: 0.5390 - val_accuracy: 0.5188 - val_loss: 0.7194 - learning_rate: 5.0000e-04\n",
      "Epoch 4/50\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 106ms/step - accuracy: 0.7407 - loss: 0.5457 - val_accuracy: 0.5188 - val_loss: 0.7427 - learning_rate: 5.0000e-04\n",
      "Epoch 5/50\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 123ms/step - accuracy: 0.7390 - loss: 0.5342 - val_accuracy: 0.5188 - val_loss: 0.7545 - learning_rate: 1.0000e-04\n",
      "\n",
      "Fold 1 Results:\n",
      "Fold: 1.0000\n",
      "K: 5.0000\n",
      "Test_Accuracy: 0.4800\n",
      "Train_Accuracy: 0.7563\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "AUC_ROC: 0.7945\n",
      "F1_Score: 0.0000\n",
      "Training_Loss: 0.5164\n",
      "Testing_Loss: 0.7593\n",
      "Time_Taken: 15.2012\n",
      "Epochs_Run: 5.0000\n",
      "Specificity: 1.0000\n",
      "MCC: 0.0000\n",
      "Log_Loss: 0.8087\n",
      "G_Mean: 0.0000\n",
      "Youdens_J: 0.0000\n",
      "Balanced_Accuracy: 0.5000\n",
      "Cohens_Kappa: 0.0000\n",
      "\n",
      "Fold 2/5\n",
      "Training data shape: (640, 74, 74, 3) (limited for CPU)\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/optimizers/base_optimizer.py:774: UserWarning: Gradients do not exist for variables ['max_vi_t_block/window_attention/sequential_1/conv2d_1/kernel', 'max_vi_t_block/window_attention/sequential_1/conv2d_1/bias', 'max_vi_t_block/window_attention/sequential_1/batch_normalization_1/gamma', 'max_vi_t_block/window_attention/sequential_1/batch_normalization_1/beta', 'max_vi_t_block_1/window_attention_1/sequential_3/conv2d_3/kernel', 'max_vi_t_block_1/window_attention_1/sequential_3/conv2d_3/bias', 'max_vi_t_block_1/window_attention_1/sequential_3/batch_normalization_3/gamma', 'max_vi_t_block_1/window_attention_1/sequential_3/batch_normalization_3/beta'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 117ms/step - accuracy: 0.6385 - loss: 0.6705 - val_accuracy: 0.5188 - val_loss: 0.6873 - learning_rate: 0.0050\n",
      "Epoch 2/50\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.7673 - loss: 0.5563 - val_accuracy: 0.5063 - val_loss: 0.7048 - learning_rate: 0.0050\n",
      "Epoch 3/50\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.7211 - loss: 0.6057 - val_accuracy: 0.5063 - val_loss: 0.6935 - learning_rate: 5.0000e-04\n",
      "\n",
      "Fold 2 Results:\n",
      "Fold: 2.0000\n",
      "K: 5.0000\n",
      "Test_Accuracy: 0.5500\n",
      "Train_Accuracy: 0.7594\n",
      "Precision: 0.5365\n",
      "Recall: 0.9904\n",
      "AUC_ROC: 0.7379\n",
      "F1_Score: 0.6959\n",
      "Training_Loss: 0.5544\n",
      "Testing_Loss: 0.6745\n",
      "Time_Taken: 10.6485\n",
      "Epochs_Run: 3.0000\n",
      "Specificity: 0.0729\n",
      "MCC: 0.1614\n",
      "Log_Loss: 0.6958\n",
      "G_Mean: 0.2687\n",
      "Youdens_J: 0.0633\n",
      "Balanced_Accuracy: 0.5317\n",
      "Cohens_Kappa: 0.0656\n",
      "\n",
      "Fold 3/5\n",
      "Training data shape: (640, 74, 74, 3) (limited for CPU)\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/optimizers/base_optimizer.py:774: UserWarning: Gradients do not exist for variables ['max_vi_t_block/window_attention/sequential_1/conv2d_1/kernel', 'max_vi_t_block/window_attention/sequential_1/conv2d_1/bias', 'max_vi_t_block/window_attention/sequential_1/batch_normalization_1/gamma', 'max_vi_t_block/window_attention/sequential_1/batch_normalization_1/beta', 'max_vi_t_block_1/window_attention_1/sequential_3/conv2d_3/kernel', 'max_vi_t_block_1/window_attention_1/sequential_3/conv2d_3/bias', 'max_vi_t_block_1/window_attention_1/sequential_3/batch_normalization_3/gamma', 'max_vi_t_block_1/window_attention_1/sequential_3/batch_normalization_3/beta'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 120ms/step - accuracy: 0.7375 - loss: 0.5779 - val_accuracy: 0.5000 - val_loss: 0.7739 - learning_rate: 0.0050\n",
      "Epoch 2/50\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 117ms/step - accuracy: 0.7035 - loss: 0.5961 - val_accuracy: 0.6687 - val_loss: 0.6421 - learning_rate: 0.0050\n",
      "Epoch 3/50\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.7745 - loss: 0.5576 - val_accuracy: 0.6438 - val_loss: 0.6451 - learning_rate: 0.0050\n",
      "Epoch 4/50\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.7118 - loss: 0.6006 - val_accuracy: 0.6438 - val_loss: 0.6466 - learning_rate: 5.0000e-04\n",
      "\n",
      "Fold 3 Results:\n",
      "Fold: 3.0000\n",
      "K: 5.0000\n",
      "Test_Accuracy: 0.7300\n",
      "Train_Accuracy: 0.7563\n",
      "Precision: 0.6923\n",
      "Recall: 0.8654\n",
      "AUC_ROC: 0.7758\n",
      "F1_Score: 0.7692\n",
      "Training_Loss: 0.5453\n",
      "Testing_Loss: 0.5968\n",
      "Time_Taken: 13.0458\n",
      "Epochs_Run: 4.0000\n",
      "Specificity: 0.5833\n",
      "MCC: 0.4700\n",
      "Log_Loss: 0.7473\n",
      "G_Mean: 0.7105\n",
      "Youdens_J: 0.4487\n",
      "Balanced_Accuracy: 0.7244\n",
      "Cohens_Kappa: 0.4534\n",
      "\n",
      "Fold 4/5\n",
      "Training data shape: (640, 74, 74, 3) (limited for CPU)\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/optimizers/base_optimizer.py:774: UserWarning: Gradients do not exist for variables ['max_vi_t_block/window_attention/sequential_1/conv2d_1/kernel', 'max_vi_t_block/window_attention/sequential_1/conv2d_1/bias', 'max_vi_t_block/window_attention/sequential_1/batch_normalization_1/gamma', 'max_vi_t_block/window_attention/sequential_1/batch_normalization_1/beta', 'max_vi_t_block_1/window_attention_1/sequential_3/conv2d_3/kernel', 'max_vi_t_block_1/window_attention_1/sequential_3/conv2d_3/bias', 'max_vi_t_block_1/window_attention_1/sequential_3/batch_normalization_3/gamma', 'max_vi_t_block_1/window_attention_1/sequential_3/batch_normalization_3/beta'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 116ms/step - accuracy: 0.6518 - loss: 0.6358 - val_accuracy: 0.5375 - val_loss: 0.6740 - learning_rate: 0.0050\n",
      "Epoch 2/50\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 99ms/step - accuracy: 0.7070 - loss: 0.6063 - val_accuracy: 0.5375 - val_loss: 0.7269 - learning_rate: 0.0050\n",
      "Epoch 3/50\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 105ms/step - accuracy: 0.7416 - loss: 0.5725 - val_accuracy: 0.5375 - val_loss: 0.7229 - learning_rate: 5.0000e-04\n",
      "\n",
      "Fold 4 Results:\n",
      "Fold: 4.0000\n",
      "K: 5.0000\n",
      "Test_Accuracy: 0.4800\n",
      "Train_Accuracy: 0.7328\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "AUC_ROC: 0.6239\n",
      "F1_Score: 0.0000\n",
      "Training_Loss: 0.5853\n",
      "Testing_Loss: 0.6989\n",
      "Time_Taken: 10.3043\n",
      "Epochs_Run: 3.0000\n",
      "Specificity: 1.0000\n",
      "MCC: 0.0000\n",
      "Log_Loss: 0.7200\n",
      "G_Mean: 0.0000\n",
      "Youdens_J: 0.0000\n",
      "Balanced_Accuracy: 0.5000\n",
      "Cohens_Kappa: 0.0000\n",
      "\n",
      "Fold 5/5\n",
      "Training data shape: (640, 74, 74, 3) (limited for CPU)\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/optimizers/base_optimizer.py:774: UserWarning: Gradients do not exist for variables ['max_vi_t_block/window_attention/sequential_1/conv2d_1/kernel', 'max_vi_t_block/window_attention/sequential_1/conv2d_1/bias', 'max_vi_t_block/window_attention/sequential_1/batch_normalization_1/gamma', 'max_vi_t_block/window_attention/sequential_1/batch_normalization_1/beta', 'max_vi_t_block_1/window_attention_1/sequential_3/conv2d_3/kernel', 'max_vi_t_block_1/window_attention_1/sequential_3/conv2d_3/bias', 'max_vi_t_block_1/window_attention_1/sequential_3/batch_normalization_3/gamma', 'max_vi_t_block_1/window_attention_1/sequential_3/batch_normalization_3/beta'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 125ms/step - accuracy: 0.7155 - loss: 0.6015 - val_accuracy: 0.4625 - val_loss: 1.3577 - learning_rate: 0.0050\n",
      "Epoch 2/50\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.7665 - loss: 0.5393 - val_accuracy: 0.4625 - val_loss: 1.0907 - learning_rate: 0.0050\n",
      "Epoch 3/50\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.7588 - loss: 0.5479 - val_accuracy: 0.4625 - val_loss: 1.4859 - learning_rate: 0.0050\n",
      "Epoch 4/50\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.7526 - loss: 0.5494 - val_accuracy: 0.4625 - val_loss: 1.3099 - learning_rate: 5.0000e-04\n",
      "\n",
      "Fold 5 Results:\n",
      "Fold: 5.0000\n",
      "K: 5.0000\n",
      "Test_Accuracy: 0.4800\n",
      "Train_Accuracy: 0.7656\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "AUC_ROC: 0.6899\n",
      "F1_Score: 0.0000\n",
      "Training_Loss: 0.5337\n",
      "Testing_Loss: 1.0520\n",
      "Time_Taken: 12.8769\n",
      "Epochs_Run: 4.0000\n",
      "Specificity: 1.0000\n",
      "MCC: 0.0000\n",
      "Log_Loss: 1.0877\n",
      "G_Mean: 0.0000\n",
      "Youdens_J: 0.0000\n",
      "Balanced_Accuracy: 0.5000\n",
      "Cohens_Kappa: 0.0000\n",
      "\n",
      "=== Starting 10-fold cross-validation ===\n",
      "\n",
      "Fold 1/10\n",
      "Training data shape: (720, 74, 74, 3) (limited for CPU)\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/optimizers/base_optimizer.py:774: UserWarning: Gradients do not exist for variables ['max_vi_t_block/window_attention/sequential_1/conv2d_1/kernel', 'max_vi_t_block/window_attention/sequential_1/conv2d_1/bias', 'max_vi_t_block/window_attention/sequential_1/batch_normalization_1/gamma', 'max_vi_t_block/window_attention/sequential_1/batch_normalization_1/beta', 'max_vi_t_block_1/window_attention_1/sequential_3/conv2d_3/kernel', 'max_vi_t_block_1/window_attention_1/sequential_3/conv2d_3/bias', 'max_vi_t_block_1/window_attention_1/sequential_3/batch_normalization_3/gamma', 'max_vi_t_block_1/window_attention_1/sequential_3/batch_normalization_3/beta'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 114ms/step - accuracy: 0.7173 - loss: 0.6368 - val_accuracy: 0.5125 - val_loss: 0.6793 - learning_rate: 0.0050\n",
      "Epoch 2/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 111ms/step - accuracy: 0.7368 - loss: 0.5773 - val_accuracy: 0.7875 - val_loss: 0.5832 - learning_rate: 0.0050\n",
      "Epoch 3/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 113ms/step - accuracy: 0.7470 - loss: 0.5895 - val_accuracy: 0.7000 - val_loss: 0.5934 - learning_rate: 0.0050\n",
      "Epoch 4/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 104ms/step - accuracy: 0.7541 - loss: 0.5494 - val_accuracy: 0.7375 - val_loss: 0.6012 - learning_rate: 5.0000e-04\n",
      "\n",
      "Fold 1 Results:\n",
      "Fold: 1.0000\n",
      "K: 10.0000\n",
      "Test_Accuracy: 0.7350\n",
      "Train_Accuracy: 0.7583\n",
      "Precision: 0.7008\n",
      "Recall: 0.8558\n",
      "AUC_ROC: 0.7641\n",
      "F1_Score: 0.7706\n",
      "Training_Loss: 0.5400\n",
      "Testing_Loss: 0.6135\n",
      "Time_Taken: 14.4383\n",
      "Epochs_Run: 4.0000\n",
      "Specificity: 0.6042\n",
      "MCC: 0.4773\n",
      "Log_Loss: 0.7319\n",
      "G_Mean: 0.7190\n",
      "Youdens_J: 0.4599\n",
      "Balanced_Accuracy: 0.7300\n",
      "Cohens_Kappa: 0.4642\n",
      "\n",
      "Fold 2/10\n",
      "Training data shape: (720, 74, 74, 3) (limited for CPU)\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/optimizers/base_optimizer.py:774: UserWarning: Gradients do not exist for variables ['max_vi_t_block/window_attention/sequential_1/conv2d_1/kernel', 'max_vi_t_block/window_attention/sequential_1/conv2d_1/bias', 'max_vi_t_block/window_attention/sequential_1/batch_normalization_1/gamma', 'max_vi_t_block/window_attention/sequential_1/batch_normalization_1/beta', 'max_vi_t_block_1/window_attention_1/sequential_3/conv2d_3/kernel', 'max_vi_t_block_1/window_attention_1/sequential_3/conv2d_3/bias', 'max_vi_t_block_1/window_attention_1/sequential_3/batch_normalization_3/gamma', 'max_vi_t_block_1/window_attention_1/sequential_3/batch_normalization_3/beta'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 116ms/step - accuracy: 0.7315 - loss: 0.6072 - val_accuracy: 0.4750 - val_loss: 0.8053 - learning_rate: 0.0050\n",
      "Epoch 2/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 112ms/step - accuracy: 0.7567 - loss: 0.5581 - val_accuracy: 0.4875 - val_loss: 0.7535 - learning_rate: 0.0050\n",
      "Epoch 3/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.7664 - loss: 0.5388 - val_accuracy: 0.7125 - val_loss: 0.5956 - learning_rate: 0.0050\n",
      "Epoch 4/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 105ms/step - accuracy: 0.7635 - loss: 0.5525 - val_accuracy: 0.5250 - val_loss: 0.8036 - learning_rate: 0.0050\n",
      "Epoch 5/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 102ms/step - accuracy: 0.7650 - loss: 0.5247 - val_accuracy: 0.5250 - val_loss: 0.6530 - learning_rate: 5.0000e-04\n",
      "\n",
      "Fold 2 Results:\n",
      "Fold: 2.0000\n",
      "K: 10.0000\n",
      "Test_Accuracy: 0.7300\n",
      "Train_Accuracy: 0.7597\n",
      "Precision: 0.6953\n",
      "Recall: 0.8558\n",
      "AUC_ROC: 0.7697\n",
      "F1_Score: 0.7672\n",
      "Training_Loss: 0.5322\n",
      "Testing_Loss: 0.5985\n",
      "Time_Taken: 16.7391\n",
      "Epochs_Run: 5.0000\n",
      "Specificity: 0.5938\n",
      "MCC: 0.4679\n",
      "Log_Loss: 0.7264\n",
      "G_Mean: 0.7128\n",
      "Youdens_J: 0.4495\n",
      "Balanced_Accuracy: 0.7248\n",
      "Cohens_Kappa: 0.4539\n",
      "\n",
      "Fold 3/10\n",
      "Training data shape: (720, 74, 74, 3) (limited for CPU)\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/optimizers/base_optimizer.py:774: UserWarning: Gradients do not exist for variables ['max_vi_t_block/window_attention/sequential_1/conv2d_1/kernel', 'max_vi_t_block/window_attention/sequential_1/conv2d_1/bias', 'max_vi_t_block/window_attention/sequential_1/batch_normalization_1/gamma', 'max_vi_t_block/window_attention/sequential_1/batch_normalization_1/beta', 'max_vi_t_block_1/window_attention_1/sequential_3/conv2d_3/kernel', 'max_vi_t_block_1/window_attention_1/sequential_3/conv2d_3/bias', 'max_vi_t_block_1/window_attention_1/sequential_3/batch_normalization_3/gamma', 'max_vi_t_block_1/window_attention_1/sequential_3/batch_normalization_3/beta'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 123ms/step - accuracy: 0.6848 - loss: 0.6166 - val_accuracy: 0.6000 - val_loss: 0.6566 - learning_rate: 0.0050\n",
      "Epoch 2/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 99ms/step - accuracy: 0.7730 - loss: 0.5497 - val_accuracy: 0.5375 - val_loss: 0.6897 - learning_rate: 0.0050\n",
      "Epoch 3/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 110ms/step - accuracy: 0.7240 - loss: 0.5606 - val_accuracy: 0.5750 - val_loss: 0.6724 - learning_rate: 5.0000e-04\n",
      "\n",
      "Fold 3 Results:\n",
      "Fold: 3.0000\n",
      "K: 10.0000\n",
      "Test_Accuracy: 0.7100\n",
      "Train_Accuracy: 0.7417\n",
      "Precision: 0.6667\n",
      "Recall: 0.8846\n",
      "AUC_ROC: 0.7516\n",
      "F1_Score: 0.7603\n",
      "Training_Loss: 0.5535\n",
      "Testing_Loss: 0.6208\n",
      "Time_Taken: 11.9369\n",
      "Epochs_Run: 3.0000\n",
      "Specificity: 0.5208\n",
      "MCC: 0.4380\n",
      "Log_Loss: 0.7180\n",
      "G_Mean: 0.6788\n",
      "Youdens_J: 0.4054\n",
      "Balanced_Accuracy: 0.7027\n",
      "Cohens_Kappa: 0.4110\n",
      "\n",
      "Fold 4/10\n",
      "Training data shape: (720, 74, 74, 3) (limited for CPU)\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/optimizers/base_optimizer.py:774: UserWarning: Gradients do not exist for variables ['max_vi_t_block/window_attention/sequential_1/conv2d_1/kernel', 'max_vi_t_block/window_attention/sequential_1/conv2d_1/bias', 'max_vi_t_block/window_attention/sequential_1/batch_normalization_1/gamma', 'max_vi_t_block/window_attention/sequential_1/batch_normalization_1/beta', 'max_vi_t_block_1/window_attention_1/sequential_3/conv2d_3/kernel', 'max_vi_t_block_1/window_attention_1/sequential_3/conv2d_3/bias', 'max_vi_t_block_1/window_attention_1/sequential_3/batch_normalization_3/gamma', 'max_vi_t_block_1/window_attention_1/sequential_3/batch_normalization_3/beta'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 120ms/step - accuracy: 0.7159 - loss: 0.5964 - val_accuracy: 0.4375 - val_loss: 0.9437 - learning_rate: 0.0050\n",
      "Epoch 2/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 109ms/step - accuracy: 0.7466 - loss: 0.5685 - val_accuracy: 0.4375 - val_loss: 0.9613 - learning_rate: 0.0050\n",
      "Epoch 3/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 102ms/step - accuracy: 0.7438 - loss: 0.5607 - val_accuracy: 0.4375 - val_loss: 0.9240 - learning_rate: 5.0000e-04\n",
      "\n",
      "Fold 4 Results:\n",
      "Fold: 4.0000\n",
      "K: 10.0000\n",
      "Test_Accuracy: 0.4800\n",
      "Train_Accuracy: 0.7583\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "AUC_ROC: 0.7023\n",
      "F1_Score: 0.0000\n",
      "Training_Loss: 0.5431\n",
      "Testing_Loss: 0.8853\n",
      "Time_Taken: 11.9218\n",
      "Epochs_Run: 3.0000\n",
      "Specificity: 1.0000\n",
      "MCC: 0.0000\n",
      "Log_Loss: 0.9211\n",
      "G_Mean: 0.0000\n",
      "Youdens_J: 0.0000\n",
      "Balanced_Accuracy: 0.5000\n",
      "Cohens_Kappa: 0.0000\n",
      "\n",
      "Fold 5/10\n",
      "Training data shape: (720, 74, 74, 3) (limited for CPU)\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/optimizers/base_optimizer.py:774: UserWarning: Gradients do not exist for variables ['max_vi_t_block/window_attention/sequential_1/conv2d_1/kernel', 'max_vi_t_block/window_attention/sequential_1/conv2d_1/bias', 'max_vi_t_block/window_attention/sequential_1/batch_normalization_1/gamma', 'max_vi_t_block/window_attention/sequential_1/batch_normalization_1/beta', 'max_vi_t_block_1/window_attention_1/sequential_3/conv2d_3/kernel', 'max_vi_t_block_1/window_attention_1/sequential_3/conv2d_3/bias', 'max_vi_t_block_1/window_attention_1/sequential_3/batch_normalization_3/gamma', 'max_vi_t_block_1/window_attention_1/sequential_3/batch_normalization_3/beta'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 121ms/step - accuracy: 0.6840 - loss: 0.6198 - val_accuracy: 0.5000 - val_loss: 1.2733 - learning_rate: 0.0050\n",
      "Epoch 2/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.7308 - loss: 0.5856 - val_accuracy: 0.5000 - val_loss: 0.9504 - learning_rate: 0.0050\n",
      "Epoch 3/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.7296 - loss: 0.5969 - val_accuracy: 0.5000 - val_loss: 1.1932 - learning_rate: 0.0050\n",
      "Epoch 4/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 105ms/step - accuracy: 0.7472 - loss: 0.5738 - val_accuracy: 0.5000 - val_loss: 1.2234 - learning_rate: 5.0000e-04\n",
      "\n",
      "Fold 5 Results:\n",
      "Fold: 5.0000\n",
      "K: 10.0000\n",
      "Test_Accuracy: 0.4800\n",
      "Train_Accuracy: 0.7583\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "AUC_ROC: 0.7746\n",
      "F1_Score: 0.0000\n",
      "Training_Loss: 0.5530\n",
      "Testing_Loss: 0.9594\n",
      "Time_Taken: 14.4211\n",
      "Epochs_Run: 4.0000\n",
      "Specificity: 1.0000\n",
      "MCC: 0.0000\n",
      "Log_Loss: 1.0349\n",
      "G_Mean: 0.0000\n",
      "Youdens_J: 0.0000\n",
      "Balanced_Accuracy: 0.5000\n",
      "Cohens_Kappa: 0.0000\n",
      "\n",
      "Fold 6/10\n",
      "Training data shape: (720, 74, 74, 3) (limited for CPU)\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/optimizers/base_optimizer.py:774: UserWarning: Gradients do not exist for variables ['max_vi_t_block/window_attention/sequential_1/conv2d_1/kernel', 'max_vi_t_block/window_attention/sequential_1/conv2d_1/bias', 'max_vi_t_block/window_attention/sequential_1/batch_normalization_1/gamma', 'max_vi_t_block/window_attention/sequential_1/batch_normalization_1/beta', 'max_vi_t_block_1/window_attention_1/sequential_3/conv2d_3/kernel', 'max_vi_t_block_1/window_attention_1/sequential_3/conv2d_3/bias', 'max_vi_t_block_1/window_attention_1/sequential_3/batch_normalization_3/gamma', 'max_vi_t_block_1/window_attention_1/sequential_3/batch_normalization_3/beta'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 116ms/step - accuracy: 0.6780 - loss: 0.6291 - val_accuracy: 0.5000 - val_loss: 0.7174 - learning_rate: 0.0050\n",
      "Epoch 2/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 119ms/step - accuracy: 0.7644 - loss: 0.5472 - val_accuracy: 0.5000 - val_loss: 0.7136 - learning_rate: 0.0050\n",
      "Epoch 3/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 111ms/step - accuracy: 0.7612 - loss: 0.5615 - val_accuracy: 0.5000 - val_loss: 0.7082 - learning_rate: 0.0050\n",
      "\n",
      "Fold 6 Results:\n",
      "Fold: 6.0000\n",
      "K: 10.0000\n",
      "Test_Accuracy: 0.4800\n",
      "Train_Accuracy: 0.7667\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "AUC_ROC: 0.7505\n",
      "F1_Score: 0.0000\n",
      "Training_Loss: 0.5476\n",
      "Testing_Loss: 0.7047\n",
      "Time_Taken: 12.2588\n",
      "Epochs_Run: 3.0000\n",
      "Specificity: 1.0000\n",
      "MCC: 0.0000\n",
      "Log_Loss: 0.7572\n",
      "G_Mean: 0.0000\n",
      "Youdens_J: 0.0000\n",
      "Balanced_Accuracy: 0.5000\n",
      "Cohens_Kappa: 0.0000\n",
      "\n",
      "Fold 7/10\n",
      "Training data shape: (720, 74, 74, 3) (limited for CPU)\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/optimizers/base_optimizer.py:774: UserWarning: Gradients do not exist for variables ['max_vi_t_block/window_attention/sequential_1/conv2d_1/kernel', 'max_vi_t_block/window_attention/sequential_1/conv2d_1/bias', 'max_vi_t_block/window_attention/sequential_1/batch_normalization_1/gamma', 'max_vi_t_block/window_attention/sequential_1/batch_normalization_1/beta', 'max_vi_t_block_1/window_attention_1/sequential_3/conv2d_3/kernel', 'max_vi_t_block_1/window_attention_1/sequential_3/conv2d_3/bias', 'max_vi_t_block_1/window_attention_1/sequential_3/batch_normalization_3/gamma', 'max_vi_t_block_1/window_attention_1/sequential_3/batch_normalization_3/beta'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 124ms/step - accuracy: 0.6764 - loss: 0.6358 - val_accuracy: 0.5750 - val_loss: 0.6135 - learning_rate: 0.0050\n",
      "Epoch 2/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 100ms/step - accuracy: 0.7652 - loss: 0.5512 - val_accuracy: 0.6250 - val_loss: 0.5752 - learning_rate: 0.0050\n",
      "Epoch 3/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 116ms/step - accuracy: 0.7388 - loss: 0.5658 - val_accuracy: 0.5750 - val_loss: 0.6138 - learning_rate: 0.0050\n",
      "Epoch 4/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 101ms/step - accuracy: 0.7357 - loss: 0.5593 - val_accuracy: 0.5750 - val_loss: 0.5732 - learning_rate: 5.0000e-04\n",
      "\n",
      "Fold 7 Results:\n",
      "Fold: 7.0000\n",
      "K: 10.0000\n",
      "Test_Accuracy: 0.4800\n",
      "Train_Accuracy: 0.7486\n",
      "Precision: 0.5000\n",
      "Recall: 0.0673\n",
      "AUC_ROC: 0.7297\n",
      "F1_Score: 0.1186\n",
      "Training_Loss: 0.5508\n",
      "Testing_Loss: 0.6376\n",
      "Time_Taken: 14.4940\n",
      "Epochs_Run: 4.0000\n",
      "Specificity: 0.9271\n",
      "MCC: -0.0110\n",
      "Log_Loss: 0.7376\n",
      "G_Mean: 0.2498\n",
      "Youdens_J: -0.0056\n",
      "Balanced_Accuracy: 0.4972\n",
      "Cohens_Kappa: -0.0054\n",
      "\n",
      "Fold 8/10\n",
      "Training data shape: (720, 74, 74, 3) (limited for CPU)\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/optimizers/base_optimizer.py:774: UserWarning: Gradients do not exist for variables ['max_vi_t_block/window_attention/sequential_1/conv2d_1/kernel', 'max_vi_t_block/window_attention/sequential_1/conv2d_1/bias', 'max_vi_t_block/window_attention/sequential_1/batch_normalization_1/gamma', 'max_vi_t_block/window_attention/sequential_1/batch_normalization_1/beta', 'max_vi_t_block_1/window_attention_1/sequential_3/conv2d_3/kernel', 'max_vi_t_block_1/window_attention_1/sequential_3/conv2d_3/bias', 'max_vi_t_block_1/window_attention_1/sequential_3/batch_normalization_3/gamma', 'max_vi_t_block_1/window_attention_1/sequential_3/batch_normalization_3/beta'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 114ms/step - accuracy: 0.6445 - loss: 0.6499 - val_accuracy: 0.5000 - val_loss: 0.8876 - learning_rate: 0.0050\n",
      "Epoch 2/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 102ms/step - accuracy: 0.7438 - loss: 0.5712 - val_accuracy: 0.6750 - val_loss: 0.6082 - learning_rate: 0.0050\n",
      "Epoch 3/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 118ms/step - accuracy: 0.7497 - loss: 0.5618 - val_accuracy: 0.5000 - val_loss: 0.7464 - learning_rate: 0.0050\n",
      "Epoch 4/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 103ms/step - accuracy: 0.7511 - loss: 0.5448 - val_accuracy: 0.5000 - val_loss: 0.8882 - learning_rate: 5.0000e-04\n",
      "\n",
      "Fold 8 Results:\n",
      "Fold: 8.0000\n",
      "K: 10.0000\n",
      "Test_Accuracy: 0.6550\n",
      "Train_Accuracy: 0.7542\n",
      "Precision: 0.7215\n",
      "Recall: 0.5481\n",
      "AUC_ROC: 0.7664\n",
      "F1_Score: 0.6230\n",
      "Training_Loss: 0.5490\n",
      "Testing_Loss: 0.6312\n",
      "Time_Taken: 14.3792\n",
      "Epochs_Run: 4.0000\n",
      "Specificity: 0.7708\n",
      "MCC: 0.3259\n",
      "Log_Loss: 0.7265\n",
      "G_Mean: 0.6500\n",
      "Youdens_J: 0.3189\n",
      "Balanced_Accuracy: 0.6595\n",
      "Cohens_Kappa: 0.3157\n",
      "\n",
      "Fold 9/10\n",
      "Training data shape: (720, 74, 74, 3) (limited for CPU)\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/optimizers/base_optimizer.py:774: UserWarning: Gradients do not exist for variables ['max_vi_t_block/window_attention/sequential_1/conv2d_1/kernel', 'max_vi_t_block/window_attention/sequential_1/conv2d_1/bias', 'max_vi_t_block/window_attention/sequential_1/batch_normalization_1/gamma', 'max_vi_t_block/window_attention/sequential_1/batch_normalization_1/beta', 'max_vi_t_block_1/window_attention_1/sequential_3/conv2d_3/kernel', 'max_vi_t_block_1/window_attention_1/sequential_3/conv2d_3/bias', 'max_vi_t_block_1/window_attention_1/sequential_3/batch_normalization_3/gamma', 'max_vi_t_block_1/window_attention_1/sequential_3/batch_normalization_3/beta'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 111ms/step - accuracy: 0.6742 - loss: 0.6363 - val_accuracy: 0.5125 - val_loss: 0.8325 - learning_rate: 0.0050\n",
      "Epoch 2/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 94ms/step - accuracy: 0.7517 - loss: 0.5875 - val_accuracy: 0.5125 - val_loss: 0.7551 - learning_rate: 0.0050\n",
      "Epoch 3/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 109ms/step - accuracy: 0.7388 - loss: 0.5826 - val_accuracy: 0.5125 - val_loss: 0.6878 - learning_rate: 0.0050\n",
      "Epoch 4/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 96ms/step - accuracy: 0.7445 - loss: 0.5529 - val_accuracy: 0.5875 - val_loss: 0.6521 - learning_rate: 0.0050\n",
      "Epoch 5/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 97ms/step - accuracy: 0.7672 - loss: 0.5468 - val_accuracy: 0.5125 - val_loss: 0.6816 - learning_rate: 0.0050\n",
      "Epoch 6/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 97ms/step - accuracy: 0.7628 - loss: 0.5375 - val_accuracy: 0.5125 - val_loss: 0.6544 - learning_rate: 5.0000e-04\n",
      "\n",
      "Fold 9 Results:\n",
      "Fold: 9.0000\n",
      "K: 10.0000\n",
      "Test_Accuracy: 0.5650\n",
      "Train_Accuracy: 0.7681\n",
      "Precision: 0.7429\n",
      "Recall: 0.2500\n",
      "AUC_ROC: 0.7689\n",
      "F1_Score: 0.3741\n",
      "Training_Loss: 0.5344\n",
      "Testing_Loss: 0.6439\n",
      "Time_Taken: 18.2519\n",
      "Epochs_Run: 6.0000\n",
      "Specificity: 0.9062\n",
      "MCC: 0.2054\n",
      "Log_Loss: 0.7161\n",
      "G_Mean: 0.4760\n",
      "Youdens_J: 0.1562\n",
      "Balanced_Accuracy: 0.5781\n",
      "Cohens_Kappa: 0.1520\n",
      "\n",
      "Fold 10/10\n",
      "Training data shape: (720, 74, 74, 3) (limited for CPU)\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/optimizers/base_optimizer.py:774: UserWarning: Gradients do not exist for variables ['max_vi_t_block/window_attention/sequential_1/conv2d_1/kernel', 'max_vi_t_block/window_attention/sequential_1/conv2d_1/bias', 'max_vi_t_block/window_attention/sequential_1/batch_normalization_1/gamma', 'max_vi_t_block/window_attention/sequential_1/batch_normalization_1/beta', 'max_vi_t_block_1/window_attention_1/sequential_3/conv2d_3/kernel', 'max_vi_t_block_1/window_attention_1/sequential_3/conv2d_3/bias', 'max_vi_t_block_1/window_attention_1/sequential_3/batch_normalization_3/gamma', 'max_vi_t_block_1/window_attention_1/sequential_3/batch_normalization_3/beta'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 122ms/step - accuracy: 0.6707 - loss: 0.6302 - val_accuracy: 0.5375 - val_loss: 0.6759 - learning_rate: 0.0050\n",
      "Epoch 2/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 92ms/step - accuracy: 0.7746 - loss: 0.5445 - val_accuracy: 0.4125 - val_loss: 0.8136 - learning_rate: 0.0050\n",
      "Epoch 3/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 102ms/step - accuracy: 0.7537 - loss: 0.5355 - val_accuracy: 0.4125 - val_loss: 0.8476 - learning_rate: 5.0000e-04\n",
      "\n",
      "Fold 10 Results:\n",
      "Fold: 10.0000\n",
      "K: 10.0000\n",
      "Test_Accuracy: 0.5600\n",
      "Train_Accuracy: 0.7569\n",
      "Precision: 0.7667\n",
      "Recall: 0.2212\n",
      "AUC_ROC: 0.7745\n",
      "F1_Score: 0.3433\n",
      "Training_Loss: 0.5359\n",
      "Testing_Loss: 0.6486\n",
      "Time_Taken: 11.6222\n",
      "Epochs_Run: 3.0000\n",
      "Specificity: 0.9271\n",
      "MCC: 0.2074\n",
      "Log_Loss: 0.8654\n",
      "G_Mean: 0.4528\n",
      "Youdens_J: 0.1482\n",
      "Balanced_Accuracy: 0.5741\n",
      "Cohens_Kappa: 0.1440\n",
      "\n",
      "=== Starting 15-fold cross-validation ===\n",
      "\n",
      "Fold 1/15\n",
      "Training data shape: (746, 74, 74, 3) (limited for CPU)\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/optimizers/base_optimizer.py:774: UserWarning: Gradients do not exist for variables ['max_vi_t_block/window_attention/sequential_1/conv2d_1/kernel', 'max_vi_t_block/window_attention/sequential_1/conv2d_1/bias', 'max_vi_t_block/window_attention/sequential_1/batch_normalization_1/gamma', 'max_vi_t_block/window_attention/sequential_1/batch_normalization_1/beta', 'max_vi_t_block_1/window_attention_1/sequential_3/conv2d_3/kernel', 'max_vi_t_block_1/window_attention_1/sequential_3/conv2d_3/bias', 'max_vi_t_block_1/window_attention_1/sequential_3/batch_normalization_3/gamma', 'max_vi_t_block_1/window_attention_1/sequential_3/batch_normalization_3/beta'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 107ms/step - accuracy: 0.7083 - loss: 0.6004 - val_accuracy: 0.5185 - val_loss: 0.7116 - learning_rate: 0.0050\n",
      "Epoch 2/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 106ms/step - accuracy: 0.7293 - loss: 0.5835 - val_accuracy: 0.7593 - val_loss: 0.6199 - learning_rate: 0.0050\n",
      "Epoch 3/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 94ms/step - accuracy: 0.7610 - loss: 0.5559 - val_accuracy: 0.5185 - val_loss: 0.6471 - learning_rate: 0.0050\n",
      "Epoch 4/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 96ms/step - accuracy: 0.7633 - loss: 0.5517 - val_accuracy: 0.5185 - val_loss: 0.6306 - learning_rate: 5.0000e-04\n",
      "\n",
      "Fold 1 Results:\n",
      "Fold: 1.0000\n",
      "K: 15.0000\n",
      "Test_Accuracy: 0.7250\n",
      "Train_Accuracy: 0.7547\n",
      "Precision: 0.6899\n",
      "Recall: 0.8558\n",
      "AUC_ROC: 0.7333\n",
      "F1_Score: 0.7639\n",
      "Training_Loss: 0.5657\n",
      "Testing_Loss: 0.6354\n",
      "Time_Taken: 14.0053\n",
      "Epochs_Run: 4.0000\n",
      "Specificity: 0.5833\n",
      "MCC: 0.4585\n",
      "Log_Loss: 0.7132\n",
      "G_Mean: 0.7065\n",
      "Youdens_J: 0.4391\n",
      "Balanced_Accuracy: 0.7196\n",
      "Cohens_Kappa: 0.4435\n",
      "\n",
      "Fold 2/15\n",
      "Training data shape: (746, 74, 74, 3) (limited for CPU)\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/optimizers/base_optimizer.py:774: UserWarning: Gradients do not exist for variables ['max_vi_t_block/window_attention/sequential_1/conv2d_1/kernel', 'max_vi_t_block/window_attention/sequential_1/conv2d_1/bias', 'max_vi_t_block/window_attention/sequential_1/batch_normalization_1/gamma', 'max_vi_t_block/window_attention/sequential_1/batch_normalization_1/beta', 'max_vi_t_block_1/window_attention_1/sequential_3/conv2d_3/kernel', 'max_vi_t_block_1/window_attention_1/sequential_3/conv2d_3/bias', 'max_vi_t_block_1/window_attention_1/sequential_3/batch_normalization_3/gamma', 'max_vi_t_block_1/window_attention_1/sequential_3/batch_normalization_3/beta'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 107ms/step - accuracy: 0.6567 - loss: 0.6359 - val_accuracy: 0.4815 - val_loss: 0.6828 - learning_rate: 0.0050\n",
      "Epoch 2/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 106ms/step - accuracy: 0.7503 - loss: 0.5691 - val_accuracy: 0.4815 - val_loss: 0.7539 - learning_rate: 0.0050\n",
      "Epoch 3/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 95ms/step - accuracy: 0.7695 - loss: 0.5448 - val_accuracy: 0.4815 - val_loss: 0.6841 - learning_rate: 5.0000e-04\n",
      "\n",
      "Fold 2 Results:\n",
      "Fold: 2.0000\n",
      "K: 15.0000\n",
      "Test_Accuracy: 0.4800\n",
      "Train_Accuracy: 0.7560\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "AUC_ROC: 0.6766\n",
      "F1_Score: 0.0000\n",
      "Training_Loss: 0.5488\n",
      "Testing_Loss: 0.6830\n",
      "Time_Taken: 11.7373\n",
      "Epochs_Run: 3.0000\n",
      "Specificity: 1.0000\n",
      "MCC: 0.0000\n",
      "Log_Loss: 0.7383\n",
      "G_Mean: 0.0000\n",
      "Youdens_J: 0.0000\n",
      "Balanced_Accuracy: 0.5000\n",
      "Cohens_Kappa: 0.0000\n",
      "\n",
      "Fold 3/15\n",
      "Training data shape: (746, 74, 74, 3) (limited for CPU)\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/optimizers/base_optimizer.py:774: UserWarning: Gradients do not exist for variables ['max_vi_t_block/window_attention/sequential_1/conv2d_1/kernel', 'max_vi_t_block/window_attention/sequential_1/conv2d_1/bias', 'max_vi_t_block/window_attention/sequential_1/batch_normalization_1/gamma', 'max_vi_t_block/window_attention/sequential_1/batch_normalization_1/beta', 'max_vi_t_block_1/window_attention_1/sequential_3/conv2d_3/kernel', 'max_vi_t_block_1/window_attention_1/sequential_3/conv2d_3/bias', 'max_vi_t_block_1/window_attention_1/sequential_3/batch_normalization_3/gamma', 'max_vi_t_block_1/window_attention_1/sequential_3/batch_normalization_3/beta'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 111ms/step - accuracy: 0.6897 - loss: 0.6218 - val_accuracy: 0.5556 - val_loss: 0.6546 - learning_rate: 0.0050\n",
      "Epoch 2/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 91ms/step - accuracy: 0.7413 - loss: 0.5513 - val_accuracy: 0.5556 - val_loss: 0.6520 - learning_rate: 0.0050\n",
      "Epoch 3/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 110ms/step - accuracy: 0.7677 - loss: 0.5683 - val_accuracy: 0.5556 - val_loss: 0.7617 - learning_rate: 0.0050\n",
      "\n",
      "Fold 3 Results:\n",
      "Fold: 3.0000\n",
      "K: 15.0000\n",
      "Test_Accuracy: 0.4800\n",
      "Train_Accuracy: 0.7507\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "AUC_ROC: 0.7451\n",
      "F1_Score: 0.0000\n",
      "Training_Loss: 0.5787\n",
      "Testing_Loss: 0.7274\n",
      "Time_Taken: 11.9290\n",
      "Epochs_Run: 3.0000\n",
      "Specificity: 1.0000\n",
      "MCC: 0.0000\n",
      "Log_Loss: 0.7734\n",
      "G_Mean: 0.0000\n",
      "Youdens_J: 0.0000\n",
      "Balanced_Accuracy: 0.5000\n",
      "Cohens_Kappa: 0.0000\n",
      "\n",
      "Fold 4/15\n",
      "Training data shape: (746, 74, 74, 3) (limited for CPU)\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/optimizers/base_optimizer.py:774: UserWarning: Gradients do not exist for variables ['max_vi_t_block/window_attention/sequential_1/conv2d_1/kernel', 'max_vi_t_block/window_attention/sequential_1/conv2d_1/bias', 'max_vi_t_block/window_attention/sequential_1/batch_normalization_1/gamma', 'max_vi_t_block/window_attention/sequential_1/batch_normalization_1/beta', 'max_vi_t_block_1/window_attention_1/sequential_3/conv2d_3/kernel', 'max_vi_t_block_1/window_attention_1/sequential_3/conv2d_3/bias', 'max_vi_t_block_1/window_attention_1/sequential_3/batch_normalization_3/gamma', 'max_vi_t_block_1/window_attention_1/sequential_3/batch_normalization_3/beta'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 113ms/step - accuracy: 0.6997 - loss: 0.6228 - val_accuracy: 0.5926 - val_loss: 0.9309 - learning_rate: 0.0050\n",
      "Epoch 2/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 102ms/step - accuracy: 0.7526 - loss: 0.5698 - val_accuracy: 0.5926 - val_loss: 0.8120 - learning_rate: 0.0050\n",
      "Epoch 3/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 105ms/step - accuracy: 0.7443 - loss: 0.5901 - val_accuracy: 0.5926 - val_loss: 0.7455 - learning_rate: 0.0050\n",
      "Epoch 4/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 95ms/step - accuracy: 0.7713 - loss: 0.5702 - val_accuracy: 0.5926 - val_loss: 0.6805 - learning_rate: 0.0050\n",
      "Epoch 5/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 95ms/step - accuracy: 0.7709 - loss: 0.5488 - val_accuracy: 0.5926 - val_loss: 0.6538 - learning_rate: 0.0050\n",
      "Epoch 6/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 94ms/step - accuracy: 0.7771 - loss: 0.5048 - val_accuracy: 0.6111 - val_loss: 0.6746 - learning_rate: 0.0050\n",
      "Epoch 7/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 102ms/step - accuracy: 0.7658 - loss: 0.5645 - val_accuracy: 0.6296 - val_loss: 0.6534 - learning_rate: 5.0000e-04\n",
      "\n",
      "Fold 4 Results:\n",
      "Fold: 4.0000\n",
      "K: 15.0000\n",
      "Test_Accuracy: 0.4800\n",
      "Train_Accuracy: 0.7708\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "AUC_ROC: 0.7609\n",
      "F1_Score: 0.0000\n",
      "Training_Loss: 0.5471\n",
      "Testing_Loss: 0.7017\n",
      "Time_Taken: 21.3354\n",
      "Epochs_Run: 7.0000\n",
      "Specificity: 1.0000\n",
      "MCC: 0.0000\n",
      "Log_Loss: 0.7969\n",
      "G_Mean: 0.0000\n",
      "Youdens_J: 0.0000\n",
      "Balanced_Accuracy: 0.5000\n",
      "Cohens_Kappa: 0.0000\n",
      "\n",
      "Fold 5/15\n",
      "Training data shape: (746, 74, 74, 3) (limited for CPU)\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/optimizers/base_optimizer.py:774: UserWarning: Gradients do not exist for variables ['max_vi_t_block/window_attention/sequential_1/conv2d_1/kernel', 'max_vi_t_block/window_attention/sequential_1/conv2d_1/bias', 'max_vi_t_block/window_attention/sequential_1/batch_normalization_1/gamma', 'max_vi_t_block/window_attention/sequential_1/batch_normalization_1/beta', 'max_vi_t_block_1/window_attention_1/sequential_3/conv2d_3/kernel', 'max_vi_t_block_1/window_attention_1/sequential_3/conv2d_3/bias', 'max_vi_t_block_1/window_attention_1/sequential_3/batch_normalization_3/gamma', 'max_vi_t_block_1/window_attention_1/sequential_3/batch_normalization_3/beta'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 103ms/step - accuracy: 0.6573 - loss: 0.6492 - val_accuracy: 0.5185 - val_loss: 0.7200 - learning_rate: 0.0050\n",
      "Epoch 2/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 92ms/step - accuracy: 0.7662 - loss: 0.5299 - val_accuracy: 0.7222 - val_loss: 0.5903 - learning_rate: 0.0050\n",
      "Epoch 3/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 96ms/step - accuracy: 0.7645 - loss: 0.5501 - val_accuracy: 0.5185 - val_loss: 0.6722 - learning_rate: 0.0050\n",
      "Epoch 4/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 96ms/step - accuracy: 0.7445 - loss: 0.5624 - val_accuracy: 0.5185 - val_loss: 0.7783 - learning_rate: 5.0000e-04\n",
      "\n",
      "Fold 5 Results:\n",
      "Fold: 5.0000\n",
      "K: 15.0000\n",
      "Test_Accuracy: 0.7300\n",
      "Train_Accuracy: 0.7641\n",
      "Precision: 0.6923\n",
      "Recall: 0.8654\n",
      "AUC_ROC: 0.7589\n",
      "F1_Score: 0.7692\n",
      "Training_Loss: 0.5418\n",
      "Testing_Loss: 0.5821\n",
      "Time_Taken: 13.6719\n",
      "Epochs_Run: 4.0000\n",
      "Specificity: 0.5833\n",
      "MCC: 0.4700\n",
      "Log_Loss: 0.7634\n",
      "G_Mean: 0.7105\n",
      "Youdens_J: 0.4487\n",
      "Balanced_Accuracy: 0.7244\n",
      "Cohens_Kappa: 0.4534\n",
      "\n",
      "Fold 6/15\n",
      "Training data shape: (747, 74, 74, 3) (limited for CPU)\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/optimizers/base_optimizer.py:774: UserWarning: Gradients do not exist for variables ['max_vi_t_block/window_attention/sequential_1/conv2d_1/kernel', 'max_vi_t_block/window_attention/sequential_1/conv2d_1/bias', 'max_vi_t_block/window_attention/sequential_1/batch_normalization_1/gamma', 'max_vi_t_block/window_attention/sequential_1/batch_normalization_1/beta', 'max_vi_t_block_1/window_attention_1/sequential_3/conv2d_3/kernel', 'max_vi_t_block_1/window_attention_1/sequential_3/conv2d_3/bias', 'max_vi_t_block_1/window_attention_1/sequential_3/batch_normalization_3/gamma', 'max_vi_t_block_1/window_attention_1/sequential_3/batch_normalization_3/beta'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 105ms/step - accuracy: 0.6668 - loss: 0.6432 - val_accuracy: 0.4151 - val_loss: 1.0587 - learning_rate: 0.0050\n",
      "Epoch 2/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 92ms/step - accuracy: 0.7325 - loss: 0.5849 - val_accuracy: 0.4151 - val_loss: 1.0256 - learning_rate: 0.0050\n",
      "Epoch 3/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 98ms/step - accuracy: 0.7806 - loss: 0.5396 - val_accuracy: 0.4151 - val_loss: 0.7460 - learning_rate: 0.0050\n",
      "Epoch 4/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 104ms/step - accuracy: 0.7498 - loss: 0.5436 - val_accuracy: 0.4151 - val_loss: 1.1752 - learning_rate: 0.0050\n",
      "Epoch 5/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 97ms/step - accuracy: 0.7743 - loss: 0.5481 - val_accuracy: 0.4151 - val_loss: 1.0669 - learning_rate: 5.0000e-04\n",
      "\n",
      "Fold 6 Results:\n",
      "Fold: 6.0000\n",
      "K: 15.0000\n",
      "Test_Accuracy: 0.4800\n",
      "Train_Accuracy: 0.7671\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "AUC_ROC: 0.7382\n",
      "F1_Score: 0.0000\n",
      "Training_Loss: 0.5412\n",
      "Testing_Loss: 0.7089\n",
      "Time_Taken: 16.2866\n",
      "Epochs_Run: 5.0000\n",
      "Specificity: 1.0000\n",
      "MCC: 0.0000\n",
      "Log_Loss: 0.7615\n",
      "G_Mean: 0.0000\n",
      "Youdens_J: 0.0000\n",
      "Balanced_Accuracy: 0.5000\n",
      "Cohens_Kappa: 0.0000\n",
      "\n",
      "Fold 7/15\n",
      "Training data shape: (747, 74, 74, 3) (limited for CPU)\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/optimizers/base_optimizer.py:774: UserWarning: Gradients do not exist for variables ['max_vi_t_block/window_attention/sequential_1/conv2d_1/kernel', 'max_vi_t_block/window_attention/sequential_1/conv2d_1/bias', 'max_vi_t_block/window_attention/sequential_1/batch_normalization_1/gamma', 'max_vi_t_block/window_attention/sequential_1/batch_normalization_1/beta', 'max_vi_t_block_1/window_attention_1/sequential_3/conv2d_3/kernel', 'max_vi_t_block_1/window_attention_1/sequential_3/conv2d_3/bias', 'max_vi_t_block_1/window_attention_1/sequential_3/batch_normalization_3/gamma', 'max_vi_t_block_1/window_attention_1/sequential_3/batch_normalization_3/beta'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 144ms/step - accuracy: 0.7116 - loss: 0.5972 - val_accuracy: 0.4906 - val_loss: 0.7548 - learning_rate: 0.0050\n",
      "Epoch 2/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 106ms/step - accuracy: 0.7427 - loss: 0.5879 - val_accuracy: 0.6038 - val_loss: 0.6800 - learning_rate: 0.0050\n",
      "Epoch 3/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 115ms/step - accuracy: 0.7494 - loss: 0.5813 - val_accuracy: 0.6038 - val_loss: 0.6825 - learning_rate: 0.0050\n",
      "Epoch 4/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 196ms/step - accuracy: 0.7552 - loss: 0.5478 - val_accuracy: 0.6415 - val_loss: 0.6727 - learning_rate: 5.0000e-04\n",
      "\n",
      "Fold 7 Results:\n",
      "Fold: 7.0000\n",
      "K: 15.0000\n",
      "Test_Accuracy: 0.6050\n",
      "Train_Accuracy: 0.7590\n",
      "Precision: 0.6471\n",
      "Recall: 0.5288\n",
      "AUC_ROC: 0.6624\n",
      "F1_Score: 0.5820\n",
      "Training_Loss: 0.5396\n",
      "Testing_Loss: 0.6471\n",
      "Time_Taken: 17.9833\n",
      "Epochs_Run: 4.0000\n",
      "Specificity: 0.6875\n",
      "MCC: 0.2186\n",
      "Log_Loss: 0.7269\n",
      "G_Mean: 0.6030\n",
      "Youdens_J: 0.2163\n",
      "Balanced_Accuracy: 0.6082\n",
      "Cohens_Kappa: 0.2147\n",
      "\n",
      "Fold 8/15\n",
      "Training data shape: (747, 74, 74, 3) (limited for CPU)\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/optimizers/base_optimizer.py:774: UserWarning: Gradients do not exist for variables ['max_vi_t_block/window_attention/sequential_1/conv2d_1/kernel', 'max_vi_t_block/window_attention/sequential_1/conv2d_1/bias', 'max_vi_t_block/window_attention/sequential_1/batch_normalization_1/gamma', 'max_vi_t_block/window_attention/sequential_1/batch_normalization_1/beta', 'max_vi_t_block_1/window_attention_1/sequential_3/conv2d_3/kernel', 'max_vi_t_block_1/window_attention_1/sequential_3/conv2d_3/bias', 'max_vi_t_block_1/window_attention_1/sequential_3/batch_normalization_3/gamma', 'max_vi_t_block_1/window_attention_1/sequential_3/batch_normalization_3/beta'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 110ms/step - accuracy: 0.6720 - loss: 0.6419 - val_accuracy: 0.5094 - val_loss: 0.9220 - learning_rate: 0.0050\n",
      "Epoch 2/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 96ms/step - accuracy: 0.7573 - loss: 0.5916 - val_accuracy: 0.5094 - val_loss: 0.9756 - learning_rate: 0.0050\n",
      "Epoch 3/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 116ms/step - accuracy: 0.7689 - loss: 0.5453 - val_accuracy: 0.5094 - val_loss: 0.9402 - learning_rate: 5.0000e-04\n",
      "\n",
      "Fold 8 Results:\n",
      "Fold: 8.0000\n",
      "K: 15.0000\n",
      "Test_Accuracy: 0.4800\n",
      "Train_Accuracy: 0.7577\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "AUC_ROC: 0.2592\n",
      "F1_Score: 0.0000\n",
      "Training_Loss: 0.5599\n",
      "Testing_Loss: 0.9780\n",
      "Time_Taken: 12.1687\n",
      "Epochs_Run: 3.0000\n",
      "Specificity: 1.0000\n",
      "MCC: 0.0000\n",
      "Log_Loss: 0.9453\n",
      "G_Mean: 0.0000\n",
      "Youdens_J: 0.0000\n",
      "Balanced_Accuracy: 0.5000\n",
      "Cohens_Kappa: 0.0000\n",
      "\n",
      "Fold 9/15\n",
      "Training data shape: (747, 74, 74, 3) (limited for CPU)\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/optimizers/base_optimizer.py:774: UserWarning: Gradients do not exist for variables ['max_vi_t_block/window_attention/sequential_1/conv2d_1/kernel', 'max_vi_t_block/window_attention/sequential_1/conv2d_1/bias', 'max_vi_t_block/window_attention/sequential_1/batch_normalization_1/gamma', 'max_vi_t_block/window_attention/sequential_1/batch_normalization_1/beta', 'max_vi_t_block_1/window_attention_1/sequential_3/conv2d_3/kernel', 'max_vi_t_block_1/window_attention_1/sequential_3/conv2d_3/bias', 'max_vi_t_block_1/window_attention_1/sequential_3/batch_normalization_3/gamma', 'max_vi_t_block_1/window_attention_1/sequential_3/batch_normalization_3/beta'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 126ms/step - accuracy: 0.6838 - loss: 0.6157 - val_accuracy: 0.5094 - val_loss: 0.8641 - learning_rate: 0.0050\n",
      "Epoch 2/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 113ms/step - accuracy: 0.7650 - loss: 0.5657 - val_accuracy: 0.5094 - val_loss: 0.7001 - learning_rate: 0.0050\n",
      "Epoch 3/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 108ms/step - accuracy: 0.7655 - loss: 0.5388 - val_accuracy: 0.7358 - val_loss: 0.6140 - learning_rate: 0.0050\n",
      "Epoch 4/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 110ms/step - accuracy: 0.7649 - loss: 0.5532 - val_accuracy: 0.5094 - val_loss: 0.7348 - learning_rate: 0.0050\n",
      "Epoch 5/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 102ms/step - accuracy: 0.7573 - loss: 0.5509 - val_accuracy: 0.5094 - val_loss: 0.7756 - learning_rate: 5.0000e-04\n",
      "\n",
      "Fold 9 Results:\n",
      "Fold: 9.0000\n",
      "K: 15.0000\n",
      "Test_Accuracy: 0.7650\n",
      "Train_Accuracy: 0.7537\n",
      "Precision: 0.7436\n",
      "Recall: 0.8365\n",
      "AUC_ROC: 0.7526\n",
      "F1_Score: 0.7873\n",
      "Training_Loss: 0.5482\n",
      "Testing_Loss: 0.6228\n",
      "Time_Taken: 17.8780\n",
      "Epochs_Run: 5.0000\n",
      "Specificity: 0.6875\n",
      "MCC: 0.5314\n",
      "Log_Loss: 0.7155\n",
      "G_Mean: 0.7584\n",
      "Youdens_J: 0.5240\n",
      "Balanced_Accuracy: 0.7620\n",
      "Cohens_Kappa: 0.5268\n",
      "\n",
      "Fold 10/15\n",
      "Training data shape: (747, 74, 74, 3) (limited for CPU)\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/optimizers/base_optimizer.py:774: UserWarning: Gradients do not exist for variables ['max_vi_t_block/window_attention/sequential_1/conv2d_1/kernel', 'max_vi_t_block/window_attention/sequential_1/conv2d_1/bias', 'max_vi_t_block/window_attention/sequential_1/batch_normalization_1/gamma', 'max_vi_t_block/window_attention/sequential_1/batch_normalization_1/beta', 'max_vi_t_block_1/window_attention_1/sequential_3/conv2d_3/kernel', 'max_vi_t_block_1/window_attention_1/sequential_3/conv2d_3/bias', 'max_vi_t_block_1/window_attention_1/sequential_3/batch_normalization_3/gamma', 'max_vi_t_block_1/window_attention_1/sequential_3/batch_normalization_3/beta'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 127ms/step - accuracy: 0.6772 - loss: 0.6259 - val_accuracy: 0.5283 - val_loss: 0.6101 - learning_rate: 0.0050\n",
      "Epoch 2/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 118ms/step - accuracy: 0.6457 - loss: 0.6267 - val_accuracy: 0.5660 - val_loss: 0.6123 - learning_rate: 0.0050\n",
      "Epoch 3/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 99ms/step - accuracy: 0.7138 - loss: 0.6103 - val_accuracy: 0.5849 - val_loss: 0.5641 - learning_rate: 5.0000e-04\n",
      "Epoch 4/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 113ms/step - accuracy: 0.7636 - loss: 0.5424 - val_accuracy: 0.8491 - val_loss: 0.5122 - learning_rate: 5.0000e-04\n",
      "Epoch 5/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 105ms/step - accuracy: 0.7472 - loss: 0.5639 - val_accuracy: 0.8491 - val_loss: 0.4885 - learning_rate: 5.0000e-04\n",
      "Epoch 6/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 103ms/step - accuracy: 0.7544 - loss: 0.5356 - val_accuracy: 0.8491 - val_loss: 0.4742 - learning_rate: 5.0000e-04\n",
      "Epoch 7/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 101ms/step - accuracy: 0.7744 - loss: 0.5245 - val_accuracy: 0.8491 - val_loss: 0.4796 - learning_rate: 5.0000e-04\n",
      "\n",
      "Fold 10 Results:\n",
      "Fold: 10.0000\n",
      "K: 15.0000\n",
      "Test_Accuracy: 0.7350\n",
      "Train_Accuracy: 0.7671\n",
      "Precision: 0.7008\n",
      "Recall: 0.8558\n",
      "AUC_ROC: 0.7532\n",
      "F1_Score: 0.7706\n",
      "Training_Loss: 0.5332\n",
      "Testing_Loss: 0.5815\n",
      "Time_Taken: 22.6987\n",
      "Epochs_Run: 7.0000\n",
      "Specificity: 0.6042\n",
      "MCC: 0.4773\n",
      "Log_Loss: 0.7770\n",
      "G_Mean: 0.7190\n",
      "Youdens_J: 0.4599\n",
      "Balanced_Accuracy: 0.7300\n",
      "Cohens_Kappa: 0.4642\n",
      "\n",
      "Fold 11/15\n",
      "Training data shape: (747, 74, 74, 3) (limited for CPU)\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/optimizers/base_optimizer.py:774: UserWarning: Gradients do not exist for variables ['max_vi_t_block/window_attention/sequential_1/conv2d_1/kernel', 'max_vi_t_block/window_attention/sequential_1/conv2d_1/bias', 'max_vi_t_block/window_attention/sequential_1/batch_normalization_1/gamma', 'max_vi_t_block/window_attention/sequential_1/batch_normalization_1/beta', 'max_vi_t_block_1/window_attention_1/sequential_3/conv2d_3/kernel', 'max_vi_t_block_1/window_attention_1/sequential_3/conv2d_3/bias', 'max_vi_t_block_1/window_attention_1/sequential_3/batch_normalization_3/gamma', 'max_vi_t_block_1/window_attention_1/sequential_3/batch_normalization_3/beta'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 108ms/step - accuracy: 0.6414 - loss: 0.6786 - val_accuracy: 0.4717 - val_loss: 0.7544 - learning_rate: 0.0050\n",
      "Epoch 2/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 93ms/step - accuracy: 0.7470 - loss: 0.5644 - val_accuracy: 0.4717 - val_loss: 1.1361 - learning_rate: 0.0050\n",
      "Epoch 3/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 97ms/step - accuracy: 0.7454 - loss: 0.5752 - val_accuracy: 0.4717 - val_loss: 0.9791 - learning_rate: 5.0000e-04\n",
      "\n",
      "Fold 11 Results:\n",
      "Fold: 11.0000\n",
      "K: 15.0000\n",
      "Test_Accuracy: 0.4800\n",
      "Train_Accuracy: 0.7470\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "AUC_ROC: 0.5450\n",
      "F1_Score: 0.0000\n",
      "Training_Loss: 0.5857\n",
      "Testing_Loss: 0.7403\n",
      "Time_Taken: 11.3959\n",
      "Epochs_Run: 3.0000\n",
      "Specificity: 1.0000\n",
      "MCC: 0.0000\n",
      "Log_Loss: 0.7627\n",
      "G_Mean: 0.0000\n",
      "Youdens_J: 0.0000\n",
      "Balanced_Accuracy: 0.5000\n",
      "Cohens_Kappa: 0.0000\n",
      "\n",
      "Fold 12/15\n",
      "Training data shape: (747, 74, 74, 3) (limited for CPU)\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/optimizers/base_optimizer.py:774: UserWarning: Gradients do not exist for variables ['max_vi_t_block/window_attention/sequential_1/conv2d_1/kernel', 'max_vi_t_block/window_attention/sequential_1/conv2d_1/bias', 'max_vi_t_block/window_attention/sequential_1/batch_normalization_1/gamma', 'max_vi_t_block/window_attention/sequential_1/batch_normalization_1/beta', 'max_vi_t_block_1/window_attention_1/sequential_3/conv2d_3/kernel', 'max_vi_t_block_1/window_attention_1/sequential_3/conv2d_3/bias', 'max_vi_t_block_1/window_attention_1/sequential_3/batch_normalization_3/gamma', 'max_vi_t_block_1/window_attention_1/sequential_3/batch_normalization_3/beta'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 107ms/step - accuracy: 0.6853 - loss: 0.6481 - val_accuracy: 0.6792 - val_loss: 0.6337 - learning_rate: 0.0050\n",
      "Epoch 2/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 100ms/step - accuracy: 0.7598 - loss: 0.5568 - val_accuracy: 0.6792 - val_loss: 0.5965 - learning_rate: 0.0050\n",
      "Epoch 3/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 97ms/step - accuracy: 0.7666 - loss: 0.5565 - val_accuracy: 0.6038 - val_loss: 0.6832 - learning_rate: 0.0050\n",
      "Epoch 4/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 98ms/step - accuracy: 0.7605 - loss: 0.5451 - val_accuracy: 0.6038 - val_loss: 0.7183 - learning_rate: 5.0000e-04\n",
      "\n",
      "Fold 12 Results:\n",
      "Fold: 12.0000\n",
      "K: 15.0000\n",
      "Test_Accuracy: 0.7300\n",
      "Train_Accuracy: 0.7617\n",
      "Precision: 0.7119\n",
      "Recall: 0.8077\n",
      "AUC_ROC: 0.7606\n",
      "F1_Score: 0.7568\n",
      "Training_Loss: 0.5411\n",
      "Testing_Loss: 0.6337\n",
      "Time_Taken: 13.9253\n",
      "Epochs_Run: 4.0000\n",
      "Specificity: 0.6458\n",
      "MCC: 0.4607\n",
      "Log_Loss: 0.7118\n",
      "G_Mean: 0.7222\n",
      "Youdens_J: 0.4535\n",
      "Balanced_Accuracy: 0.7268\n",
      "Cohens_Kappa: 0.4561\n",
      "\n",
      "Fold 13/15\n",
      "Training data shape: (747, 74, 74, 3) (limited for CPU)\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/optimizers/base_optimizer.py:774: UserWarning: Gradients do not exist for variables ['max_vi_t_block/window_attention/sequential_1/conv2d_1/kernel', 'max_vi_t_block/window_attention/sequential_1/conv2d_1/bias', 'max_vi_t_block/window_attention/sequential_1/batch_normalization_1/gamma', 'max_vi_t_block/window_attention/sequential_1/batch_normalization_1/beta', 'max_vi_t_block_1/window_attention_1/sequential_3/conv2d_3/kernel', 'max_vi_t_block_1/window_attention_1/sequential_3/conv2d_3/bias', 'max_vi_t_block_1/window_attention_1/sequential_3/batch_normalization_3/gamma', 'max_vi_t_block_1/window_attention_1/sequential_3/batch_normalization_3/beta'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 106ms/step - accuracy: 0.6781 - loss: 0.6520 - val_accuracy: 0.5472 - val_loss: 0.8902 - learning_rate: 0.0050\n",
      "Epoch 2/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 93ms/step - accuracy: 0.7312 - loss: 0.5758 - val_accuracy: 0.5472 - val_loss: 1.0139 - learning_rate: 0.0050\n",
      "Epoch 3/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 96ms/step - accuracy: 0.7736 - loss: 0.5355 - val_accuracy: 0.5472 - val_loss: 0.9504 - learning_rate: 5.0000e-04\n",
      "\n",
      "Fold 13 Results:\n",
      "Fold: 13.0000\n",
      "K: 15.0000\n",
      "Test_Accuracy: 0.4800\n",
      "Train_Accuracy: 0.7604\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "AUC_ROC: 0.6359\n",
      "F1_Score: 0.0000\n",
      "Training_Loss: 0.5534\n",
      "Testing_Loss: 0.9860\n",
      "Time_Taken: 11.3731\n",
      "Epochs_Run: 3.0000\n",
      "Specificity: 1.0000\n",
      "MCC: 0.0000\n",
      "Log_Loss: 1.0004\n",
      "G_Mean: 0.0000\n",
      "Youdens_J: 0.0000\n",
      "Balanced_Accuracy: 0.5000\n",
      "Cohens_Kappa: 0.0000\n",
      "\n",
      "Fold 14/15\n",
      "Training data shape: (747, 74, 74, 3) (limited for CPU)\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/optimizers/base_optimizer.py:774: UserWarning: Gradients do not exist for variables ['max_vi_t_block/window_attention/sequential_1/conv2d_1/kernel', 'max_vi_t_block/window_attention/sequential_1/conv2d_1/bias', 'max_vi_t_block/window_attention/sequential_1/batch_normalization_1/gamma', 'max_vi_t_block/window_attention/sequential_1/batch_normalization_1/beta', 'max_vi_t_block_1/window_attention_1/sequential_3/conv2d_3/kernel', 'max_vi_t_block_1/window_attention_1/sequential_3/conv2d_3/bias', 'max_vi_t_block_1/window_attention_1/sequential_3/batch_normalization_3/gamma', 'max_vi_t_block_1/window_attention_1/sequential_3/batch_normalization_3/beta'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 107ms/step - accuracy: 0.6958 - loss: 0.6282 - val_accuracy: 0.4340 - val_loss: 0.6811 - learning_rate: 0.0050\n",
      "Epoch 2/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 94ms/step - accuracy: 0.7203 - loss: 0.5649 - val_accuracy: 0.7736 - val_loss: 0.6389 - learning_rate: 0.0050\n",
      "Epoch 3/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 96ms/step - accuracy: 0.7736 - loss: 0.5511 - val_accuracy: 0.3585 - val_loss: 0.6744 - learning_rate: 0.0050\n",
      "Epoch 4/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 97ms/step - accuracy: 0.7709 - loss: 0.5415 - val_accuracy: 0.4717 - val_loss: 0.6666 - learning_rate: 5.0000e-04\n",
      "\n",
      "Fold 14 Results:\n",
      "Fold: 14.0000\n",
      "K: 15.0000\n",
      "Test_Accuracy: 0.7300\n",
      "Train_Accuracy: 0.7644\n",
      "Precision: 0.6923\n",
      "Recall: 0.8654\n",
      "AUC_ROC: 0.7547\n",
      "F1_Score: 0.7692\n",
      "Training_Loss: 0.5342\n",
      "Testing_Loss: 0.6404\n",
      "Time_Taken: 13.7554\n",
      "Epochs_Run: 4.0000\n",
      "Specificity: 0.5833\n",
      "MCC: 0.4700\n",
      "Log_Loss: 0.7053\n",
      "G_Mean: 0.7105\n",
      "Youdens_J: 0.4487\n",
      "Balanced_Accuracy: 0.7244\n",
      "Cohens_Kappa: 0.4534\n",
      "\n",
      "Fold 15/15\n",
      "Training data shape: (747, 74, 74, 3) (limited for CPU)\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/optimizers/base_optimizer.py:774: UserWarning: Gradients do not exist for variables ['max_vi_t_block/window_attention/sequential_1/conv2d_1/kernel', 'max_vi_t_block/window_attention/sequential_1/conv2d_1/bias', 'max_vi_t_block/window_attention/sequential_1/batch_normalization_1/gamma', 'max_vi_t_block/window_attention/sequential_1/batch_normalization_1/beta', 'max_vi_t_block_1/window_attention_1/sequential_3/conv2d_3/kernel', 'max_vi_t_block_1/window_attention_1/sequential_3/conv2d_3/bias', 'max_vi_t_block_1/window_attention_1/sequential_3/batch_normalization_3/gamma', 'max_vi_t_block_1/window_attention_1/sequential_3/batch_normalization_3/beta'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 120ms/step - accuracy: 0.6822 - loss: 0.6553 - val_accuracy: 0.4717 - val_loss: 0.6797 - learning_rate: 0.0050\n",
      "Epoch 2/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 93ms/step - accuracy: 0.7594 - loss: 0.5618 - val_accuracy: 0.5660 - val_loss: 0.6263 - learning_rate: 0.0050\n",
      "Epoch 3/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 97ms/step - accuracy: 0.7568 - loss: 0.5516 - val_accuracy: 0.4717 - val_loss: 0.6609 - learning_rate: 0.0050\n",
      "Epoch 4/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 98ms/step - accuracy: 0.7463 - loss: 0.5580 - val_accuracy: 0.4717 - val_loss: 0.6999 - learning_rate: 5.0000e-04\n",
      "\n",
      "Fold 15 Results:\n",
      "Fold: 15.0000\n",
      "K: 15.0000\n",
      "Test_Accuracy: 0.5500\n",
      "Train_Accuracy: 0.7550\n",
      "Precision: 0.9375\n",
      "Recall: 0.1442\n",
      "AUC_ROC: 0.7765\n",
      "F1_Score: 0.2500\n",
      "Training_Loss: 0.5556\n",
      "Testing_Loss: 0.6363\n",
      "Time_Taken: 14.0732\n",
      "Epochs_Run: 4.0000\n",
      "Specificity: 0.9896\n",
      "MCC: 0.2464\n",
      "Log_Loss: 0.7685\n",
      "G_Mean: 0.3778\n",
      "Youdens_J: 0.1338\n",
      "Balanced_Accuracy: 0.5669\n",
      "Cohens_Kappa: 0.1293\n",
      "\n",
      "=== Starting 20-fold cross-validation ===\n",
      "\n",
      "Fold 1/20\n",
      "Training data shape: (760, 74, 74, 3) (limited for CPU)\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/optimizers/base_optimizer.py:774: UserWarning: Gradients do not exist for variables ['max_vi_t_block/window_attention/sequential_1/conv2d_1/kernel', 'max_vi_t_block/window_attention/sequential_1/conv2d_1/bias', 'max_vi_t_block/window_attention/sequential_1/batch_normalization_1/gamma', 'max_vi_t_block/window_attention/sequential_1/batch_normalization_1/beta', 'max_vi_t_block_1/window_attention_1/sequential_3/conv2d_3/kernel', 'max_vi_t_block_1/window_attention_1/sequential_3/conv2d_3/bias', 'max_vi_t_block_1/window_attention_1/sequential_3/batch_normalization_3/gamma', 'max_vi_t_block_1/window_attention_1/sequential_3/batch_normalization_3/beta'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 110ms/step - accuracy: 0.6983 - loss: 0.5967 - val_accuracy: 0.5000 - val_loss: 0.9031 - learning_rate: 0.0050\n",
      "Epoch 2/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 96ms/step - accuracy: 0.7609 - loss: 0.5739 - val_accuracy: 0.5000 - val_loss: 0.7220 - learning_rate: 0.0050\n",
      "Epoch 3/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 97ms/step - accuracy: 0.7458 - loss: 0.5675 - val_accuracy: 0.5000 - val_loss: 0.7568 - learning_rate: 0.0050\n",
      "Epoch 4/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 98ms/step - accuracy: 0.7792 - loss: 0.5248 - val_accuracy: 0.5000 - val_loss: 0.7170 - learning_rate: 5.0000e-04\n",
      "\n",
      "Fold 1 Results:\n",
      "Fold: 1.0000\n",
      "K: 20.0000\n",
      "Test_Accuracy: 0.4800\n",
      "Train_Accuracy: 0.7645\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "AUC_ROC: 0.6658\n",
      "F1_Score: 0.0000\n",
      "Training_Loss: 0.5374\n",
      "Testing_Loss: 0.7315\n",
      "Time_Taken: 13.9137\n",
      "Epochs_Run: 4.0000\n",
      "Specificity: 1.0000\n",
      "MCC: 0.0000\n",
      "Log_Loss: 0.7675\n",
      "G_Mean: 0.0000\n",
      "Youdens_J: 0.0000\n",
      "Balanced_Accuracy: 0.5000\n",
      "Cohens_Kappa: 0.0000\n",
      "\n",
      "Fold 2/20\n",
      "Training data shape: (760, 74, 74, 3) (limited for CPU)\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/optimizers/base_optimizer.py:774: UserWarning: Gradients do not exist for variables ['max_vi_t_block/window_attention/sequential_1/conv2d_1/kernel', 'max_vi_t_block/window_attention/sequential_1/conv2d_1/bias', 'max_vi_t_block/window_attention/sequential_1/batch_normalization_1/gamma', 'max_vi_t_block/window_attention/sequential_1/batch_normalization_1/beta', 'max_vi_t_block_1/window_attention_1/sequential_3/conv2d_3/kernel', 'max_vi_t_block_1/window_attention_1/sequential_3/conv2d_3/bias', 'max_vi_t_block_1/window_attention_1/sequential_3/batch_normalization_3/gamma', 'max_vi_t_block_1/window_attention_1/sequential_3/batch_normalization_3/beta'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 110ms/step - accuracy: 0.6681 - loss: 0.6388 - val_accuracy: 0.5250 - val_loss: 0.8394 - learning_rate: 0.0050\n",
      "Epoch 2/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 102ms/step - accuracy: 0.7733 - loss: 0.5254 - val_accuracy: 0.5250 - val_loss: 0.7736 - learning_rate: 0.0050\n",
      "Epoch 3/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 97ms/step - accuracy: 0.7772 - loss: 0.5229 - val_accuracy: 0.5250 - val_loss: 0.7155 - learning_rate: 0.0050\n",
      "Epoch 4/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 98ms/step - accuracy: 0.7332 - loss: 0.5543 - val_accuracy: 0.5250 - val_loss: 0.7420 - learning_rate: 0.0050\n",
      "Epoch 5/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 113ms/step - accuracy: 0.7777 - loss: 0.5225 - val_accuracy: 0.5250 - val_loss: 0.7499 - learning_rate: 5.0000e-04\n",
      "\n",
      "Fold 2 Results:\n",
      "Fold: 2.0000\n",
      "K: 20.0000\n",
      "Test_Accuracy: 0.4800\n",
      "Train_Accuracy: 0.7592\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "AUC_ROC: 0.7085\n",
      "F1_Score: 0.0000\n",
      "Training_Loss: 0.5482\n",
      "Testing_Loss: 0.7610\n",
      "Time_Taken: 16.7538\n",
      "Epochs_Run: 5.0000\n",
      "Specificity: 1.0000\n",
      "MCC: 0.0000\n",
      "Log_Loss: 0.8195\n",
      "G_Mean: 0.0000\n",
      "Youdens_J: 0.0000\n",
      "Balanced_Accuracy: 0.5000\n",
      "Cohens_Kappa: 0.0000\n",
      "\n",
      "Fold 3/20\n",
      "Training data shape: (760, 74, 74, 3) (limited for CPU)\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/optimizers/base_optimizer.py:774: UserWarning: Gradients do not exist for variables ['max_vi_t_block/window_attention/sequential_1/conv2d_1/kernel', 'max_vi_t_block/window_attention/sequential_1/conv2d_1/bias', 'max_vi_t_block/window_attention/sequential_1/batch_normalization_1/gamma', 'max_vi_t_block/window_attention/sequential_1/batch_normalization_1/beta', 'max_vi_t_block_1/window_attention_1/sequential_3/conv2d_3/kernel', 'max_vi_t_block_1/window_attention_1/sequential_3/conv2d_3/bias', 'max_vi_t_block_1/window_attention_1/sequential_3/batch_normalization_3/gamma', 'max_vi_t_block_1/window_attention_1/sequential_3/batch_normalization_3/beta'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 109ms/step - accuracy: 0.6832 - loss: 0.6414 - val_accuracy: 0.5250 - val_loss: 0.6901 - learning_rate: 0.0050\n",
      "Epoch 2/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 94ms/step - accuracy: 0.7223 - loss: 0.5854 - val_accuracy: 0.5250 - val_loss: 0.6730 - learning_rate: 0.0050\n",
      "Epoch 3/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 106ms/step - accuracy: 0.7318 - loss: 0.5916 - val_accuracy: 0.5250 - val_loss: 0.7481 - learning_rate: 0.0050\n",
      "\n",
      "Fold 3 Results:\n",
      "Fold: 3.0000\n",
      "K: 20.0000\n",
      "Test_Accuracy: 0.4800\n",
      "Train_Accuracy: 0.7605\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "AUC_ROC: 0.6836\n",
      "F1_Score: 0.0000\n",
      "Training_Loss: 0.5578\n",
      "Testing_Loss: 0.6989\n",
      "Time_Taken: 11.7136\n",
      "Epochs_Run: 3.0000\n",
      "Specificity: 1.0000\n",
      "MCC: 0.0000\n",
      "Log_Loss: 0.7065\n",
      "G_Mean: 0.0000\n",
      "Youdens_J: 0.0000\n",
      "Balanced_Accuracy: 0.5000\n",
      "Cohens_Kappa: 0.0000\n",
      "\n",
      "Fold 4/20\n",
      "Training data shape: (760, 74, 74, 3) (limited for CPU)\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/optimizers/base_optimizer.py:774: UserWarning: Gradients do not exist for variables ['max_vi_t_block/window_attention/sequential_1/conv2d_1/kernel', 'max_vi_t_block/window_attention/sequential_1/conv2d_1/bias', 'max_vi_t_block/window_attention/sequential_1/batch_normalization_1/gamma', 'max_vi_t_block/window_attention/sequential_1/batch_normalization_1/beta', 'max_vi_t_block_1/window_attention_1/sequential_3/conv2d_3/kernel', 'max_vi_t_block_1/window_attention_1/sequential_3/conv2d_3/bias', 'max_vi_t_block_1/window_attention_1/sequential_3/batch_normalization_3/gamma', 'max_vi_t_block_1/window_attention_1/sequential_3/batch_normalization_3/beta'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 108ms/step - accuracy: 0.7022 - loss: 0.6515 - val_accuracy: 0.5250 - val_loss: 0.7234 - learning_rate: 0.0050\n",
      "Epoch 2/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 94ms/step - accuracy: 0.7696 - loss: 0.5521 - val_accuracy: 0.5250 - val_loss: 0.6979 - learning_rate: 0.0050\n",
      "Epoch 3/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 98ms/step - accuracy: 0.7476 - loss: 0.5707 - val_accuracy: 0.7000 - val_loss: 0.6250 - learning_rate: 0.0050\n",
      "Epoch 4/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 109ms/step - accuracy: 0.7712 - loss: 0.5505 - val_accuracy: 0.5250 - val_loss: 0.6844 - learning_rate: 0.0050\n",
      "Epoch 5/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 97ms/step - accuracy: 0.7495 - loss: 0.5427 - val_accuracy: 0.5250 - val_loss: 0.6926 - learning_rate: 5.0000e-04\n",
      "\n",
      "Fold 4 Results:\n",
      "Fold: 4.0000\n",
      "K: 20.0000\n",
      "Test_Accuracy: 0.6950\n",
      "Train_Accuracy: 0.7553\n",
      "Precision: 0.6483\n",
      "Recall: 0.9038\n",
      "AUC_ROC: 0.7592\n",
      "F1_Score: 0.7550\n",
      "Training_Loss: 0.5397\n",
      "Testing_Loss: 0.6184\n",
      "Time_Taken: 16.5499\n",
      "Epochs_Run: 5.0000\n",
      "Specificity: 0.4688\n",
      "MCC: 0.4169\n",
      "Log_Loss: 0.7180\n",
      "G_Mean: 0.6509\n",
      "Youdens_J: 0.3726\n",
      "Balanced_Accuracy: 0.6863\n",
      "Cohens_Kappa: 0.3788\n",
      "\n",
      "Fold 5/20\n",
      "Training data shape: (760, 74, 74, 3) (limited for CPU)\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/optimizers/base_optimizer.py:774: UserWarning: Gradients do not exist for variables ['max_vi_t_block/window_attention/sequential_1/conv2d_1/kernel', 'max_vi_t_block/window_attention/sequential_1/conv2d_1/bias', 'max_vi_t_block/window_attention/sequential_1/batch_normalization_1/gamma', 'max_vi_t_block/window_attention/sequential_1/batch_normalization_1/beta', 'max_vi_t_block_1/window_attention_1/sequential_3/conv2d_3/kernel', 'max_vi_t_block_1/window_attention_1/sequential_3/conv2d_3/bias', 'max_vi_t_block_1/window_attention_1/sequential_3/batch_normalization_3/gamma', 'max_vi_t_block_1/window_attention_1/sequential_3/batch_normalization_3/beta'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 108ms/step - accuracy: 0.7353 - loss: 0.5923 - val_accuracy: 0.4250 - val_loss: 0.7186 - learning_rate: 0.0050\n",
      "Epoch 2/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 111ms/step - accuracy: 0.7301 - loss: 0.5911 - val_accuracy: 0.4500 - val_loss: 0.6917 - learning_rate: 0.0050\n",
      "Epoch 3/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 100ms/step - accuracy: 0.7746 - loss: 0.5469 - val_accuracy: 0.6500 - val_loss: 0.6540 - learning_rate: 0.0050\n",
      "Epoch 4/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 98ms/step - accuracy: 0.7335 - loss: 0.5828 - val_accuracy: 0.5750 - val_loss: 0.6573 - learning_rate: 0.0050\n",
      "Epoch 5/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 96ms/step - accuracy: 0.7731 - loss: 0.5217 - val_accuracy: 0.5750 - val_loss: 0.6911 - learning_rate: 5.0000e-04\n",
      "\n",
      "Fold 5 Results:\n",
      "Fold: 5.0000\n",
      "K: 20.0000\n",
      "Test_Accuracy: 0.7450\n",
      "Train_Accuracy: 0.7684\n",
      "Precision: 0.7154\n",
      "Recall: 0.8462\n",
      "AUC_ROC: 0.7658\n",
      "F1_Score: 0.7753\n",
      "Training_Loss: 0.5299\n",
      "Testing_Loss: 0.6078\n",
      "Time_Taken: 16.6650\n",
      "Epochs_Run: 5.0000\n",
      "Specificity: 0.6354\n",
      "MCC: 0.4944\n",
      "Log_Loss: 0.7371\n",
      "G_Mean: 0.7333\n",
      "Youdens_J: 0.4816\n",
      "Balanced_Accuracy: 0.7408\n",
      "Cohens_Kappa: 0.4853\n",
      "\n",
      "Fold 6/20\n",
      "Training data shape: (760, 74, 74, 3) (limited for CPU)\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/optimizers/base_optimizer.py:774: UserWarning: Gradients do not exist for variables ['max_vi_t_block/window_attention/sequential_1/conv2d_1/kernel', 'max_vi_t_block/window_attention/sequential_1/conv2d_1/bias', 'max_vi_t_block/window_attention/sequential_1/batch_normalization_1/gamma', 'max_vi_t_block/window_attention/sequential_1/batch_normalization_1/beta', 'max_vi_t_block_1/window_attention_1/sequential_3/conv2d_3/kernel', 'max_vi_t_block_1/window_attention_1/sequential_3/conv2d_3/bias', 'max_vi_t_block_1/window_attention_1/sequential_3/batch_normalization_3/gamma', 'max_vi_t_block_1/window_attention_1/sequential_3/batch_normalization_3/beta'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 122ms/step - accuracy: 0.6723 - loss: 0.6344 - val_accuracy: 0.4250 - val_loss: 0.7681 - learning_rate: 0.0050\n",
      "Epoch 2/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 93ms/step - accuracy: 0.7419 - loss: 0.5807 - val_accuracy: 0.5250 - val_loss: 0.6880 - learning_rate: 0.0050\n",
      "Epoch 3/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 98ms/step - accuracy: 0.7502 - loss: 0.5604 - val_accuracy: 0.5750 - val_loss: 0.6672 - learning_rate: 0.0050\n",
      "Epoch 4/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 96ms/step - accuracy: 0.7672 - loss: 0.5444 - val_accuracy: 0.7750 - val_loss: 0.6492 - learning_rate: 0.0050\n",
      "Epoch 5/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 98ms/step - accuracy: 0.7332 - loss: 0.5661 - val_accuracy: 0.8000 - val_loss: 0.5678 - learning_rate: 0.0050\n",
      "Epoch 6/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 97ms/step - accuracy: 0.7710 - loss: 0.5385 - val_accuracy: 0.6750 - val_loss: 0.6327 - learning_rate: 0.0050\n",
      "Epoch 7/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 109ms/step - accuracy: 0.7531 - loss: 0.5397 - val_accuracy: 0.8000 - val_loss: 0.6013 - learning_rate: 5.0000e-04\n",
      "\n",
      "Fold 6 Results:\n",
      "Fold: 6.0000\n",
      "K: 20.0000\n",
      "Test_Accuracy: 0.7200\n",
      "Train_Accuracy: 0.7671\n",
      "Precision: 0.6846\n",
      "Recall: 0.8558\n",
      "AUC_ROC: 0.7911\n",
      "F1_Score: 0.7607\n",
      "Training_Loss: 0.5228\n",
      "Testing_Loss: 0.5888\n",
      "Time_Taken: 21.4342\n",
      "Epochs_Run: 7.0000\n",
      "Specificity: 0.5729\n",
      "MCC: 0.4490\n",
      "Log_Loss: 0.7365\n",
      "G_Mean: 0.7002\n",
      "Youdens_J: 0.4287\n",
      "Balanced_Accuracy: 0.7143\n",
      "Cohens_Kappa: 0.4332\n",
      "\n",
      "Fold 7/20\n",
      "Training data shape: (760, 74, 74, 3) (limited for CPU)\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/optimizers/base_optimizer.py:774: UserWarning: Gradients do not exist for variables ['max_vi_t_block/window_attention/sequential_1/conv2d_1/kernel', 'max_vi_t_block/window_attention/sequential_1/conv2d_1/bias', 'max_vi_t_block/window_attention/sequential_1/batch_normalization_1/gamma', 'max_vi_t_block/window_attention/sequential_1/batch_normalization_1/beta', 'max_vi_t_block_1/window_attention_1/sequential_3/conv2d_3/kernel', 'max_vi_t_block_1/window_attention_1/sequential_3/conv2d_3/bias', 'max_vi_t_block_1/window_attention_1/sequential_3/batch_normalization_3/gamma', 'max_vi_t_block_1/window_attention_1/sequential_3/batch_normalization_3/beta'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 108ms/step - accuracy: 0.6682 - loss: 0.6456 - val_accuracy: 0.4500 - val_loss: 0.7711 - learning_rate: 0.0050\n",
      "Epoch 2/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 95ms/step - accuracy: 0.7613 - loss: 0.5694 - val_accuracy: 0.4500 - val_loss: 0.8383 - learning_rate: 0.0050\n",
      "Epoch 3/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 98ms/step - accuracy: 0.7474 - loss: 0.5448 - val_accuracy: 0.4500 - val_loss: 0.8124 - learning_rate: 5.0000e-04\n",
      "\n",
      "Fold 7 Results:\n",
      "Fold: 7.0000\n",
      "K: 20.0000\n",
      "Test_Accuracy: 0.4800\n",
      "Train_Accuracy: 0.7618\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "AUC_ROC: 0.7659\n",
      "F1_Score: 0.0000\n",
      "Training_Loss: 0.5356\n",
      "Testing_Loss: 0.7302\n",
      "Time_Taken: 11.5616\n",
      "Epochs_Run: 3.0000\n",
      "Specificity: 1.0000\n",
      "MCC: 0.0000\n",
      "Log_Loss: 0.8069\n",
      "G_Mean: 0.0000\n",
      "Youdens_J: 0.0000\n",
      "Balanced_Accuracy: 0.5000\n",
      "Cohens_Kappa: 0.0000\n",
      "\n",
      "Fold 8/20\n",
      "Training data shape: (760, 74, 74, 3) (limited for CPU)\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/optimizers/base_optimizer.py:774: UserWarning: Gradients do not exist for variables ['max_vi_t_block/window_attention/sequential_1/conv2d_1/kernel', 'max_vi_t_block/window_attention/sequential_1/conv2d_1/bias', 'max_vi_t_block/window_attention/sequential_1/batch_normalization_1/gamma', 'max_vi_t_block/window_attention/sequential_1/batch_normalization_1/beta', 'max_vi_t_block_1/window_attention_1/sequential_3/conv2d_3/kernel', 'max_vi_t_block_1/window_attention_1/sequential_3/conv2d_3/bias', 'max_vi_t_block_1/window_attention_1/sequential_3/batch_normalization_3/gamma', 'max_vi_t_block_1/window_attention_1/sequential_3/batch_normalization_3/beta'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 109ms/step - accuracy: 0.7078 - loss: 0.6034 - val_accuracy: 0.6000 - val_loss: 0.6570 - learning_rate: 0.0050\n",
      "Epoch 2/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 93ms/step - accuracy: 0.7655 - loss: 0.5379 - val_accuracy: 0.4250 - val_loss: 0.7190 - learning_rate: 0.0050\n",
      "Epoch 3/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 97ms/step - accuracy: 0.7184 - loss: 0.5747 - val_accuracy: 0.4250 - val_loss: 0.7163 - learning_rate: 5.0000e-04\n",
      "\n",
      "Fold 8 Results:\n",
      "Fold: 8.0000\n",
      "K: 20.0000\n",
      "Test_Accuracy: 0.6300\n",
      "Train_Accuracy: 0.7487\n",
      "Precision: 0.5893\n",
      "Recall: 0.9519\n",
      "AUC_ROC: 0.7735\n",
      "F1_Score: 0.7279\n",
      "Training_Loss: 0.5603\n",
      "Testing_Loss: 0.6501\n",
      "Time_Taken: 11.4946\n",
      "Epochs_Run: 3.0000\n",
      "Specificity: 0.2812\n",
      "MCC: 0.3178\n",
      "Log_Loss: 0.7083\n",
      "G_Mean: 0.5174\n",
      "Youdens_J: 0.2332\n",
      "Balanced_Accuracy: 0.6166\n",
      "Cohens_Kappa: 0.2393\n",
      "\n",
      "Fold 9/20\n",
      "Training data shape: (760, 74, 74, 3) (limited for CPU)\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/optimizers/base_optimizer.py:774: UserWarning: Gradients do not exist for variables ['max_vi_t_block/window_attention/sequential_1/conv2d_1/kernel', 'max_vi_t_block/window_attention/sequential_1/conv2d_1/bias', 'max_vi_t_block/window_attention/sequential_1/batch_normalization_1/gamma', 'max_vi_t_block/window_attention/sequential_1/batch_normalization_1/beta', 'max_vi_t_block_1/window_attention_1/sequential_3/conv2d_3/kernel', 'max_vi_t_block_1/window_attention_1/sequential_3/conv2d_3/bias', 'max_vi_t_block_1/window_attention_1/sequential_3/batch_normalization_3/gamma', 'max_vi_t_block_1/window_attention_1/sequential_3/batch_normalization_3/beta'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 108ms/step - accuracy: 0.6704 - loss: 0.6554 - val_accuracy: 0.4750 - val_loss: 0.9482 - learning_rate: 0.0050\n",
      "Epoch 2/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 93ms/step - accuracy: 0.7462 - loss: 0.5510 - val_accuracy: 0.4750 - val_loss: 0.7496 - learning_rate: 0.0050\n",
      "Epoch 3/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 98ms/step - accuracy: 0.7603 - loss: 0.5330 - val_accuracy: 0.5250 - val_loss: 0.6645 - learning_rate: 0.0050\n",
      "Epoch 4/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 101ms/step - accuracy: 0.7391 - loss: 0.5729 - val_accuracy: 0.5250 - val_loss: 0.6666 - learning_rate: 0.0050\n",
      "Epoch 5/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 106ms/step - accuracy: 0.7681 - loss: 0.5362 - val_accuracy: 0.6500 - val_loss: 0.6571 - learning_rate: 5.0000e-04\n",
      "\n",
      "Fold 9 Results:\n",
      "Fold: 9.0000\n",
      "K: 20.0000\n",
      "Test_Accuracy: 0.5150\n",
      "Train_Accuracy: 0.7618\n",
      "Precision: 0.7692\n",
      "Recall: 0.0962\n",
      "AUC_ROC: 0.7603\n",
      "F1_Score: 0.1709\n",
      "Training_Loss: 0.5422\n",
      "Testing_Loss: 0.6358\n",
      "Time_Taken: 16.4842\n",
      "Epochs_Run: 5.0000\n",
      "Specificity: 0.9688\n",
      "MCC: 0.1315\n",
      "Log_Loss: 0.7470\n",
      "G_Mean: 0.3052\n",
      "Youdens_J: 0.0649\n",
      "Balanced_Accuracy: 0.5325\n",
      "Cohens_Kappa: 0.0626\n",
      "\n",
      "Fold 10/20\n",
      "Training data shape: (760, 74, 74, 3) (limited for CPU)\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/optimizers/base_optimizer.py:774: UserWarning: Gradients do not exist for variables ['max_vi_t_block/window_attention/sequential_1/conv2d_1/kernel', 'max_vi_t_block/window_attention/sequential_1/conv2d_1/bias', 'max_vi_t_block/window_attention/sequential_1/batch_normalization_1/gamma', 'max_vi_t_block/window_attention/sequential_1/batch_normalization_1/beta', 'max_vi_t_block_1/window_attention_1/sequential_3/conv2d_3/kernel', 'max_vi_t_block_1/window_attention_1/sequential_3/conv2d_3/bias', 'max_vi_t_block_1/window_attention_1/sequential_3/batch_normalization_3/gamma', 'max_vi_t_block_1/window_attention_1/sequential_3/batch_normalization_3/beta'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 109ms/step - accuracy: 0.6790 - loss: 0.6563 - val_accuracy: 0.5250 - val_loss: 0.6738 - learning_rate: 0.0050\n",
      "Epoch 2/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 94ms/step - accuracy: 0.7301 - loss: 0.6086 - val_accuracy: 0.5500 - val_loss: 0.6544 - learning_rate: 0.0050\n",
      "Epoch 3/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 111ms/step - accuracy: 0.7341 - loss: 0.5584 - val_accuracy: 0.7000 - val_loss: 0.6480 - learning_rate: 0.0050\n",
      "Epoch 4/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 97ms/step - accuracy: 0.7376 - loss: 0.5681 - val_accuracy: 0.6750 - val_loss: 0.6490 - learning_rate: 0.0050\n",
      "Epoch 5/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 97ms/step - accuracy: 0.7535 - loss: 0.5454 - val_accuracy: 0.6750 - val_loss: 0.6446 - learning_rate: 5.0000e-04\n",
      "\n",
      "Fold 10 Results:\n",
      "Fold: 10.0000\n",
      "K: 20.0000\n",
      "Test_Accuracy: 0.7500\n",
      "Train_Accuracy: 0.7645\n",
      "Precision: 0.7455\n",
      "Recall: 0.7885\n",
      "AUC_ROC: 0.7764\n",
      "F1_Score: 0.7664\n",
      "Training_Loss: 0.5336\n",
      "Testing_Loss: 0.6004\n",
      "Time_Taken: 16.5094\n",
      "Epochs_Run: 5.0000\n",
      "Specificity: 0.7083\n",
      "MCC: 0.4989\n",
      "Log_Loss: 0.7834\n",
      "G_Mean: 0.7473\n",
      "Youdens_J: 0.4968\n",
      "Balanced_Accuracy: 0.7484\n",
      "Cohens_Kappa: 0.4980\n",
      "\n",
      "Fold 11/20\n",
      "Training data shape: (760, 74, 74, 3) (limited for CPU)\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/optimizers/base_optimizer.py:774: UserWarning: Gradients do not exist for variables ['max_vi_t_block/window_attention/sequential_1/conv2d_1/kernel', 'max_vi_t_block/window_attention/sequential_1/conv2d_1/bias', 'max_vi_t_block/window_attention/sequential_1/batch_normalization_1/gamma', 'max_vi_t_block/window_attention/sequential_1/batch_normalization_1/beta', 'max_vi_t_block_1/window_attention_1/sequential_3/conv2d_3/kernel', 'max_vi_t_block_1/window_attention_1/sequential_3/conv2d_3/bias', 'max_vi_t_block_1/window_attention_1/sequential_3/batch_normalization_3/gamma', 'max_vi_t_block_1/window_attention_1/sequential_3/batch_normalization_3/beta'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 125ms/step - accuracy: 0.6506 - loss: 0.6554 - val_accuracy: 0.5500 - val_loss: 1.0674 - learning_rate: 0.0050\n",
      "Epoch 2/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 93ms/step - accuracy: 0.7618 - loss: 0.5593 - val_accuracy: 0.5500 - val_loss: 0.9392 - learning_rate: 0.0050\n",
      "Epoch 3/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 97ms/step - accuracy: 0.7574 - loss: 0.5358 - val_accuracy: 0.5500 - val_loss: 0.7589 - learning_rate: 0.0050\n",
      "Epoch 4/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 98ms/step - accuracy: 0.7487 - loss: 0.5610 - val_accuracy: 0.5500 - val_loss: 0.7115 - learning_rate: 0.0050\n",
      "Epoch 5/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 97ms/step - accuracy: 0.7630 - loss: 0.5596 - val_accuracy: 0.6000 - val_loss: 0.6662 - learning_rate: 0.0050\n",
      "Epoch 6/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 97ms/step - accuracy: 0.7806 - loss: 0.5211 - val_accuracy: 0.5500 - val_loss: 0.6843 - learning_rate: 0.0050\n",
      "Epoch 7/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 98ms/step - accuracy: 0.7557 - loss: 0.5307 - val_accuracy: 0.5500 - val_loss: 0.7134 - learning_rate: 5.0000e-04\n",
      "\n",
      "Fold 11 Results:\n",
      "Fold: 11.0000\n",
      "K: 20.0000\n",
      "Test_Accuracy: 0.4850\n",
      "Train_Accuracy: 0.7697\n",
      "Precision: 1.0000\n",
      "Recall: 0.0096\n",
      "AUC_ROC: 0.7827\n",
      "F1_Score: 0.0190\n",
      "Training_Loss: 0.5196\n",
      "Testing_Loss: 0.6538\n",
      "Time_Taken: 21.2725\n",
      "Epochs_Run: 7.0000\n",
      "Specificity: 1.0000\n",
      "MCC: 0.0681\n",
      "Log_Loss: 0.7565\n",
      "G_Mean: 0.0981\n",
      "Youdens_J: 0.0096\n",
      "Balanced_Accuracy: 0.5048\n",
      "Cohens_Kappa: 0.0092\n",
      "\n",
      "Fold 12/20\n",
      "Training data shape: (760, 74, 74, 3) (limited for CPU)\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/optimizers/base_optimizer.py:774: UserWarning: Gradients do not exist for variables ['max_vi_t_block/window_attention/sequential_1/conv2d_1/kernel', 'max_vi_t_block/window_attention/sequential_1/conv2d_1/bias', 'max_vi_t_block/window_attention/sequential_1/batch_normalization_1/gamma', 'max_vi_t_block/window_attention/sequential_1/batch_normalization_1/beta', 'max_vi_t_block_1/window_attention_1/sequential_3/conv2d_3/kernel', 'max_vi_t_block_1/window_attention_1/sequential_3/conv2d_3/bias', 'max_vi_t_block_1/window_attention_1/sequential_3/batch_normalization_3/gamma', 'max_vi_t_block_1/window_attention_1/sequential_3/batch_normalization_3/beta'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 111ms/step - accuracy: 0.6771 - loss: 0.6272 - val_accuracy: 0.4500 - val_loss: 0.8398 - learning_rate: 0.0050\n",
      "Epoch 2/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 93ms/step - accuracy: 0.7549 - loss: 0.5948 - val_accuracy: 0.4500 - val_loss: 0.6919 - learning_rate: 0.0050\n",
      "Epoch 3/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 98ms/step - accuracy: 0.7582 - loss: 0.5642 - val_accuracy: 0.4500 - val_loss: 0.7338 - learning_rate: 0.0050\n",
      "Epoch 4/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 111ms/step - accuracy: 0.7767 - loss: 0.5207 - val_accuracy: 0.4500 - val_loss: 0.7233 - learning_rate: 5.0000e-04\n",
      "\n",
      "Fold 12 Results:\n",
      "Fold: 12.0000\n",
      "K: 20.0000\n",
      "Test_Accuracy: 0.4800\n",
      "Train_Accuracy: 0.7539\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "AUC_ROC: 0.7537\n",
      "F1_Score: 0.0000\n",
      "Training_Loss: 0.5445\n",
      "Testing_Loss: 0.6557\n",
      "Time_Taken: 14.2429\n",
      "Epochs_Run: 4.0000\n",
      "Specificity: 1.0000\n",
      "MCC: 0.0000\n",
      "Log_Loss: 0.7794\n",
      "G_Mean: 0.0000\n",
      "Youdens_J: 0.0000\n",
      "Balanced_Accuracy: 0.5000\n",
      "Cohens_Kappa: 0.0000\n",
      "\n",
      "Fold 13/20\n",
      "Training data shape: (760, 74, 74, 3) (limited for CPU)\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/optimizers/base_optimizer.py:774: UserWarning: Gradients do not exist for variables ['max_vi_t_block/window_attention/sequential_1/conv2d_1/kernel', 'max_vi_t_block/window_attention/sequential_1/conv2d_1/bias', 'max_vi_t_block/window_attention/sequential_1/batch_normalization_1/gamma', 'max_vi_t_block/window_attention/sequential_1/batch_normalization_1/beta', 'max_vi_t_block_1/window_attention_1/sequential_3/conv2d_3/kernel', 'max_vi_t_block_1/window_attention_1/sequential_3/conv2d_3/bias', 'max_vi_t_block_1/window_attention_1/sequential_3/batch_normalization_3/gamma', 'max_vi_t_block_1/window_attention_1/sequential_3/batch_normalization_3/beta'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 108ms/step - accuracy: 0.6727 - loss: 0.6277 - val_accuracy: 0.6750 - val_loss: 0.5759 - learning_rate: 0.0050\n",
      "Epoch 2/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 94ms/step - accuracy: 0.7473 - loss: 0.5706 - val_accuracy: 0.6750 - val_loss: 0.5856 - learning_rate: 0.0050\n",
      "Epoch 3/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 111ms/step - accuracy: 0.7540 - loss: 0.5592 - val_accuracy: 0.6000 - val_loss: 0.6164 - learning_rate: 5.0000e-04\n",
      "\n",
      "Fold 13 Results:\n",
      "Fold: 13.0000\n",
      "K: 20.0000\n",
      "Test_Accuracy: 0.6300\n",
      "Train_Accuracy: 0.7566\n",
      "Precision: 0.5904\n",
      "Recall: 0.9423\n",
      "AUC_ROC: 0.7518\n",
      "F1_Score: 0.7259\n",
      "Training_Loss: 0.5475\n",
      "Testing_Loss: 0.6472\n",
      "Time_Taken: 11.8952\n",
      "Epochs_Run: 3.0000\n",
      "Specificity: 0.2917\n",
      "MCC: 0.3112\n",
      "Log_Loss: 0.7658\n",
      "G_Mean: 0.5243\n",
      "Youdens_J: 0.2340\n",
      "Balanced_Accuracy: 0.6170\n",
      "Cohens_Kappa: 0.2399\n",
      "\n",
      "Fold 14/20\n",
      "Training data shape: (760, 74, 74, 3) (limited for CPU)\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/optimizers/base_optimizer.py:774: UserWarning: Gradients do not exist for variables ['max_vi_t_block/window_attention/sequential_1/conv2d_1/kernel', 'max_vi_t_block/window_attention/sequential_1/conv2d_1/bias', 'max_vi_t_block/window_attention/sequential_1/batch_normalization_1/gamma', 'max_vi_t_block/window_attention/sequential_1/batch_normalization_1/beta', 'max_vi_t_block_1/window_attention_1/sequential_3/conv2d_3/kernel', 'max_vi_t_block_1/window_attention_1/sequential_3/conv2d_3/bias', 'max_vi_t_block_1/window_attention_1/sequential_3/batch_normalization_3/gamma', 'max_vi_t_block_1/window_attention_1/sequential_3/batch_normalization_3/beta'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 110ms/step - accuracy: 0.6654 - loss: 0.6722 - val_accuracy: 0.6000 - val_loss: 0.7557 - learning_rate: 0.0050\n",
      "Epoch 2/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 95ms/step - accuracy: 0.7570 - loss: 0.5530 - val_accuracy: 0.6000 - val_loss: 0.6982 - learning_rate: 0.0050\n",
      "Epoch 3/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 108ms/step - accuracy: 0.7544 - loss: 0.5624 - val_accuracy: 0.6000 - val_loss: 0.6603 - learning_rate: 0.0050\n",
      "Epoch 4/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 102ms/step - accuracy: 0.7526 - loss: 0.5305 - val_accuracy: 0.6000 - val_loss: 0.6405 - learning_rate: 0.0050\n",
      "Epoch 5/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 96ms/step - accuracy: 0.7408 - loss: 0.5557 - val_accuracy: 0.6000 - val_loss: 0.6905 - learning_rate: 0.0050\n",
      "\n",
      "Fold 14 Results:\n",
      "Fold: 14.0000\n",
      "K: 20.0000\n",
      "Test_Accuracy: 0.4800\n",
      "Train_Accuracy: 0.7566\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "AUC_ROC: 0.7146\n",
      "F1_Score: 0.0000\n",
      "Training_Loss: 0.5441\n",
      "Testing_Loss: 0.7089\n",
      "Time_Taken: 16.6293\n",
      "Epochs_Run: 5.0000\n",
      "Specificity: 1.0000\n",
      "MCC: 0.0000\n",
      "Log_Loss: 0.7592\n",
      "G_Mean: 0.0000\n",
      "Youdens_J: 0.0000\n",
      "Balanced_Accuracy: 0.5000\n",
      "Cohens_Kappa: 0.0000\n",
      "\n",
      "Fold 15/20\n",
      "Training data shape: (760, 74, 74, 3) (limited for CPU)\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/optimizers/base_optimizer.py:774: UserWarning: Gradients do not exist for variables ['max_vi_t_block/window_attention/sequential_1/conv2d_1/kernel', 'max_vi_t_block/window_attention/sequential_1/conv2d_1/bias', 'max_vi_t_block/window_attention/sequential_1/batch_normalization_1/gamma', 'max_vi_t_block/window_attention/sequential_1/batch_normalization_1/beta', 'max_vi_t_block_1/window_attention_1/sequential_3/conv2d_3/kernel', 'max_vi_t_block_1/window_attention_1/sequential_3/conv2d_3/bias', 'max_vi_t_block_1/window_attention_1/sequential_3/batch_normalization_3/gamma', 'max_vi_t_block_1/window_attention_1/sequential_3/batch_normalization_3/beta'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 115ms/step - accuracy: 0.6752 - loss: 0.6348 - val_accuracy: 0.3750 - val_loss: 1.0273 - learning_rate: 0.0050\n",
      "Epoch 2/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 101ms/step - accuracy: 0.7285 - loss: 0.5903 - val_accuracy: 0.3750 - val_loss: 1.0414 - learning_rate: 0.0050\n",
      "Epoch 3/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 97ms/step - accuracy: 0.7523 - loss: 0.5518 - val_accuracy: 0.3750 - val_loss: 0.9265 - learning_rate: 5.0000e-04\n",
      "Epoch 4/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 97ms/step - accuracy: 0.7278 - loss: 0.5741 - val_accuracy: 0.3750 - val_loss: 0.8424 - learning_rate: 5.0000e-04\n",
      "Epoch 5/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 97ms/step - accuracy: 0.7473 - loss: 0.5606 - val_accuracy: 0.3750 - val_loss: 0.7961 - learning_rate: 5.0000e-04\n",
      "Epoch 6/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 97ms/step - accuracy: 0.7396 - loss: 0.5677 - val_accuracy: 0.3750 - val_loss: 0.7598 - learning_rate: 5.0000e-04\n",
      "Epoch 7/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 97ms/step - accuracy: 0.7479 - loss: 0.5321 - val_accuracy: 0.3750 - val_loss: 0.7552 - learning_rate: 5.0000e-04\n",
      "Epoch 8/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 111ms/step - accuracy: 0.7428 - loss: 0.5424 - val_accuracy: 0.3750 - val_loss: 0.7654 - learning_rate: 5.0000e-04\n",
      "\n",
      "Fold 15 Results:\n",
      "Fold: 15.0000\n",
      "K: 20.0000\n",
      "Test_Accuracy: 0.4800\n",
      "Train_Accuracy: 0.7579\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "AUC_ROC: 0.7401\n",
      "F1_Score: 0.0000\n",
      "Training_Loss: 0.5345\n",
      "Testing_Loss: 0.7218\n",
      "Time_Taken: 23.7912\n",
      "Epochs_Run: 8.0000\n",
      "Specificity: 1.0000\n",
      "MCC: 0.0000\n",
      "Log_Loss: 0.9448\n",
      "G_Mean: 0.0000\n",
      "Youdens_J: 0.0000\n",
      "Balanced_Accuracy: 0.5000\n",
      "Cohens_Kappa: 0.0000\n",
      "\n",
      "Fold 16/20\n",
      "Training data shape: (760, 74, 74, 3) (limited for CPU)\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/optimizers/base_optimizer.py:774: UserWarning: Gradients do not exist for variables ['max_vi_t_block/window_attention/sequential_1/conv2d_1/kernel', 'max_vi_t_block/window_attention/sequential_1/conv2d_1/bias', 'max_vi_t_block/window_attention/sequential_1/batch_normalization_1/gamma', 'max_vi_t_block/window_attention/sequential_1/batch_normalization_1/beta', 'max_vi_t_block_1/window_attention_1/sequential_3/conv2d_3/kernel', 'max_vi_t_block_1/window_attention_1/sequential_3/conv2d_3/bias', 'max_vi_t_block_1/window_attention_1/sequential_3/batch_normalization_3/gamma', 'max_vi_t_block_1/window_attention_1/sequential_3/batch_normalization_3/beta'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 109ms/step - accuracy: 0.6931 - loss: 0.5936 - val_accuracy: 0.7000 - val_loss: 0.6268 - learning_rate: 0.0050\n",
      "Epoch 2/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 94ms/step - accuracy: 0.7498 - loss: 0.5778 - val_accuracy: 0.6250 - val_loss: 0.5461 - learning_rate: 0.0050\n",
      "Epoch 3/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 109ms/step - accuracy: 0.7773 - loss: 0.5322 - val_accuracy: 0.6250 - val_loss: 0.6165 - learning_rate: 0.0050\n",
      "Epoch 4/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 98ms/step - accuracy: 0.7813 - loss: 0.5237 - val_accuracy: 0.6250 - val_loss: 0.6303 - learning_rate: 5.0000e-04\n",
      "\n",
      "Fold 16 Results:\n",
      "Fold: 16.0000\n",
      "K: 20.0000\n",
      "Test_Accuracy: 0.4800\n",
      "Train_Accuracy: 0.7632\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "AUC_ROC: 0.7690\n",
      "F1_Score: 0.0000\n",
      "Training_Loss: 0.5441\n",
      "Testing_Loss: 0.6610\n",
      "Time_Taken: 14.1929\n",
      "Epochs_Run: 4.0000\n",
      "Specificity: 1.0000\n",
      "MCC: 0.0000\n",
      "Log_Loss: 0.8065\n",
      "G_Mean: 0.0000\n",
      "Youdens_J: 0.0000\n",
      "Balanced_Accuracy: 0.5000\n",
      "Cohens_Kappa: 0.0000\n",
      "\n",
      "Fold 17/20\n",
      "Training data shape: (760, 74, 74, 3) (limited for CPU)\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/optimizers/base_optimizer.py:774: UserWarning: Gradients do not exist for variables ['max_vi_t_block/window_attention/sequential_1/conv2d_1/kernel', 'max_vi_t_block/window_attention/sequential_1/conv2d_1/bias', 'max_vi_t_block/window_attention/sequential_1/batch_normalization_1/gamma', 'max_vi_t_block/window_attention/sequential_1/batch_normalization_1/beta', 'max_vi_t_block_1/window_attention_1/sequential_3/conv2d_3/kernel', 'max_vi_t_block_1/window_attention_1/sequential_3/conv2d_3/bias', 'max_vi_t_block_1/window_attention_1/sequential_3/batch_normalization_3/gamma', 'max_vi_t_block_1/window_attention_1/sequential_3/batch_normalization_3/beta'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 109ms/step - accuracy: 0.6480 - loss: 0.6618 - val_accuracy: 0.5250 - val_loss: 0.7111 - learning_rate: 0.0050\n",
      "Epoch 2/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 104ms/step - accuracy: 0.7450 - loss: 0.5649 - val_accuracy: 0.5250 - val_loss: 0.6821 - learning_rate: 0.0050\n",
      "Epoch 3/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 97ms/step - accuracy: 0.7956 - loss: 0.5205 - val_accuracy: 0.6750 - val_loss: 0.6484 - learning_rate: 0.0050\n",
      "Epoch 4/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 95ms/step - accuracy: 0.7497 - loss: 0.5732 - val_accuracy: 0.6250 - val_loss: 0.6363 - learning_rate: 0.0050\n",
      "Epoch 5/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 101ms/step - accuracy: 0.7711 - loss: 0.5304 - val_accuracy: 0.6250 - val_loss: 0.6439 - learning_rate: 0.0050\n",
      "\n",
      "Fold 17 Results:\n",
      "Fold: 17.0000\n",
      "K: 20.0000\n",
      "Test_Accuracy: 0.7350\n",
      "Train_Accuracy: 0.7645\n",
      "Precision: 0.7040\n",
      "Recall: 0.8462\n",
      "AUC_ROC: 0.7401\n",
      "F1_Score: 0.7686\n",
      "Training_Loss: 0.5341\n",
      "Testing_Loss: 0.6200\n",
      "Time_Taken: 16.5157\n",
      "Epochs_Run: 5.0000\n",
      "Specificity: 0.6146\n",
      "MCC: 0.4755\n",
      "Log_Loss: 0.7246\n",
      "G_Mean: 0.7211\n",
      "Youdens_J: 0.4607\n",
      "Balanced_Accuracy: 0.7304\n",
      "Cohens_Kappa: 0.4646\n",
      "\n",
      "Fold 18/20\n",
      "Training data shape: (760, 74, 74, 3) (limited for CPU)\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/optimizers/base_optimizer.py:774: UserWarning: Gradients do not exist for variables ['max_vi_t_block/window_attention/sequential_1/conv2d_1/kernel', 'max_vi_t_block/window_attention/sequential_1/conv2d_1/bias', 'max_vi_t_block/window_attention/sequential_1/batch_normalization_1/gamma', 'max_vi_t_block/window_attention/sequential_1/batch_normalization_1/beta', 'max_vi_t_block_1/window_attention_1/sequential_3/conv2d_3/kernel', 'max_vi_t_block_1/window_attention_1/sequential_3/conv2d_3/bias', 'max_vi_t_block_1/window_attention_1/sequential_3/batch_normalization_3/gamma', 'max_vi_t_block_1/window_attention_1/sequential_3/batch_normalization_3/beta'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 112ms/step - accuracy: 0.7133 - loss: 0.6093 - val_accuracy: 0.5000 - val_loss: 0.8259 - learning_rate: 0.0050\n",
      "Epoch 2/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 93ms/step - accuracy: 0.7581 - loss: 0.5668 - val_accuracy: 0.5000 - val_loss: 0.9201 - learning_rate: 0.0050\n",
      "Epoch 3/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 97ms/step - accuracy: 0.7575 - loss: 0.5509 - val_accuracy: 0.5000 - val_loss: 0.9788 - learning_rate: 5.0000e-04\n",
      "\n",
      "Fold 18 Results:\n",
      "Fold: 18.0000\n",
      "K: 20.0000\n",
      "Test_Accuracy: 0.4800\n",
      "Train_Accuracy: 0.7566\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "AUC_ROC: 0.5184\n",
      "F1_Score: 0.0000\n",
      "Training_Loss: 0.5468\n",
      "Testing_Loss: 0.8537\n",
      "Time_Taken: 11.6699\n",
      "Epochs_Run: 3.0000\n",
      "Specificity: 1.0000\n",
      "MCC: 0.0000\n",
      "Log_Loss: 0.8585\n",
      "G_Mean: 0.0000\n",
      "Youdens_J: 0.0000\n",
      "Balanced_Accuracy: 0.5000\n",
      "Cohens_Kappa: 0.0000\n",
      "\n",
      "Fold 19/20\n",
      "Training data shape: (760, 74, 74, 3) (limited for CPU)\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/optimizers/base_optimizer.py:774: UserWarning: Gradients do not exist for variables ['max_vi_t_block/window_attention/sequential_1/conv2d_1/kernel', 'max_vi_t_block/window_attention/sequential_1/conv2d_1/bias', 'max_vi_t_block/window_attention/sequential_1/batch_normalization_1/gamma', 'max_vi_t_block/window_attention/sequential_1/batch_normalization_1/beta', 'max_vi_t_block_1/window_attention_1/sequential_3/conv2d_3/kernel', 'max_vi_t_block_1/window_attention_1/sequential_3/conv2d_3/bias', 'max_vi_t_block_1/window_attention_1/sequential_3/batch_normalization_3/gamma', 'max_vi_t_block_1/window_attention_1/sequential_3/batch_normalization_3/beta'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 118ms/step - accuracy: 0.7134 - loss: 0.5932 - val_accuracy: 0.2750 - val_loss: 1.2593 - learning_rate: 0.0050\n",
      "Epoch 2/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 95ms/step - accuracy: 0.7016 - loss: 0.6111 - val_accuracy: 0.2750 - val_loss: 1.1276 - learning_rate: 0.0050\n",
      "Epoch 3/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 97ms/step - accuracy: 0.7508 - loss: 0.5675 - val_accuracy: 0.2750 - val_loss: 0.9860 - learning_rate: 0.0050\n",
      "Epoch 4/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 97ms/step - accuracy: 0.7770 - loss: 0.5257 - val_accuracy: 0.2750 - val_loss: 0.8416 - learning_rate: 0.0050\n",
      "Epoch 5/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 98ms/step - accuracy: 0.7522 - loss: 0.5281 - val_accuracy: 0.2750 - val_loss: 0.7170 - learning_rate: 0.0050\n",
      "Epoch 6/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 98ms/step - accuracy: 0.7443 - loss: 0.5469 - val_accuracy: 0.2750 - val_loss: 0.9039 - learning_rate: 0.0050\n",
      "Epoch 7/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 112ms/step - accuracy: 0.7373 - loss: 0.5598 - val_accuracy: 0.2750 - val_loss: 0.9292 - learning_rate: 5.0000e-04\n",
      "\n",
      "Fold 19 Results:\n",
      "Fold: 19.0000\n",
      "K: 20.0000\n",
      "Test_Accuracy: 0.4800\n",
      "Train_Accuracy: 0.7592\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "AUC_ROC: 0.7202\n",
      "F1_Score: 0.0000\n",
      "Training_Loss: 0.5305\n",
      "Testing_Loss: 0.6459\n",
      "Time_Taken: 21.5781\n",
      "Epochs_Run: 7.0000\n",
      "Specificity: 1.0000\n",
      "MCC: 0.0000\n",
      "Log_Loss: 0.7639\n",
      "G_Mean: 0.0000\n",
      "Youdens_J: 0.0000\n",
      "Balanced_Accuracy: 0.5000\n",
      "Cohens_Kappa: 0.0000\n",
      "\n",
      "Fold 20/20\n",
      "Training data shape: (760, 74, 74, 3) (limited for CPU)\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/optimizers/base_optimizer.py:774: UserWarning: Gradients do not exist for variables ['max_vi_t_block/window_attention/sequential_1/conv2d_1/kernel', 'max_vi_t_block/window_attention/sequential_1/conv2d_1/bias', 'max_vi_t_block/window_attention/sequential_1/batch_normalization_1/gamma', 'max_vi_t_block/window_attention/sequential_1/batch_normalization_1/beta', 'max_vi_t_block_1/window_attention_1/sequential_3/conv2d_3/kernel', 'max_vi_t_block_1/window_attention_1/sequential_3/conv2d_3/bias', 'max_vi_t_block_1/window_attention_1/sequential_3/batch_normalization_3/gamma', 'max_vi_t_block_1/window_attention_1/sequential_3/batch_normalization_3/beta'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 114ms/step - accuracy: 0.6610 - loss: 0.6353 - val_accuracy: 0.5500 - val_loss: 0.6628 - learning_rate: 0.0050\n",
      "Epoch 2/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 93ms/step - accuracy: 0.7527 - loss: 0.5754 - val_accuracy: 0.7500 - val_loss: 0.6152 - learning_rate: 0.0050\n",
      "Epoch 3/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 110ms/step - accuracy: 0.7589 - loss: 0.5536 - val_accuracy: 0.7750 - val_loss: 0.6056 - learning_rate: 0.0050\n",
      "Epoch 4/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 98ms/step - accuracy: 0.7609 - loss: 0.5415 - val_accuracy: 0.6000 - val_loss: 0.6198 - learning_rate: 0.0050\n",
      "\n",
      "Fold 20 Results:\n",
      "Fold: 20.0000\n",
      "K: 20.0000\n",
      "Test_Accuracy: 0.7150\n",
      "Train_Accuracy: 0.7618\n",
      "Precision: 0.6767\n",
      "Recall: 0.8654\n",
      "AUC_ROC: 0.7458\n",
      "F1_Score: 0.7595\n",
      "Training_Loss: 0.5429\n",
      "Testing_Loss: 0.6143\n",
      "Time_Taken: 14.3171\n",
      "Epochs_Run: 4.0000\n",
      "Specificity: 0.5521\n",
      "MCC: 0.4419\n",
      "Log_Loss: 0.7284\n",
      "G_Mean: 0.6912\n",
      "Youdens_J: 0.4175\n",
      "Balanced_Accuracy: 0.7087\n",
      "Cohens_Kappa: 0.4224\n",
      "\n",
      "=== Summary Statistics ===\n",
      "\n",
      "K=5 Cross-Validation Results:\n",
      "Average Test Accuracy: 0.5440 ± 0.1083\n",
      "Average F1 Score: 0.2930 ± 0.4021\n",
      "Average AUC-ROC: 0.7244 ± 0.0690\n",
      "Average Training Time: 12.42 seconds\n",
      "Average Epochs Run: 3.8\n",
      "\n",
      "K=10 Cross-Validation Results:\n",
      "Average Test Accuracy: 0.5875 ± 0.1101\n",
      "Average F1 Score: 0.3757 ± 0.3348\n",
      "Average AUC-ROC: 0.7552 ± 0.0232\n",
      "Average Training Time: 14.05 seconds\n",
      "Average Epochs Run: 3.9\n",
      "\n",
      "K=15 Cross-Validation Results:\n",
      "Average Test Accuracy: 0.5953 ± 0.1239\n",
      "Average F1 Score: 0.3633 ± 0.3756\n",
      "Average AUC-ROC: 0.6875 ± 0.1342\n",
      "Average Training Time: 14.95 seconds\n",
      "Average Epochs Run: 4.2\n",
      "\n",
      "K=20 Cross-Validation Results:\n",
      "Average Test Accuracy: 0.5710 ± 0.1142\n",
      "Average F1 Score: 0.3115 ± 0.3735\n",
      "Average AUC-ROC: 0.7343 ± 0.0604\n",
      "Average Training Time: 15.96 seconds\n",
      "Average Epochs Run: 4.8\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model, Input\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import (\n",
    "    f1_score, confusion_matrix, log_loss, matthews_corrcoef,\n",
    "    balanced_accuracy_score, cohen_kappa_score, roc_auc_score\n",
    ")\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime\n",
    "import gc\n",
    "import os\n",
    "\n",
    "# Enable memory growth to prevent GPU memory allocation issues\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if physical_devices:\n",
    "    for device in physical_devices:\n",
    "        tf.config.experimental.set_memory_growth(device, True)\n",
    "    print(\"GPU memory growth enabled\")\n",
    "\n",
    "# Add channel dimension to the input data - simplified\n",
    "def preprocess_data(X):\n",
    "    if len(X.shape) == 3:\n",
    "        X = np.expand_dims(X, axis=-1)\n",
    "        X = np.repeat(X, 3, axis=-1)\n",
    "    return X\n",
    "\n",
    "# Lightweight WindowAttention optimized for CPU\n",
    "class WindowAttention(layers.Layer):\n",
    "    def __init__(self, window_size, num_heads, dropout_rate=0.1, **kwargs):\n",
    "        super(WindowAttention, self).__init__(**kwargs)\n",
    "        self.window_size = window_size\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Significantly smaller key dimension for CPU\n",
    "        key_dim = max(16, input_shape[-1] // (self.num_heads * 2))\n",
    "        \n",
    "        # For CPU, consider using a simpler attention mechanism\n",
    "        self.mha = layers.MultiHeadAttention(\n",
    "            num_heads=self.num_heads,\n",
    "            key_dim=key_dim,\n",
    "            dropout=self.dropout_rate\n",
    "        )\n",
    "        \n",
    "        # Cache input shape for faster processing\n",
    "        self.input_height = input_shape[1]\n",
    "        self.input_width = input_shape[2]\n",
    "        self.channels = input_shape[3]\n",
    "        \n",
    "        # Add direct projection option as alternative to full attention\n",
    "        self.use_lightweight = (self.window_size > 4)  # Use lightweight for larger windows\n",
    "        if self.use_lightweight:\n",
    "            # Create a lightweight alternative that's faster on CPU\n",
    "            self.conv_proj = tf.keras.Sequential([\n",
    "                layers.Conv2D(self.channels, kernel_size=3, padding='same'),\n",
    "                layers.BatchNormalization(),\n",
    "                layers.Activation('relu')\n",
    "            ])\n",
    "            \n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        # For very large windows on CPU, use the lightweight path\n",
    "        if self.use_lightweight and not training:\n",
    "            return self.conv_proj(inputs)\n",
    "            \n",
    "        B = tf.shape(inputs)[0]\n",
    "        H, W, C = self.input_height, self.input_width, self.channels\n",
    "        \n",
    "        # Pre-calculate padding to avoid conditional logic\n",
    "        pad_h = (self.window_size - (H % self.window_size)) % self.window_size\n",
    "        pad_w = (self.window_size - (W % self.window_size)) % self.window_size\n",
    "        \n",
    "        # Apply padding if needed\n",
    "        if pad_h > 0 or pad_w > 0:\n",
    "            x = tf.pad(inputs, [[0, 0], [0, pad_h], [0, pad_w], [0, 0]])\n",
    "            padded_H, padded_W = H + pad_h, W + pad_w\n",
    "        else:\n",
    "            x = inputs\n",
    "            padded_H, padded_W = H, W\n",
    "        \n",
    "        # Use static window counts for better CPU performance\n",
    "        num_h_windows = padded_H // self.window_size\n",
    "        num_w_windows = padded_W // self.window_size\n",
    "        \n",
    "        # Use a more direct reshape flow for CPU\n",
    "        x = tf.reshape(x, [B, num_h_windows, self.window_size, num_w_windows, self.window_size, C])\n",
    "        x = tf.transpose(x, [0, 1, 3, 2, 4, 5])\n",
    "        x = tf.reshape(x, [-1, self.window_size * self.window_size, C])\n",
    "        \n",
    "        # Apply attention\n",
    "        attention_output = self.mha(x, x, training=training)\n",
    "        \n",
    "        # Reshape back directly\n",
    "        x = tf.reshape(attention_output, [B, num_h_windows, num_w_windows, self.window_size, self.window_size, C])\n",
    "        x = tf.transpose(x, [0, 1, 3, 2, 4, 5])\n",
    "        x = tf.reshape(x, [B, padded_H, padded_W, C])\n",
    "        \n",
    "        # Remove padding if needed\n",
    "        if pad_h > 0 or pad_w > 0:\n",
    "            return x[:, :H, :W, :]\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "# Simplified MLP block\n",
    "class MLPBlock(layers.Layer):\n",
    "    def __init__(self, hidden_units, dropout_rate=0.1, **kwargs):\n",
    "        super(MLPBlock, self).__init__(**kwargs)\n",
    "        self.mlp = tf.keras.Sequential([\n",
    "            layers.Dense(units, activation='gelu') \n",
    "            for units in hidden_units\n",
    "        ])\n",
    "        self.dropout = layers.Dropout(dropout_rate)\n",
    "        \n",
    "    def call(self, inputs, training=False):\n",
    "        x = self.mlp(inputs)\n",
    "        return self.dropout(x, training=training)\n",
    "\n",
    "# Optimized MaxViT block\n",
    "class MaxViTBlock(layers.Layer):\n",
    "    def __init__(self, num_heads=8, mlp_units=[128, 64], window_size=7, dropout_rate=0.1, **kwargs):\n",
    "        super(MaxViTBlock, self).__init__(**kwargs)\n",
    "        \n",
    "        # Initialize layers\n",
    "        self.window_attention = WindowAttention(\n",
    "            window_size=window_size, \n",
    "            num_heads=num_heads, \n",
    "            dropout_rate=dropout_rate\n",
    "        )\n",
    "        self.mlp = MLPBlock(\n",
    "            hidden_units=mlp_units, \n",
    "            dropout_rate=dropout_rate\n",
    "        )\n",
    "        self.norm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        # Attention branch\n",
    "        x_norm = self.norm1(inputs)\n",
    "        attention_output = self.window_attention(x_norm, training=training)\n",
    "        x = inputs + attention_output\n",
    "        \n",
    "        # MLP branch\n",
    "        x_norm2 = self.norm2(x)\n",
    "        mlp_output = self.mlp(x_norm2, training=training)\n",
    "        x = x + mlp_output\n",
    "\n",
    "        return x\n",
    "\n",
    "def create_maxvit_model(input_shape=(28, 28, 3), num_classes=1, depth=2):\n",
    "    \"\"\"\n",
    "    Create a highly efficient MaxViT model for CPU-only environments\n",
    "    \"\"\"\n",
    "    inputs = Input(shape=input_shape)\n",
    "    \n",
    "    # Initial embedding with more aggressive downsampling for CPU\n",
    "    x = layers.Conv2D(16, kernel_size=5, strides=2, padding=\"same\")(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)  # Using relu instead of gelu for CPU efficiency\n",
    "    \n",
    "    # Progressive feature dimension - much smaller for CPU\n",
    "    features = [16, 32, 64, 128][:depth]  # Reduced feature dimensions\n",
    "    \n",
    "    # Calculate static window sizes ahead of time\n",
    "    h, w = input_shape[0] // 2, input_shape[1] // 2\n",
    "    \n",
    "    # MaxViT blocks - reduced complexity for CPU\n",
    "    for i, dim in enumerate(features):\n",
    "        # Feature dimension adjustment\n",
    "        if i > 0:\n",
    "            x = layers.Conv2D(dim, kernel_size=1, padding=\"same\")(x)\n",
    "            x = layers.BatchNormalization()(x)\n",
    "        \n",
    "        # Use a smaller fixed window size for CPU\n",
    "        window_size = max(2, min(5, h // 2))  # Reduced from max 7 to max 5\n",
    "        \n",
    "        # MaxViT block with fewer heads for CPU\n",
    "        x = MaxViTBlock(\n",
    "            num_heads=max(1, min(4, dim // 16)),  # Reduced max heads from 8 to 4\n",
    "            mlp_units=[dim, dim],  # Simpler MLP structure\n",
    "            window_size=window_size,\n",
    "            dropout_rate=0.1\n",
    "        )(x)\n",
    "        \n",
    "        # More aggressive downsampling for CPU\n",
    "        if i < len(features) - 1:\n",
    "            x = layers.AveragePooling2D(pool_size=2)(x)  # Pool instead of Conv2D for CPU\n",
    "            h, w = h // 2, w // 2\n",
    "    \n",
    "    # Global pooling and simpler classification head\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dense(32, activation=\"relu\")(x)  # Reduced from 64 to 32\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    \n",
    "    # Output layer\n",
    "    if num_classes == 1:\n",
    "        outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    else:\n",
    "        outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "    \n",
    "    return Model(inputs, outputs)\n",
    "\n",
    "def calculate_metrics(y_true, y_pred, y_pred_proba=None):\n",
    "    \"\"\"Calculate all required metrics\"\"\"\n",
    "    \n",
    "    # Convert predictions to binary\n",
    "    if len(y_pred.shape) > 1 and y_pred.shape[1] > 1:\n",
    "        y_pred_binary = np.argmax(y_pred, axis=1)\n",
    "    else:\n",
    "        y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "    \n",
    "    # Calculate confusion matrix\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred_binary).ravel()\n",
    "    \n",
    "    # Basic metrics\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    # Calculate additional metrics\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    mcc = matthews_corrcoef(y_true, y_pred_binary)\n",
    "    \n",
    "    # Calculate log loss if probabilities provided\n",
    "    if y_pred_proba is not None:\n",
    "        log_loss_val = log_loss(y_true, y_pred_proba)\n",
    "    else:\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
    "        log_loss_val = -np.mean(y_true * np.log(y_pred_clipped) + \n",
    "                        (1 - y_true) * np.log(1 - y_pred_clipped))\n",
    "    \n",
    "    # Additional metrics\n",
    "    g_mean = np.sqrt(recall * specificity) if (recall * specificity) > 0 else 0\n",
    "    youdens_j = recall + specificity - 1\n",
    "    balanced_acc = balanced_accuracy_score(y_true, y_pred_binary)\n",
    "    kappa = cohen_kappa_score(y_true, y_pred_binary)\n",
    "    \n",
    "    # AUC-ROC score\n",
    "    try:\n",
    "        if y_pred_proba is not None:\n",
    "            auc_roc = roc_auc_score(y_true, y_pred_proba)\n",
    "        else:\n",
    "            auc_roc = roc_auc_score(y_true, y_pred)\n",
    "    except:\n",
    "        auc_roc = 0.5\n",
    "    \n",
    "    return {\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1_Score': f1,\n",
    "        'AUC_ROC': auc_roc,\n",
    "        'TP': tp,\n",
    "        'TN': tn,\n",
    "        'FP': fp,\n",
    "        'FN': fn,\n",
    "        'Specificity': specificity,\n",
    "        'MCC': mcc,\n",
    "        'Log_Loss': log_loss_val,\n",
    "        'G_Mean': g_mean,\n",
    "        'Youdens_J': youdens_j,\n",
    "        'Balanced_Accuracy': balanced_acc,\n",
    "        'Cohens_Kappa': kappa\n",
    "    }\n",
    "\n",
    "def perform_kfold_cv(X, y, X_test=None, y_test=None, k_folds=[3], epochs=20):\n",
    "    \"\"\"\n",
    "    Perform k-fold cross-validation with the MaxViT model, optimized for CPU-only environments\n",
    "    \"\"\"\n",
    "    # Reduce input dimensions for CPU by downsampling large inputs \n",
    "    def downsample_if_needed(X):\n",
    "        # If inputs are large, downsample them for CPU efficiency\n",
    "        if X.shape[1] > 64 or X.shape[2] > 64:\n",
    "            from skimage.transform import resize\n",
    "            factor = max(1, min(X.shape[1], X.shape[2]) // 64)\n",
    "            target_shape = (X.shape[0], X.shape[1]//factor, X.shape[2]//factor)\n",
    "            X_small = np.zeros(target_shape + (X.shape[3],) if len(X.shape) > 3 else target_shape)\n",
    "            \n",
    "            for i in range(X.shape[0]):\n",
    "                if len(X.shape) > 3:\n",
    "                    X_small[i] = resize(X[i], (target_shape[1], target_shape[2], X.shape[3]))\n",
    "                else:\n",
    "                    X_small[i] = resize(X[i], (target_shape[1], target_shape[2]))\n",
    "            \n",
    "            print(f\"Downsampled data from {X.shape[1]}x{X.shape[2]} to {X_small.shape[1]}x{X_small.shape[2]}\")\n",
    "            return X_small\n",
    "        return X\n",
    "    \n",
    "    # Preprocess and downsample the data for CPU\n",
    "    X = downsample_if_needed(X)\n",
    "    X = preprocess_data(X)\n",
    "    if X_test is not None:\n",
    "        X_test = downsample_if_needed(X_test)\n",
    "        X_test = preprocess_data(X_test)\n",
    "    \n",
    "    # Results file\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    results_file = f'outputresults/maxvit_results_{timestamp}111.csv'\n",
    "    all_results = []\n",
    "    \n",
    "    # More aggressive early stopping for CPU\n",
    "    callbacks = [\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=2,  # Very low patience for CPU\n",
    "            restore_best_weights=True,\n",
    "            min_delta=0.02  # Higher delta for faster termination\n",
    "        ),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.1,  # Very aggressive reduction\n",
    "            patience=1,  # Minimal patience\n",
    "            min_lr=1e-4  # Higher min_lr to avoid too many steps\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    # Set thread optimization for CPU\n",
    "    tf.config.threading.set_inter_op_parallelism_threads(2)  # Reduce thread contention\n",
    "    tf.config.threading.set_intra_op_parallelism_threads(4)  # Set reasonable thread count\n",
    "    \n",
    "    # Run for each k-fold setting\n",
    "    for k in k_folds:\n",
    "        print(f\"\\n=== Starting {k}-fold cross-validation ===\")\n",
    "        \n",
    "        # Initialize K-fold\n",
    "        kfold = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "        \n",
    "        for fold, (train_idx, val_idx) in enumerate(kfold.split(X)):\n",
    "            print(f'\\nFold {fold + 1}/{k}')\n",
    "            \n",
    "            # Split data - use smaller validation set for CPU\n",
    "            train_size = min(len(train_idx), 1000)  # Limit training set size\n",
    "            val_size = min(len(val_idx), 200)  # Limit validation set size\n",
    "            \n",
    "            # Subsample data for faster CPU processing\n",
    "            train_idx = train_idx[:train_size]\n",
    "            val_idx = val_idx[:val_size]\n",
    "            \n",
    "            X_train_fold, X_val_fold = X[train_idx], X[val_idx]\n",
    "            y_train_fold, y_val_fold = y[train_idx], y[val_idx]\n",
    "            \n",
    "            print(f\"Training data shape: {X_train_fold.shape} (limited for CPU)\")\n",
    "            \n",
    "            # Get input shape from actual data\n",
    "            input_shape = X_train_fold.shape[1:]\n",
    "            \n",
    "            # Create model with minimal depth for CPU\n",
    "            model = create_maxvit_model(input_shape=input_shape, depth=2)  # Ultra-light depth for CPU\n",
    "            \n",
    "            # Use simpler optimizer for CPU\n",
    "            optimizer = tf.keras.optimizers.Adam(learning_rate=0.005)  # Higher learning rate\n",
    "                \n",
    "            model.compile(\n",
    "                optimizer=optimizer,\n",
    "                loss='binary_crossentropy',\n",
    "                metrics=['accuracy']  # Minimal metrics for CPU\n",
    "            )\n",
    "            \n",
    "            # Train model with optimal CPU batch size\n",
    "            start_time = time.time()\n",
    "            batch_size = 32  # Smaller batch size for CPU memory efficiency\n",
    "            history = model.fit(\n",
    "                X_train_fold, y_train_fold,\n",
    "                epochs=epochs,\n",
    "                batch_size=batch_size,\n",
    "                validation_data=(X_val_fold, y_val_fold),\n",
    "                callbacks=callbacks,\n",
    "                verbose=1\n",
    "            )\n",
    "            training_time = time.time() - start_time\n",
    "            \n",
    "            # Evaluate on validation set\n",
    "            val_results = model.evaluate(X_val_fold, y_val_fold, verbose=0)\n",
    "            val_pred = model.predict(X_val_fold, verbose=0, batch_size=batch_size)\n",
    "            val_metrics = calculate_metrics(y_val_fold, val_pred)\n",
    "            \n",
    "            # Evaluate on test set if provided\n",
    "            if X_test is not None and y_test is not None:\n",
    "                test_results = model.evaluate(X_test, y_test, verbose=0)\n",
    "                test_pred = model.predict(X_test, verbose=0, batch_size=batch_size)\n",
    "                test_metrics = calculate_metrics(y_test, test_pred)\n",
    "                eval_metrics = test_metrics\n",
    "                eval_loss = test_results[0]\n",
    "            else:\n",
    "                eval_metrics = val_metrics\n",
    "                eval_loss = val_results[0]\n",
    "            \n",
    "            # Store metrics\n",
    "            fold_metrics = {\n",
    "                'Fold': fold + 1,\n",
    "                'K': k,\n",
    "                'Test_Accuracy': eval_metrics['Accuracy'],\n",
    "                'Train_Accuracy': history.history['accuracy'][-1],\n",
    "                'Precision': eval_metrics['Precision'],\n",
    "                'Recall': eval_metrics['Recall'],\n",
    "                'AUC_ROC': eval_metrics['AUC_ROC'],\n",
    "                'F1_Score': eval_metrics['F1_Score'],\n",
    "                'Training_Loss': history.history['loss'][-1],\n",
    "                'Testing_Loss': eval_loss,\n",
    "                'Time_Taken': training_time,\n",
    "                'Epochs_Run': len(history.history['loss']),\n",
    "                'TP': eval_metrics['TP'],\n",
    "                'TN': eval_metrics['TN'],\n",
    "                'FP': eval_metrics['FP'],\n",
    "                'FN': eval_metrics['FN'],\n",
    "                'Specificity': eval_metrics['Specificity'],\n",
    "                'MCC': eval_metrics['MCC'],\n",
    "                'Log_Loss': eval_metrics['Log_Loss'],\n",
    "                'G_Mean': eval_metrics['G_Mean'],\n",
    "                'Youdens_J': eval_metrics['Youdens_J'],\n",
    "                'Balanced_Accuracy': eval_metrics['Balanced_Accuracy'],\n",
    "                'Cohens_Kappa': eval_metrics['Cohens_Kappa']\n",
    "            }\n",
    "            \n",
    "            # Add to results list\n",
    "            all_results.append(fold_metrics)\n",
    "            \n",
    "            # Print current fold results\n",
    "            print(f\"\\nFold {fold + 1} Results:\")\n",
    "            for metric, value in fold_metrics.items():\n",
    "                if isinstance(value, (int, float)):\n",
    "                    print(f\"{metric}: {value:.4f}\")\n",
    "            \n",
    "            # Clear memory\n",
    "            del model\n",
    "            del history\n",
    "            tf.keras.backend.clear_session()\n",
    "            gc.collect()\n",
    "    \n",
    "    # Convert to DataFrame and save to CSV\n",
    "    results_df = pd.DataFrame(all_results)\n",
    "    results_df.to_csv(results_file, index=False)\n",
    "    \n",
    "    # Calculate and print summary statistics\n",
    "    print(\"\\n=== Summary Statistics ===\")\n",
    "    for k in k_folds:\n",
    "        k_results = results_df[results_df['K'] == k]\n",
    "        print(f\"\\nK={k} Cross-Validation Results:\")\n",
    "        print(f\"Average Test Accuracy: {k_results['Test_Accuracy'].mean():.4f} ± {k_results['Test_Accuracy'].std():.4f}\")\n",
    "        print(f\"Average F1 Score: {k_results['F1_Score'].mean():.4f} ± {k_results['F1_Score'].std():.4f}\")\n",
    "        print(f\"Average AUC-ROC: {k_results['AUC_ROC'].mean():.4f} ± {k_results['AUC_ROC'].std():.4f}\")\n",
    "        print(f\"Average Training Time: {k_results['Time_Taken'].mean():.2f} seconds\")\n",
    "        print(f\"Average Epochs Run: {k_results['Epochs_Run'].mean():.1f}\")\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "# Usage example\n",
    "if __name__ == \"__main__\":\n",
    "    # Uncomment and run when needed\n",
    "    \n",
    "    # For 5-fold and 10-fold validation with test set evaluation\n",
    "    results = perform_kfold_cv(\n",
    "        X=X_train_full, \n",
    "        y=y_train_full,\n",
    "        X_test=X_test,\n",
    "        y_test=y_test,\n",
    "        k_folds=[5,10,15,20],  # Changed from [5, 10] to just [5] for faster execution\n",
    "        epochs=50     # Reduced from 50 to 30\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c667b018-3c91-4905-8a98-fc4ad391a0e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using scikit-learn for CPU compatibility\n",
      "Downsampled data from 224x224 to 112x112\n",
      "Data shape after preprocessing: (800, 112, 112, 3)\n",
      "Extracting features...\n",
      "Feature shape: (800, 228)\n",
      "\n",
      "=== Starting 5-fold cross-validation ===\n",
      "\n",
      "Fold 1/5\n",
      "Training data shape: (640, 228) (limited for CPU)\n",
      "Validation data shape: (160, 228)\n",
      "Training scikit-learn MLP model...\n",
      "Iteration 1, loss = 0.64658149\n",
      "Validation score: 0.750000\n",
      "Iteration 2, loss = 0.44942465\n",
      "Validation score: 0.750000\n",
      "Iteration 3, loss = 0.38131087\n",
      "Validation score: 0.718750\n",
      "Iteration 4, loss = 0.33505745\n",
      "Validation score: 0.750000\n",
      "Iteration 5, loss = 0.30603417\n",
      "Validation score: 0.750000\n",
      "Iteration 6, loss = 0.27669732\n",
      "Validation score: 0.750000\n",
      "Iteration 7, loss = 0.26499178\n",
      "Validation score: 0.734375\n",
      "Validation score did not improve more than tol=0.000100 for 5 consecutive epochs. Stopping.\n",
      "Training completed in 0.17 seconds\n",
      "\n",
      "Fold 1 Results:\n",
      "Fold: 1.0000\n",
      "K: 5.0000\n",
      "Test_Accuracy: 0.7500\n",
      "Train_Accuracy: 0.7875\n",
      "Precision: 0.6796\n",
      "Recall: 0.9091\n",
      "AUC_ROC: 0.8060\n",
      "F1_Score: 0.7778\n",
      "Training_Loss: 0.2650\n",
      "Time_Taken: 0.1680\n",
      "Epochs_Run: 7.0000\n",
      "Specificity: 0.6024\n",
      "MCC: 0.5337\n",
      "G_Mean: 0.7400\n",
      "Youdens_J: 0.5115\n",
      "Balanced_Accuracy: 0.7558\n",
      "Cohens_Kappa: 0.5053\n",
      "Model saved to sklearn_model_fold_1.joblib\n",
      "\n",
      "Fold 2/5\n",
      "Training data shape: (640, 228) (limited for CPU)\n",
      "Validation data shape: (160, 228)\n",
      "Training scikit-learn MLP model...\n",
      "Iteration 1, loss = 0.59698547\n",
      "Validation score: 0.703125\n",
      "Iteration 2, loss = 0.42526313\n",
      "Validation score: 0.687500\n",
      "Iteration 3, loss = 0.36513431\n",
      "Validation score: 0.703125\n",
      "Iteration 4, loss = 0.32190457\n",
      "Validation score: 0.687500\n",
      "Iteration 5, loss = 0.29733234\n",
      "Validation score: 0.640625\n",
      "Iteration 6, loss = 0.27123578\n",
      "Validation score: 0.687500\n",
      "Iteration 7, loss = 0.24634001\n",
      "Validation score: 0.718750\n",
      "Iteration 8, loss = 0.22421802\n",
      "Validation score: 0.734375\n",
      "Iteration 9, loss = 0.21599661\n",
      "Validation score: 0.750000\n",
      "Iteration 10, loss = 0.19736961\n",
      "Validation score: 0.734375\n",
      "Iteration 11, loss = 0.19242923\n",
      "Validation score: 0.734375\n",
      "Iteration 12, loss = 0.17564388\n",
      "Validation score: 0.703125\n",
      "Iteration 13, loss = 0.16387424\n",
      "Validation score: 0.703125\n",
      "Iteration 14, loss = 0.15362936\n",
      "Validation score: 0.734375\n",
      "Iteration 15, loss = 0.13333906\n",
      "Validation score: 0.718750\n",
      "Validation score did not improve more than tol=0.000100 for 5 consecutive epochs. Stopping.\n",
      "Training completed in 0.53 seconds\n",
      "\n",
      "Fold 2 Results:\n",
      "Fold: 2.0000\n",
      "K: 5.0000\n",
      "Test_Accuracy: 0.7250\n",
      "Train_Accuracy: 0.9125\n",
      "Precision: 0.7160\n",
      "Recall: 0.7342\n",
      "AUC_ROC: 0.8142\n",
      "F1_Score: 0.7250\n",
      "Training_Loss: 0.1333\n",
      "Time_Taken: 0.5283\n",
      "Epochs_Run: 15.0000\n",
      "Specificity: 0.7160\n",
      "MCC: 0.4502\n",
      "G_Mean: 0.7251\n",
      "Youdens_J: 0.4502\n",
      "Balanced_Accuracy: 0.7251\n",
      "Cohens_Kappa: 0.4501\n",
      "Model saved to sklearn_model_fold_2.joblib\n",
      "\n",
      "Fold 3/5\n",
      "Training data shape: (640, 228) (limited for CPU)\n",
      "Validation data shape: (160, 228)\n",
      "Training scikit-learn MLP model...\n",
      "Iteration 1, loss = 0.58947629\n",
      "Validation score: 0.828125\n",
      "Iteration 2, loss = 0.43665554\n",
      "Validation score: 0.828125\n",
      "Iteration 3, loss = 0.37195336\n",
      "Validation score: 0.812500\n",
      "Iteration 4, loss = 0.33230984\n",
      "Validation score: 0.828125\n",
      "Iteration 5, loss = 0.30084617\n",
      "Validation score: 0.812500\n",
      "Iteration 6, loss = 0.28572676\n",
      "Validation score: 0.765625\n",
      "Iteration 7, loss = 0.28157351\n",
      "Validation score: 0.859375\n",
      "Iteration 8, loss = 0.24863535\n",
      "Validation score: 0.812500\n",
      "Iteration 9, loss = 0.22333686\n",
      "Validation score: 0.781250\n",
      "Iteration 10, loss = 0.21601220\n",
      "Validation score: 0.843750\n",
      "Iteration 11, loss = 0.19909737\n",
      "Validation score: 0.828125\n",
      "Iteration 12, loss = 0.19026969\n",
      "Validation score: 0.765625\n",
      "Iteration 13, loss = 0.17723742\n",
      "Validation score: 0.812500\n",
      "Validation score did not improve more than tol=0.000100 for 5 consecutive epochs. Stopping.\n",
      "Training completed in 0.43 seconds\n",
      "\n",
      "Fold 3 Results:\n",
      "Fold: 3.0000\n",
      "K: 5.0000\n",
      "Test_Accuracy: 0.7250\n",
      "Train_Accuracy: 0.9000\n",
      "Precision: 0.7308\n",
      "Recall: 0.7125\n",
      "AUC_ROC: 0.7475\n",
      "F1_Score: 0.7215\n",
      "Training_Loss: 0.1772\n",
      "Time_Taken: 0.4347\n",
      "Epochs_Run: 13.0000\n",
      "Specificity: 0.7375\n",
      "MCC: 0.4501\n",
      "G_Mean: 0.7249\n",
      "Youdens_J: 0.4500\n",
      "Balanced_Accuracy: 0.7250\n",
      "Cohens_Kappa: 0.4500\n",
      "Model saved to sklearn_model_fold_3.joblib\n",
      "\n",
      "Fold 4/5\n",
      "Training data shape: (640, 228) (limited for CPU)\n",
      "Validation data shape: (160, 228)\n",
      "Training scikit-learn MLP model...\n",
      "Iteration 1, loss = 0.61794483\n",
      "Validation score: 0.671875\n",
      "Iteration 2, loss = 0.43850631\n",
      "Validation score: 0.671875\n",
      "Iteration 3, loss = 0.38217008\n",
      "Validation score: 0.687500\n",
      "Iteration 4, loss = 0.32651040\n",
      "Validation score: 0.703125\n",
      "Iteration 5, loss = 0.28933827\n",
      "Validation score: 0.687500\n",
      "Iteration 6, loss = 0.26896750\n",
      "Validation score: 0.671875\n",
      "Iteration 7, loss = 0.25285282\n",
      "Validation score: 0.687500\n",
      "Iteration 8, loss = 0.21751220\n",
      "Validation score: 0.640625\n",
      "Iteration 9, loss = 0.21073247\n",
      "Validation score: 0.687500\n",
      "Iteration 10, loss = 0.20133570\n",
      "Validation score: 0.640625\n",
      "Validation score did not improve more than tol=0.000100 for 5 consecutive epochs. Stopping.\n",
      "Training completed in 0.36 seconds\n",
      "\n",
      "Fold 4 Results:\n",
      "Fold: 4.0000\n",
      "K: 5.0000\n",
      "Test_Accuracy: 0.7750\n",
      "Train_Accuracy: 0.8688\n",
      "Precision: 0.7794\n",
      "Recall: 0.7162\n",
      "AUC_ROC: 0.8518\n",
      "F1_Score: 0.7465\n",
      "Training_Loss: 0.2013\n",
      "Time_Taken: 0.3600\n",
      "Epochs_Run: 10.0000\n",
      "Specificity: 0.8256\n",
      "MCC: 0.5465\n",
      "G_Mean: 0.7690\n",
      "Youdens_J: 0.5418\n",
      "Balanced_Accuracy: 0.7709\n",
      "Cohens_Kappa: 0.5449\n",
      "Model saved to sklearn_model_fold_4.joblib\n",
      "\n",
      "Fold 5/5\n",
      "Training data shape: (640, 228) (limited for CPU)\n",
      "Validation data shape: (160, 228)\n",
      "Training scikit-learn MLP model...\n",
      "Iteration 1, loss = 0.61140868\n",
      "Validation score: 0.812500\n",
      "Iteration 2, loss = 0.46603560\n",
      "Validation score: 0.828125\n",
      "Iteration 3, loss = 0.39259683\n",
      "Validation score: 0.859375\n",
      "Iteration 4, loss = 0.35309305\n",
      "Validation score: 0.843750\n",
      "Iteration 5, loss = 0.30869411\n",
      "Validation score: 0.859375\n",
      "Iteration 6, loss = 0.28810413\n",
      "Validation score: 0.875000\n",
      "Iteration 7, loss = 0.26226794\n",
      "Validation score: 0.875000\n",
      "Iteration 8, loss = 0.24425132\n",
      "Validation score: 0.812500\n",
      "Iteration 9, loss = 0.22828248\n",
      "Validation score: 0.781250\n",
      "Iteration 10, loss = 0.20392100\n",
      "Validation score: 0.796875\n",
      "Iteration 11, loss = 0.20180677\n",
      "Validation score: 0.828125\n",
      "Iteration 12, loss = 0.17437173\n",
      "Validation score: 0.812500\n",
      "Validation score did not improve more than tol=0.000100 for 5 consecutive epochs. Stopping.\n",
      "Training completed in 0.39 seconds\n",
      "\n",
      "Fold 5 Results:\n",
      "Fold: 5.0000\n",
      "K: 5.0000\n",
      "Test_Accuracy: 0.7562\n",
      "Train_Accuracy: 0.9078\n",
      "Precision: 0.7901\n",
      "Recall: 0.7442\n",
      "AUC_ROC: 0.8210\n",
      "F1_Score: 0.7665\n",
      "Training_Loss: 0.1744\n",
      "Time_Taken: 0.3916\n",
      "Epochs_Run: 12.0000\n",
      "Specificity: 0.7703\n",
      "MCC: 0.5130\n",
      "G_Mean: 0.7571\n",
      "Youdens_J: 0.5145\n",
      "Balanced_Accuracy: 0.7572\n",
      "Cohens_Kappa: 0.5120\n",
      "Model saved to sklearn_model_fold_5.joblib\n",
      "\n",
      "=== Starting 10-fold cross-validation ===\n",
      "\n",
      "Fold 1/10\n",
      "Training data shape: (720, 228) (limited for CPU)\n",
      "Validation data shape: (80, 228)\n",
      "Training scikit-learn MLP model...\n",
      "Iteration 1, loss = 0.60354900\n",
      "Validation score: 0.750000\n",
      "Iteration 2, loss = 0.43504189\n",
      "Validation score: 0.736111\n",
      "Iteration 3, loss = 0.37126118\n",
      "Validation score: 0.722222\n",
      "Iteration 4, loss = 0.33536371\n",
      "Validation score: 0.736111\n",
      "Iteration 5, loss = 0.30721988\n",
      "Validation score: 0.736111\n",
      "Iteration 6, loss = 0.28790632\n",
      "Validation score: 0.750000\n",
      "Iteration 7, loss = 0.26448527\n",
      "Validation score: 0.722222\n",
      "Validation score did not improve more than tol=0.000100 for 5 consecutive epochs. Stopping.\n",
      "Training completed in 0.30 seconds\n",
      "\n",
      "Fold 1 Results:\n",
      "Fold: 1.0000\n",
      "K: 10.0000\n",
      "Test_Accuracy: 0.7500\n",
      "Train_Accuracy: 0.7958\n",
      "Precision: 0.7111\n",
      "Recall: 0.8205\n",
      "AUC_ROC: 0.7605\n",
      "F1_Score: 0.7619\n",
      "Training_Loss: 0.2645\n",
      "Time_Taken: 0.2956\n",
      "Epochs_Run: 7.0000\n",
      "Specificity: 0.6829\n",
      "MCC: 0.5073\n",
      "G_Mean: 0.7486\n",
      "Youdens_J: 0.5034\n",
      "Balanced_Accuracy: 0.7517\n",
      "Cohens_Kappa: 0.5016\n",
      "Model saved to sklearn_model_fold_1.joblib\n",
      "\n",
      "Fold 2/10\n",
      "Training data shape: (720, 228) (limited for CPU)\n",
      "Validation data shape: (80, 228)\n",
      "Training scikit-learn MLP model...\n",
      "Iteration 1, loss = 0.60073517\n",
      "Validation score: 0.805556\n",
      "Iteration 2, loss = 0.45461975\n",
      "Validation score: 0.791667\n",
      "Iteration 3, loss = 0.38635910\n",
      "Validation score: 0.819444\n",
      "Iteration 4, loss = 0.34700722\n",
      "Validation score: 0.805556\n",
      "Iteration 5, loss = 0.31440623\n",
      "Validation score: 0.777778\n",
      "Iteration 6, loss = 0.29807284\n",
      "Validation score: 0.791667\n",
      "Iteration 7, loss = 0.27285342\n",
      "Validation score: 0.791667\n",
      "Iteration 8, loss = 0.24829010\n",
      "Validation score: 0.805556\n",
      "Iteration 9, loss = 0.23143546\n",
      "Validation score: 0.819444\n",
      "Validation score did not improve more than tol=0.000100 for 5 consecutive epochs. Stopping.\n",
      "Training completed in 0.36 seconds\n",
      "\n",
      "Fold 2 Results:\n",
      "Fold: 2.0000\n",
      "K: 10.0000\n",
      "Test_Accuracy: 0.7875\n",
      "Train_Accuracy: 0.8403\n",
      "Precision: 0.7059\n",
      "Recall: 0.9474\n",
      "AUC_ROC: 0.8647\n",
      "F1_Score: 0.8090\n",
      "Training_Loss: 0.2314\n",
      "Time_Taken: 0.3584\n",
      "Epochs_Run: 9.0000\n",
      "Specificity: 0.6429\n",
      "MCC: 0.6131\n",
      "G_Mean: 0.7804\n",
      "Youdens_J: 0.5902\n",
      "Balanced_Accuracy: 0.7951\n",
      "Cohens_Kappa: 0.5808\n",
      "Model saved to sklearn_model_fold_2.joblib\n",
      "\n",
      "Fold 3/10\n",
      "Training data shape: (720, 228) (limited for CPU)\n",
      "Validation data shape: (80, 228)\n",
      "Training scikit-learn MLP model...\n",
      "Iteration 1, loss = 0.59279803\n",
      "Validation score: 0.736111\n",
      "Iteration 2, loss = 0.43341166\n",
      "Validation score: 0.750000\n",
      "Iteration 3, loss = 0.38141433\n",
      "Validation score: 0.722222\n",
      "Iteration 4, loss = 0.34288925\n",
      "Validation score: 0.694444\n",
      "Iteration 5, loss = 0.31124902\n",
      "Validation score: 0.694444\n",
      "Iteration 6, loss = 0.28860027\n",
      "Validation score: 0.722222\n",
      "Iteration 7, loss = 0.27010001\n",
      "Validation score: 0.652778\n",
      "Iteration 8, loss = 0.24716479\n",
      "Validation score: 0.666667\n",
      "Validation score did not improve more than tol=0.000100 for 5 consecutive epochs. Stopping.\n",
      "Training completed in 0.36 seconds\n",
      "\n",
      "Fold 3 Results:\n",
      "Fold: 3.0000\n",
      "K: 10.0000\n",
      "Test_Accuracy: 0.8125\n",
      "Train_Accuracy: 0.8250\n",
      "Precision: 0.7436\n",
      "Recall: 0.8529\n",
      "AUC_ROC: 0.8293\n",
      "F1_Score: 0.7945\n",
      "Training_Loss: 0.2472\n",
      "Time_Taken: 0.3650\n",
      "Epochs_Run: 8.0000\n",
      "Specificity: 0.7826\n",
      "MCC: 0.6286\n",
      "G_Mean: 0.8170\n",
      "Youdens_J: 0.6355\n",
      "Balanced_Accuracy: 0.8178\n",
      "Cohens_Kappa: 0.6236\n",
      "Model saved to sklearn_model_fold_3.joblib\n",
      "\n",
      "Fold 4/10\n",
      "Training data shape: (720, 228) (limited for CPU)\n",
      "Validation data shape: (80, 228)\n",
      "Training scikit-learn MLP model...\n",
      "Iteration 1, loss = 0.59752580\n",
      "Validation score: 0.819444\n",
      "Iteration 2, loss = 0.44481192\n",
      "Validation score: 0.805556\n",
      "Iteration 3, loss = 0.37495277\n",
      "Validation score: 0.833333\n",
      "Iteration 4, loss = 0.32799810\n",
      "Validation score: 0.819444\n",
      "Iteration 5, loss = 0.29896190\n",
      "Validation score: 0.819444\n",
      "Iteration 6, loss = 0.26965061\n",
      "Validation score: 0.819444\n",
      "Iteration 7, loss = 0.25423671\n",
      "Validation score: 0.819444\n",
      "Iteration 8, loss = 0.24589910\n",
      "Validation score: 0.722222\n",
      "Iteration 9, loss = 0.24489753\n",
      "Validation score: 0.819444\n",
      "Validation score did not improve more than tol=0.000100 for 5 consecutive epochs. Stopping.\n",
      "Training completed in 0.37 seconds\n",
      "\n",
      "Fold 4 Results:\n",
      "Fold: 4.0000\n",
      "K: 10.0000\n",
      "Test_Accuracy: 0.7000\n",
      "Train_Accuracy: 0.8681\n",
      "Precision: 0.7442\n",
      "Recall: 0.7111\n",
      "AUC_ROC: 0.7638\n",
      "F1_Score: 0.7273\n",
      "Training_Loss: 0.2449\n",
      "Time_Taken: 0.3730\n",
      "Epochs_Run: 9.0000\n",
      "Specificity: 0.6857\n",
      "MCC: 0.3948\n",
      "G_Mean: 0.6983\n",
      "Youdens_J: 0.3968\n",
      "Balanced_Accuracy: 0.6984\n",
      "Cohens_Kappa: 0.3943\n",
      "Model saved to sklearn_model_fold_4.joblib\n",
      "\n",
      "Fold 5/10\n",
      "Training data shape: (720, 228) (limited for CPU)\n",
      "Validation data shape: (80, 228)\n",
      "Training scikit-learn MLP model...\n",
      "Iteration 1, loss = 0.58379856\n",
      "Validation score: 0.736111\n",
      "Iteration 2, loss = 0.44360834\n",
      "Validation score: 0.736111\n",
      "Iteration 3, loss = 0.38031801\n",
      "Validation score: 0.722222\n",
      "Iteration 4, loss = 0.34497126\n",
      "Validation score: 0.736111\n",
      "Iteration 5, loss = 0.31096650\n",
      "Validation score: 0.750000\n",
      "Iteration 6, loss = 0.29439325\n",
      "Validation score: 0.763889\n",
      "Iteration 7, loss = 0.26640166\n",
      "Validation score: 0.708333\n",
      "Iteration 8, loss = 0.24022691\n",
      "Validation score: 0.763889\n",
      "Iteration 9, loss = 0.22387431\n",
      "Validation score: 0.777778\n",
      "Iteration 10, loss = 0.20559918\n",
      "Validation score: 0.736111\n",
      "Iteration 11, loss = 0.18645816\n",
      "Validation score: 0.750000\n",
      "Iteration 12, loss = 0.17180605\n",
      "Validation score: 0.694444\n",
      "Iteration 13, loss = 0.17221049\n",
      "Validation score: 0.763889\n",
      "Iteration 14, loss = 0.16671583\n",
      "Validation score: 0.680556\n",
      "Iteration 15, loss = 0.15859191\n",
      "Validation score: 0.736111\n",
      "Validation score did not improve more than tol=0.000100 for 5 consecutive epochs. Stopping.\n",
      "Training completed in 0.58 seconds\n",
      "\n",
      "Fold 5 Results:\n",
      "Fold: 5.0000\n",
      "K: 10.0000\n",
      "Test_Accuracy: 0.7250\n",
      "Train_Accuracy: 0.9139\n",
      "Precision: 0.7045\n",
      "Recall: 0.7750\n",
      "AUC_ROC: 0.8094\n",
      "F1_Score: 0.7381\n",
      "Training_Loss: 0.1586\n",
      "Time_Taken: 0.5814\n",
      "Epochs_Run: 15.0000\n",
      "Specificity: 0.6750\n",
      "MCC: 0.4523\n",
      "G_Mean: 0.7233\n",
      "Youdens_J: 0.4500\n",
      "Balanced_Accuracy: 0.7250\n",
      "Cohens_Kappa: 0.4500\n",
      "Model saved to sklearn_model_fold_5.joblib\n",
      "\n",
      "Fold 6/10\n",
      "Training data shape: (720, 228) (limited for CPU)\n",
      "Validation data shape: (80, 228)\n",
      "Training scikit-learn MLP model...\n",
      "Iteration 1, loss = 0.58119265\n",
      "Validation score: 0.680556\n",
      "Iteration 2, loss = 0.42787967\n",
      "Validation score: 0.694444\n",
      "Iteration 3, loss = 0.36389868\n",
      "Validation score: 0.722222\n",
      "Iteration 4, loss = 0.32469140\n",
      "Validation score: 0.722222\n",
      "Iteration 5, loss = 0.29839822\n",
      "Validation score: 0.708333\n",
      "Iteration 6, loss = 0.29595489\n",
      "Validation score: 0.694444\n",
      "Iteration 7, loss = 0.25469947\n",
      "Validation score: 0.722222\n",
      "Iteration 8, loss = 0.24218848\n",
      "Validation score: 0.736111\n",
      "Iteration 9, loss = 0.21686332\n",
      "Validation score: 0.722222\n",
      "Iteration 10, loss = 0.21019010\n",
      "Validation score: 0.708333\n",
      "Iteration 11, loss = 0.19963857\n",
      "Validation score: 0.750000\n",
      "Iteration 12, loss = 0.19674766\n",
      "Validation score: 0.708333\n",
      "Iteration 13, loss = 0.18950956\n",
      "Validation score: 0.694444\n",
      "Iteration 14, loss = 0.16977508\n",
      "Validation score: 0.736111\n",
      "Iteration 15, loss = 0.16148257\n",
      "Validation score: 0.708333\n",
      "Iteration 16, loss = 0.15068528\n",
      "Validation score: 0.722222\n",
      "Iteration 17, loss = 0.16967795\n",
      "Validation score: 0.708333\n",
      "Validation score did not improve more than tol=0.000100 for 5 consecutive epochs. Stopping.\n",
      "Training completed in 0.73 seconds\n",
      "\n",
      "Fold 6 Results:\n",
      "Fold: 6.0000\n",
      "K: 10.0000\n",
      "Test_Accuracy: 0.7375\n",
      "Train_Accuracy: 0.9097\n",
      "Precision: 0.7568\n",
      "Recall: 0.7000\n",
      "AUC_ROC: 0.7456\n",
      "F1_Score: 0.7273\n",
      "Training_Loss: 0.1697\n",
      "Time_Taken: 0.7290\n",
      "Epochs_Run: 17.0000\n",
      "Specificity: 0.7750\n",
      "MCC: 0.4763\n",
      "G_Mean: 0.7365\n",
      "Youdens_J: 0.4750\n",
      "Balanced_Accuracy: 0.7375\n",
      "Cohens_Kappa: 0.4750\n",
      "Model saved to sklearn_model_fold_6.joblib\n",
      "\n",
      "Fold 7/10\n",
      "Training data shape: (720, 228) (limited for CPU)\n",
      "Validation data shape: (80, 228)\n",
      "Training scikit-learn MLP model...\n",
      "Iteration 1, loss = 0.59910095\n",
      "Validation score: 0.750000\n",
      "Iteration 2, loss = 0.43756125\n",
      "Validation score: 0.763889\n",
      "Iteration 3, loss = 0.37715095\n",
      "Validation score: 0.736111\n",
      "Iteration 4, loss = 0.33683299\n",
      "Validation score: 0.750000\n",
      "Iteration 5, loss = 0.31159057\n",
      "Validation score: 0.722222\n",
      "Iteration 6, loss = 0.27898426\n",
      "Validation score: 0.680556\n",
      "Iteration 7, loss = 0.24522771\n",
      "Validation score: 0.652778\n",
      "Iteration 8, loss = 0.24511651\n",
      "Validation score: 0.680556\n",
      "Validation score did not improve more than tol=0.000100 for 5 consecutive epochs. Stopping.\n",
      "Training completed in 0.31 seconds\n",
      "\n",
      "Fold 7 Results:\n",
      "Fold: 7.0000\n",
      "K: 10.0000\n",
      "Test_Accuracy: 0.8375\n",
      "Train_Accuracy: 0.8222\n",
      "Precision: 0.7838\n",
      "Recall: 0.8529\n",
      "AUC_ROC: 0.8625\n",
      "F1_Score: 0.8169\n",
      "Training_Loss: 0.2451\n",
      "Time_Taken: 0.3080\n",
      "Epochs_Run: 8.0000\n",
      "Specificity: 0.8261\n",
      "MCC: 0.6732\n",
      "G_Mean: 0.8394\n",
      "Youdens_J: 0.6790\n",
      "Balanced_Accuracy: 0.8395\n",
      "Cohens_Kappa: 0.6713\n",
      "Model saved to sklearn_model_fold_7.joblib\n",
      "\n",
      "Fold 8/10\n",
      "Training data shape: (720, 228) (limited for CPU)\n",
      "Validation data shape: (80, 228)\n",
      "Training scikit-learn MLP model...\n",
      "Iteration 1, loss = 0.61930525\n",
      "Validation score: 0.763889\n",
      "Iteration 2, loss = 0.44466381\n",
      "Validation score: 0.750000\n",
      "Iteration 3, loss = 0.38463275\n",
      "Validation score: 0.750000\n",
      "Iteration 4, loss = 0.34327526\n",
      "Validation score: 0.736111\n",
      "Iteration 5, loss = 0.30696132\n",
      "Validation score: 0.736111\n",
      "Iteration 6, loss = 0.28709609\n",
      "Validation score: 0.750000\n",
      "Iteration 7, loss = 0.24454504\n",
      "Validation score: 0.750000\n",
      "Validation score did not improve more than tol=0.000100 for 5 consecutive epochs. Stopping.\n",
      "Training completed in 0.27 seconds\n",
      "\n",
      "Fold 8 Results:\n",
      "Fold: 8.0000\n",
      "K: 10.0000\n",
      "Test_Accuracy: 0.8250\n",
      "Train_Accuracy: 0.7986\n",
      "Precision: 0.7600\n",
      "Recall: 0.9500\n",
      "AUC_ROC: 0.8781\n",
      "F1_Score: 0.8444\n",
      "Training_Loss: 0.2445\n",
      "Time_Taken: 0.2669\n",
      "Epochs_Run: 7.0000\n",
      "Specificity: 0.7000\n",
      "MCC: 0.6713\n",
      "G_Mean: 0.8155\n",
      "Youdens_J: 0.6500\n",
      "Balanced_Accuracy: 0.8250\n",
      "Cohens_Kappa: 0.6500\n",
      "Model saved to sklearn_model_fold_8.joblib\n",
      "\n",
      "Fold 9/10\n",
      "Training data shape: (720, 228) (limited for CPU)\n",
      "Validation data shape: (80, 228)\n",
      "Training scikit-learn MLP model...\n",
      "Iteration 1, loss = 0.60241485\n",
      "Validation score: 0.763889\n",
      "Iteration 2, loss = 0.43327851\n",
      "Validation score: 0.777778\n",
      "Iteration 3, loss = 0.37364986\n",
      "Validation score: 0.763889\n",
      "Iteration 4, loss = 0.33203443\n",
      "Validation score: 0.750000\n",
      "Iteration 5, loss = 0.29966357\n",
      "Validation score: 0.777778\n",
      "Iteration 6, loss = 0.28338889\n",
      "Validation score: 0.777778\n",
      "Iteration 7, loss = 0.25579169\n",
      "Validation score: 0.805556\n",
      "Iteration 8, loss = 0.22931638\n",
      "Validation score: 0.763889\n",
      "Iteration 9, loss = 0.22459391\n",
      "Validation score: 0.777778\n",
      "Iteration 10, loss = 0.20352623\n",
      "Validation score: 0.708333\n",
      "Iteration 11, loss = 0.18988521\n",
      "Validation score: 0.777778\n",
      "Iteration 12, loss = 0.18560160\n",
      "Validation score: 0.736111\n",
      "Iteration 13, loss = 0.17336663\n",
      "Validation score: 0.736111\n",
      "Validation score did not improve more than tol=0.000100 for 5 consecutive epochs. Stopping.\n",
      "Training completed in 1.60 seconds\n",
      "\n",
      "Fold 9 Results:\n",
      "Fold: 9.0000\n",
      "K: 10.0000\n",
      "Test_Accuracy: 0.7250\n",
      "Train_Accuracy: 0.8972\n",
      "Precision: 0.6735\n",
      "Recall: 0.8462\n",
      "AUC_ROC: 0.8605\n",
      "F1_Score: 0.7500\n",
      "Training_Loss: 0.1734\n",
      "Time_Taken: 1.5997\n",
      "Epochs_Run: 13.0000\n",
      "Specificity: 0.6098\n",
      "MCC: 0.4678\n",
      "G_Mean: 0.7183\n",
      "Youdens_J: 0.4559\n",
      "Balanced_Accuracy: 0.7280\n",
      "Cohens_Kappa: 0.4531\n",
      "Model saved to sklearn_model_fold_9.joblib\n",
      "\n",
      "Fold 10/10\n",
      "Training data shape: (720, 228) (limited for CPU)\n",
      "Validation data shape: (80, 228)\n",
      "Training scikit-learn MLP model...\n",
      "Iteration 1, loss = 0.59992233\n",
      "Validation score: 0.722222\n",
      "Iteration 2, loss = 0.43602020\n",
      "Validation score: 0.694444\n",
      "Iteration 3, loss = 0.37228807\n",
      "Validation score: 0.736111\n",
      "Iteration 4, loss = 0.33448491\n",
      "Validation score: 0.722222\n",
      "Iteration 5, loss = 0.30443006\n",
      "Validation score: 0.736111\n",
      "Iteration 6, loss = 0.28414639\n",
      "Validation score: 0.736111\n",
      "Iteration 7, loss = 0.25864547\n",
      "Validation score: 0.722222\n",
      "Iteration 8, loss = 0.23613192\n",
      "Validation score: 0.708333\n",
      "Iteration 9, loss = 0.22453805\n",
      "Validation score: 0.722222\n",
      "Validation score did not improve more than tol=0.000100 for 5 consecutive epochs. Stopping.\n",
      "Training completed in 0.35 seconds\n",
      "\n",
      "Fold 10 Results:\n",
      "Fold: 10.0000\n",
      "K: 10.0000\n",
      "Test_Accuracy: 0.7875\n",
      "Train_Accuracy: 0.8417\n",
      "Precision: 0.8000\n",
      "Recall: 0.8511\n",
      "AUC_ROC: 0.8459\n",
      "F1_Score: 0.8247\n",
      "Training_Loss: 0.2245\n",
      "Time_Taken: 0.3453\n",
      "Epochs_Run: 9.0000\n",
      "Specificity: 0.6970\n",
      "MCC: 0.5573\n",
      "G_Mean: 0.7702\n",
      "Youdens_J: 0.5480\n",
      "Balanced_Accuracy: 0.7740\n",
      "Cohens_Kappa: 0.5556\n",
      "Model saved to sklearn_model_fold_10.joblib\n",
      "\n",
      "=== Starting 15-fold cross-validation ===\n",
      "\n",
      "Fold 1/15\n",
      "Training data shape: (746, 228) (limited for CPU)\n",
      "Validation data shape: (54, 228)\n",
      "Training scikit-learn MLP model...\n",
      "Iteration 1, loss = 0.58781296\n",
      "Validation score: 0.733333\n",
      "Iteration 2, loss = 0.43272472\n",
      "Validation score: 0.760000\n",
      "Iteration 3, loss = 0.37853639\n",
      "Validation score: 0.786667\n",
      "Iteration 4, loss = 0.32989939\n",
      "Validation score: 0.800000\n",
      "Iteration 5, loss = 0.29756781\n",
      "Validation score: 0.786667\n",
      "Iteration 6, loss = 0.26553847\n",
      "Validation score: 0.760000\n",
      "Iteration 7, loss = 0.26063559\n",
      "Validation score: 0.760000\n",
      "Iteration 8, loss = 0.24216946\n",
      "Validation score: 0.760000\n",
      "Iteration 9, loss = 0.22314017\n",
      "Validation score: 0.800000\n",
      "Iteration 10, loss = 0.21478454\n",
      "Validation score: 0.786667\n",
      "Validation score did not improve more than tol=0.000100 for 5 consecutive epochs. Stopping.\n",
      "Training completed in 0.82 seconds\n",
      "\n",
      "Fold 1 Results:\n",
      "Fold: 1.0000\n",
      "K: 15.0000\n",
      "Test_Accuracy: 0.6667\n",
      "Train_Accuracy: 0.8686\n",
      "Precision: 0.6250\n",
      "Recall: 0.7692\n",
      "AUC_ROC: 0.7679\n",
      "F1_Score: 0.6897\n",
      "Training_Loss: 0.2148\n",
      "Time_Taken: 0.8245\n",
      "Epochs_Run: 10.0000\n",
      "Specificity: 0.5714\n",
      "MCC: 0.3464\n",
      "G_Mean: 0.6630\n",
      "Youdens_J: 0.3407\n",
      "Balanced_Accuracy: 0.6703\n",
      "Cohens_Kappa: 0.3379\n",
      "Model saved to sklearn_model_fold_1.joblib\n",
      "\n",
      "Fold 2/15\n",
      "Training data shape: (746, 228) (limited for CPU)\n",
      "Validation data shape: (54, 228)\n",
      "Training scikit-learn MLP model...\n",
      "Iteration 1, loss = 0.60415095\n",
      "Validation score: 0.746667\n",
      "Iteration 2, loss = 0.44114849\n",
      "Validation score: 0.760000\n",
      "Iteration 3, loss = 0.38716123\n",
      "Validation score: 0.720000\n",
      "Iteration 4, loss = 0.33829185\n",
      "Validation score: 0.746667\n",
      "Iteration 5, loss = 0.30801610\n",
      "Validation score: 0.773333\n",
      "Iteration 6, loss = 0.27335606\n",
      "Validation score: 0.706667\n",
      "Iteration 7, loss = 0.26310113\n",
      "Validation score: 0.733333\n",
      "Iteration 8, loss = 0.23393449\n",
      "Validation score: 0.746667\n",
      "Iteration 9, loss = 0.21521986\n",
      "Validation score: 0.800000\n",
      "Iteration 10, loss = 0.21103805\n",
      "Validation score: 0.760000\n",
      "Iteration 11, loss = 0.19582231\n",
      "Validation score: 0.733333\n",
      "Iteration 12, loss = 0.20176120\n",
      "Validation score: 0.760000\n",
      "Iteration 13, loss = 0.17358326\n",
      "Validation score: 0.746667\n",
      "Iteration 14, loss = 0.17227367\n",
      "Validation score: 0.746667\n",
      "Iteration 15, loss = 0.16585603\n",
      "Validation score: 0.773333\n",
      "Validation score did not improve more than tol=0.000100 for 5 consecutive epochs. Stopping.\n",
      "Training completed in 0.66 seconds\n",
      "\n",
      "Fold 2 Results:\n",
      "Fold: 2.0000\n",
      "K: 15.0000\n",
      "Test_Accuracy: 0.7963\n",
      "Train_Accuracy: 0.9088\n",
      "Precision: 0.8148\n",
      "Recall: 0.7857\n",
      "AUC_ROC: 0.8214\n",
      "F1_Score: 0.8000\n",
      "Training_Loss: 0.1659\n",
      "Time_Taken: 0.6569\n",
      "Epochs_Run: 15.0000\n",
      "Specificity: 0.8077\n",
      "MCC: 0.5930\n",
      "G_Mean: 0.7966\n",
      "Youdens_J: 0.5934\n",
      "Balanced_Accuracy: 0.7967\n",
      "Cohens_Kappa: 0.5926\n",
      "Model saved to sklearn_model_fold_2.joblib\n",
      "\n",
      "Fold 3/15\n",
      "Training data shape: (746, 228) (limited for CPU)\n",
      "Validation data shape: (54, 228)\n",
      "Training scikit-learn MLP model...\n",
      "Iteration 1, loss = 0.60953895\n",
      "Validation score: 0.680000\n",
      "Iteration 2, loss = 0.43414049\n",
      "Validation score: 0.680000\n",
      "Iteration 3, loss = 0.37503103\n",
      "Validation score: 0.693333\n",
      "Iteration 4, loss = 0.32530848\n",
      "Validation score: 0.680000\n",
      "Iteration 5, loss = 0.29154614\n",
      "Validation score: 0.680000\n",
      "Iteration 6, loss = 0.27618083\n",
      "Validation score: 0.693333\n",
      "Iteration 7, loss = 0.24596650\n",
      "Validation score: 0.693333\n",
      "Iteration 8, loss = 0.22575708\n",
      "Validation score: 0.693333\n",
      "Iteration 9, loss = 0.20915541\n",
      "Validation score: 0.666667\n",
      "Validation score did not improve more than tol=0.000100 for 5 consecutive epochs. Stopping.\n",
      "Training completed in 0.39 seconds\n",
      "\n",
      "Fold 3 Results:\n",
      "Fold: 3.0000\n",
      "K: 15.0000\n",
      "Test_Accuracy: 0.7407\n",
      "Train_Accuracy: 0.8445\n",
      "Precision: 0.6562\n",
      "Recall: 0.8750\n",
      "AUC_ROC: 0.8792\n",
      "F1_Score: 0.7500\n",
      "Training_Loss: 0.2092\n",
      "Time_Taken: 0.3945\n",
      "Epochs_Run: 9.0000\n",
      "Specificity: 0.6333\n",
      "MCC: 0.5141\n",
      "G_Mean: 0.7444\n",
      "Youdens_J: 0.5083\n",
      "Balanced_Accuracy: 0.7542\n",
      "Cohens_Kappa: 0.4919\n",
      "Model saved to sklearn_model_fold_3.joblib\n",
      "\n",
      "Fold 4/15\n",
      "Training data shape: (746, 228) (limited for CPU)\n",
      "Validation data shape: (54, 228)\n",
      "Training scikit-learn MLP model...\n",
      "Iteration 1, loss = 0.60193844\n",
      "Validation score: 0.746667\n",
      "Iteration 2, loss = 0.43652147\n",
      "Validation score: 0.720000\n",
      "Iteration 3, loss = 0.38163535\n",
      "Validation score: 0.720000\n",
      "Iteration 4, loss = 0.34599535\n",
      "Validation score: 0.733333\n",
      "Iteration 5, loss = 0.31056853\n",
      "Validation score: 0.720000\n",
      "Iteration 6, loss = 0.28143919\n",
      "Validation score: 0.720000\n",
      "Iteration 7, loss = 0.26865611\n",
      "Validation score: 0.813333\n",
      "Iteration 8, loss = 0.24427358\n",
      "Validation score: 0.786667\n",
      "Iteration 9, loss = 0.21857243\n",
      "Validation score: 0.773333\n",
      "Iteration 10, loss = 0.22033552\n",
      "Validation score: 0.826667\n",
      "Iteration 11, loss = 0.19671497\n",
      "Validation score: 0.746667\n",
      "Iteration 12, loss = 0.18908434\n",
      "Validation score: 0.813333\n",
      "Iteration 13, loss = 0.18986623\n",
      "Validation score: 0.760000\n",
      "Iteration 14, loss = 0.17423450\n",
      "Validation score: 0.760000\n",
      "Iteration 15, loss = 0.16458685\n",
      "Validation score: 0.826667\n",
      "Iteration 16, loss = 0.15239907\n",
      "Validation score: 0.720000\n",
      "Validation score did not improve more than tol=0.000100 for 5 consecutive epochs. Stopping.\n",
      "Training completed in 0.69 seconds\n",
      "\n",
      "Fold 4 Results:\n",
      "Fold: 4.0000\n",
      "K: 15.0000\n",
      "Test_Accuracy: 0.7963\n",
      "Train_Accuracy: 0.9155\n",
      "Precision: 0.7391\n",
      "Recall: 0.7727\n",
      "AUC_ROC: 0.8267\n",
      "F1_Score: 0.7556\n",
      "Training_Loss: 0.1524\n",
      "Time_Taken: 0.6928\n",
      "Epochs_Run: 16.0000\n",
      "Specificity: 0.8125\n",
      "MCC: 0.5815\n",
      "G_Mean: 0.7924\n",
      "Youdens_J: 0.5852\n",
      "Balanced_Accuracy: 0.7926\n",
      "Cohens_Kappa: 0.5811\n",
      "Model saved to sklearn_model_fold_4.joblib\n",
      "\n",
      "Fold 5/15\n",
      "Training data shape: (746, 228) (limited for CPU)\n",
      "Validation data shape: (54, 228)\n",
      "Training scikit-learn MLP model...\n",
      "Iteration 1, loss = 0.58900401\n",
      "Validation score: 0.773333\n",
      "Iteration 2, loss = 0.43998252\n",
      "Validation score: 0.786667\n",
      "Iteration 3, loss = 0.38987652\n",
      "Validation score: 0.800000\n",
      "Iteration 4, loss = 0.34960220\n",
      "Validation score: 0.773333\n",
      "Iteration 5, loss = 0.31248788\n",
      "Validation score: 0.813333\n",
      "Iteration 6, loss = 0.29295559\n",
      "Validation score: 0.760000\n",
      "Iteration 7, loss = 0.27427357\n",
      "Validation score: 0.786667\n",
      "Iteration 8, loss = 0.24250475\n",
      "Validation score: 0.773333\n",
      "Iteration 9, loss = 0.22625303\n",
      "Validation score: 0.773333\n",
      "Iteration 10, loss = 0.21707091\n",
      "Validation score: 0.746667\n",
      "Iteration 11, loss = 0.21617491\n",
      "Validation score: 0.773333\n",
      "Validation score did not improve more than tol=0.000100 for 5 consecutive epochs. Stopping.\n",
      "Training completed in 0.48 seconds\n",
      "\n",
      "Fold 5 Results:\n",
      "Fold: 5.0000\n",
      "K: 15.0000\n",
      "Test_Accuracy: 0.7778\n",
      "Train_Accuracy: 0.8847\n",
      "Precision: 0.7692\n",
      "Recall: 0.7692\n",
      "AUC_ROC: 0.8091\n",
      "F1_Score: 0.7692\n",
      "Training_Loss: 0.2162\n",
      "Time_Taken: 0.4835\n",
      "Epochs_Run: 11.0000\n",
      "Specificity: 0.7857\n",
      "MCC: 0.5549\n",
      "G_Mean: 0.7774\n",
      "Youdens_J: 0.5549\n",
      "Balanced_Accuracy: 0.7775\n",
      "Cohens_Kappa: 0.5549\n",
      "Model saved to sklearn_model_fold_5.joblib\n",
      "\n",
      "Fold 6/15\n",
      "Training data shape: (747, 228) (limited for CPU)\n",
      "Validation data shape: (53, 228)\n",
      "Training scikit-learn MLP model...\n",
      "Iteration 1, loss = 0.58390743\n",
      "Validation score: 0.720000\n",
      "Iteration 2, loss = 0.42749554\n",
      "Validation score: 0.720000\n",
      "Iteration 3, loss = 0.36503302\n",
      "Validation score: 0.693333\n",
      "Iteration 4, loss = 0.32642640\n",
      "Validation score: 0.680000\n",
      "Iteration 5, loss = 0.29635795\n",
      "Validation score: 0.706667\n",
      "Iteration 6, loss = 0.27280695\n",
      "Validation score: 0.746667\n",
      "Iteration 7, loss = 0.25559632\n",
      "Validation score: 0.746667\n",
      "Iteration 8, loss = 0.24853535\n",
      "Validation score: 0.786667\n",
      "Iteration 9, loss = 0.21649377\n",
      "Validation score: 0.720000\n",
      "Iteration 10, loss = 0.19898922\n",
      "Validation score: 0.746667\n",
      "Iteration 11, loss = 0.20013266\n",
      "Validation score: 0.746667\n",
      "Iteration 12, loss = 0.19317158\n",
      "Validation score: 0.706667\n",
      "Iteration 13, loss = 0.18835962\n",
      "Validation score: 0.773333\n",
      "Iteration 14, loss = 0.15987283\n",
      "Validation score: 0.733333\n",
      "Validation score did not improve more than tol=0.000100 for 5 consecutive epochs. Stopping.\n",
      "Training completed in 0.59 seconds\n",
      "\n",
      "Fold 6 Results:\n",
      "Fold: 6.0000\n",
      "K: 15.0000\n",
      "Test_Accuracy: 0.6981\n",
      "Train_Accuracy: 0.9143\n",
      "Precision: 0.7586\n",
      "Recall: 0.7097\n",
      "AUC_ROC: 0.7683\n",
      "F1_Score: 0.7333\n",
      "Training_Loss: 0.1599\n",
      "Time_Taken: 0.5892\n",
      "Epochs_Run: 14.0000\n",
      "Specificity: 0.6818\n",
      "MCC: 0.3875\n",
      "G_Mean: 0.6956\n",
      "Youdens_J: 0.3915\n",
      "Balanced_Accuracy: 0.6957\n",
      "Cohens_Kappa: 0.3864\n",
      "Model saved to sklearn_model_fold_6.joblib\n",
      "\n",
      "Fold 7/15\n",
      "Training data shape: (747, 228) (limited for CPU)\n",
      "Validation data shape: (53, 228)\n",
      "Training scikit-learn MLP model...\n",
      "Iteration 1, loss = 0.60769347\n",
      "Validation score: 0.653333\n",
      "Iteration 2, loss = 0.43458255\n",
      "Validation score: 0.640000\n",
      "Iteration 3, loss = 0.37661015\n",
      "Validation score: 0.640000\n",
      "Iteration 4, loss = 0.33671353\n",
      "Validation score: 0.653333\n",
      "Iteration 5, loss = 0.31633548\n",
      "Validation score: 0.733333\n",
      "Iteration 6, loss = 0.27772350\n",
      "Validation score: 0.666667\n",
      "Iteration 7, loss = 0.24503572\n",
      "Validation score: 0.760000\n",
      "Iteration 8, loss = 0.24091342\n",
      "Validation score: 0.733333\n",
      "Iteration 9, loss = 0.22121136\n",
      "Validation score: 0.720000\n",
      "Iteration 10, loss = 0.21957962\n",
      "Validation score: 0.720000\n",
      "Iteration 11, loss = 0.19902824\n",
      "Validation score: 0.706667\n",
      "Iteration 12, loss = 0.18192217\n",
      "Validation score: 0.720000\n",
      "Iteration 13, loss = 0.16678396\n",
      "Validation score: 0.720000\n",
      "Validation score did not improve more than tol=0.000100 for 5 consecutive epochs. Stopping.\n",
      "Training completed in 0.52 seconds\n",
      "\n",
      "Fold 7 Results:\n",
      "Fold: 7.0000\n",
      "K: 15.0000\n",
      "Test_Accuracy: 0.7358\n",
      "Train_Accuracy: 0.9050\n",
      "Precision: 0.7826\n",
      "Recall: 0.6667\n",
      "AUC_ROC: 0.7778\n",
      "F1_Score: 0.7200\n",
      "Training_Loss: 0.1668\n",
      "Time_Taken: 0.5156\n",
      "Epochs_Run: 13.0000\n",
      "Specificity: 0.8077\n",
      "MCC: 0.4785\n",
      "G_Mean: 0.7338\n",
      "Youdens_J: 0.4744\n",
      "Balanced_Accuracy: 0.7372\n",
      "Cohens_Kappa: 0.4730\n",
      "Model saved to sklearn_model_fold_7.joblib\n",
      "\n",
      "Fold 8/15\n",
      "Training data shape: (747, 228) (limited for CPU)\n",
      "Validation data shape: (53, 228)\n",
      "Training scikit-learn MLP model...\n",
      "Iteration 1, loss = 0.55925090\n",
      "Validation score: 0.733333\n",
      "Iteration 2, loss = 0.42758005\n",
      "Validation score: 0.706667\n",
      "Iteration 3, loss = 0.37364593\n",
      "Validation score: 0.733333\n",
      "Iteration 4, loss = 0.32731890\n",
      "Validation score: 0.733333\n",
      "Iteration 5, loss = 0.30917349\n",
      "Validation score: 0.786667\n",
      "Iteration 6, loss = 0.27660980\n",
      "Validation score: 0.773333\n",
      "Iteration 7, loss = 0.25426925\n",
      "Validation score: 0.746667\n",
      "Iteration 8, loss = 0.23319384\n",
      "Validation score: 0.746667\n",
      "Iteration 9, loss = 0.21724535\n",
      "Validation score: 0.746667\n",
      "Iteration 10, loss = 0.20615917\n",
      "Validation score: 0.773333\n",
      "Iteration 11, loss = 0.18715514\n",
      "Validation score: 0.680000\n",
      "Validation score did not improve more than tol=0.000100 for 5 consecutive epochs. Stopping.\n",
      "Training completed in 0.48 seconds\n",
      "\n",
      "Fold 8 Results:\n",
      "Fold: 8.0000\n",
      "K: 15.0000\n",
      "Test_Accuracy: 0.7170\n",
      "Train_Accuracy: 0.8809\n",
      "Precision: 0.7391\n",
      "Recall: 0.6538\n",
      "AUC_ROC: 0.7236\n",
      "F1_Score: 0.6939\n",
      "Training_Loss: 0.1872\n",
      "Time_Taken: 0.4773\n",
      "Epochs_Run: 11.0000\n",
      "Specificity: 0.7778\n",
      "MCC: 0.4354\n",
      "G_Mean: 0.7131\n",
      "Youdens_J: 0.4316\n",
      "Balanced_Accuracy: 0.7158\n",
      "Cohens_Kappa: 0.4325\n",
      "Model saved to sklearn_model_fold_8.joblib\n",
      "\n",
      "Fold 9/15\n",
      "Training data shape: (747, 228) (limited for CPU)\n",
      "Validation data shape: (53, 228)\n",
      "Training scikit-learn MLP model...\n",
      "Iteration 1, loss = 0.59170665\n",
      "Validation score: 0.853333\n",
      "Iteration 2, loss = 0.44962482\n",
      "Validation score: 0.826667\n",
      "Iteration 3, loss = 0.39295711\n",
      "Validation score: 0.880000\n",
      "Iteration 4, loss = 0.34511406\n",
      "Validation score: 0.853333\n",
      "Iteration 5, loss = 0.31048480\n",
      "Validation score: 0.840000\n",
      "Iteration 6, loss = 0.28159658\n",
      "Validation score: 0.826667\n",
      "Iteration 7, loss = 0.27063813\n",
      "Validation score: 0.853333\n",
      "Iteration 8, loss = 0.24577318\n",
      "Validation score: 0.800000\n",
      "Iteration 9, loss = 0.23096392\n",
      "Validation score: 0.866667\n",
      "Validation score did not improve more than tol=0.000100 for 5 consecutive epochs. Stopping.\n",
      "Training completed in 0.37 seconds\n",
      "\n",
      "Fold 9 Results:\n",
      "Fold: 9.0000\n",
      "K: 15.0000\n",
      "Test_Accuracy: 0.7736\n",
      "Train_Accuracy: 0.8581\n",
      "Precision: 0.7500\n",
      "Recall: 0.8077\n",
      "AUC_ROC: 0.8134\n",
      "F1_Score: 0.7778\n",
      "Training_Loss: 0.2310\n",
      "Time_Taken: 0.3681\n",
      "Epochs_Run: 9.0000\n",
      "Specificity: 0.7407\n",
      "MCC: 0.5492\n",
      "G_Mean: 0.7735\n",
      "Youdens_J: 0.5484\n",
      "Balanced_Accuracy: 0.7742\n",
      "Cohens_Kappa: 0.5477\n",
      "Model saved to sklearn_model_fold_9.joblib\n",
      "\n",
      "Fold 10/15\n",
      "Training data shape: (747, 228) (limited for CPU)\n",
      "Validation data shape: (53, 228)\n",
      "Training scikit-learn MLP model...\n",
      "Iteration 1, loss = 0.60217059\n",
      "Validation score: 0.720000\n",
      "Iteration 2, loss = 0.44888466\n",
      "Validation score: 0.733333\n",
      "Iteration 3, loss = 0.38027861\n",
      "Validation score: 0.706667\n",
      "Iteration 4, loss = 0.33715993\n",
      "Validation score: 0.706667\n",
      "Iteration 5, loss = 0.30640334\n",
      "Validation score: 0.720000\n",
      "Iteration 6, loss = 0.28250853\n",
      "Validation score: 0.720000\n",
      "Iteration 7, loss = 0.27189170\n",
      "Validation score: 0.693333\n",
      "Iteration 8, loss = 0.24533840\n",
      "Validation score: 0.706667\n",
      "Validation score did not improve more than tol=0.000100 for 5 consecutive epochs. Stopping.\n",
      "Training completed in 0.36 seconds\n",
      "\n",
      "Fold 10 Results:\n",
      "Fold: 10.0000\n",
      "K: 15.0000\n",
      "Test_Accuracy: 0.8302\n",
      "Train_Accuracy: 0.8313\n",
      "Precision: 0.8077\n",
      "Recall: 0.8400\n",
      "AUC_ROC: 0.8629\n",
      "F1_Score: 0.8235\n",
      "Training_Loss: 0.2453\n",
      "Time_Taken: 0.3589\n",
      "Epochs_Run: 8.0000\n",
      "Specificity: 0.8214\n",
      "MCC: 0.6605\n",
      "G_Mean: 0.8307\n",
      "Youdens_J: 0.6614\n",
      "Balanced_Accuracy: 0.8307\n",
      "Cohens_Kappa: 0.6600\n",
      "Model saved to sklearn_model_fold_10.joblib\n",
      "\n",
      "Fold 11/15\n",
      "Training data shape: (747, 228) (limited for CPU)\n",
      "Validation data shape: (53, 228)\n",
      "Training scikit-learn MLP model...\n",
      "Iteration 1, loss = 0.61283189\n",
      "Validation score: 0.786667\n",
      "Iteration 2, loss = 0.44501695\n",
      "Validation score: 0.773333\n",
      "Iteration 3, loss = 0.38060197\n",
      "Validation score: 0.773333\n",
      "Iteration 4, loss = 0.33593341\n",
      "Validation score: 0.786667\n",
      "Iteration 5, loss = 0.31029157\n",
      "Validation score: 0.746667\n",
      "Iteration 6, loss = 0.27708185\n",
      "Validation score: 0.760000\n",
      "Iteration 7, loss = 0.26075122\n",
      "Validation score: 0.800000\n",
      "Iteration 8, loss = 0.23239679\n",
      "Validation score: 0.800000\n",
      "Iteration 9, loss = 0.22429064\n",
      "Validation score: 0.773333\n",
      "Iteration 10, loss = 0.22662380\n",
      "Validation score: 0.840000\n",
      "Iteration 11, loss = 0.21308493\n",
      "Validation score: 0.786667\n",
      "Iteration 12, loss = 0.18186905\n",
      "Validation score: 0.760000\n",
      "Iteration 13, loss = 0.16729589\n",
      "Validation score: 0.786667\n",
      "Iteration 14, loss = 0.18129205\n",
      "Validation score: 0.800000\n",
      "Iteration 15, loss = 0.17057621\n",
      "Validation score: 0.786667\n",
      "Iteration 16, loss = 0.13658543\n",
      "Validation score: 0.800000\n",
      "Validation score did not improve more than tol=0.000100 for 5 consecutive epochs. Stopping.\n",
      "Training completed in 1.05 seconds\n",
      "\n",
      "Fold 11 Results:\n",
      "Fold: 11.0000\n",
      "K: 15.0000\n",
      "Test_Accuracy: 0.7925\n",
      "Train_Accuracy: 0.9210\n",
      "Precision: 0.8148\n",
      "Recall: 0.7857\n",
      "AUC_ROC: 0.8771\n",
      "F1_Score: 0.8000\n",
      "Training_Loss: 0.1366\n",
      "Time_Taken: 1.0481\n",
      "Epochs_Run: 16.0000\n",
      "Specificity: 0.8000\n",
      "MCC: 0.5849\n",
      "G_Mean: 0.7928\n",
      "Youdens_J: 0.5857\n",
      "Balanced_Accuracy: 0.7929\n",
      "Cohens_Kappa: 0.5845\n",
      "Model saved to sklearn_model_fold_11.joblib\n",
      "\n",
      "Fold 12/15\n",
      "Training data shape: (747, 228) (limited for CPU)\n",
      "Validation data shape: (53, 228)\n",
      "Training scikit-learn MLP model...\n",
      "Iteration 1, loss = 0.60602517\n",
      "Validation score: 0.760000\n",
      "Iteration 2, loss = 0.44971511\n",
      "Validation score: 0.786667\n",
      "Iteration 3, loss = 0.38619719\n",
      "Validation score: 0.786667\n",
      "Iteration 4, loss = 0.34203579\n",
      "Validation score: 0.840000\n",
      "Iteration 5, loss = 0.31873334\n",
      "Validation score: 0.800000\n",
      "Iteration 6, loss = 0.28387430\n",
      "Validation score: 0.813333\n",
      "Iteration 7, loss = 0.26723394\n",
      "Validation score: 0.840000\n",
      "Iteration 8, loss = 0.25030588\n",
      "Validation score: 0.853333\n",
      "Iteration 9, loss = 0.22854908\n",
      "Validation score: 0.840000\n",
      "Iteration 10, loss = 0.20469969\n",
      "Validation score: 0.853333\n",
      "Iteration 11, loss = 0.19640662\n",
      "Validation score: 0.826667\n",
      "Iteration 12, loss = 0.19690913\n",
      "Validation score: 0.840000\n",
      "Iteration 13, loss = 0.18137118\n",
      "Validation score: 0.853333\n",
      "Iteration 14, loss = 0.15789077\n",
      "Validation score: 0.866667\n",
      "Iteration 15, loss = 0.15347449\n",
      "Validation score: 0.853333\n",
      "Iteration 16, loss = 0.13800296\n",
      "Validation score: 0.866667\n",
      "Iteration 17, loss = 0.12296114\n",
      "Validation score: 0.840000\n",
      "Iteration 18, loss = 0.11663888\n",
      "Validation score: 0.853333\n",
      "Iteration 19, loss = 0.11763552\n",
      "Validation score: 0.826667\n",
      "Iteration 20, loss = 0.12336883\n",
      "Validation score: 0.853333\n",
      "Validation score did not improve more than tol=0.000100 for 5 consecutive epochs. Stopping.\n",
      "Training completed in 0.84 seconds\n",
      "\n",
      "Fold 12 Results:\n",
      "Fold: 12.0000\n",
      "K: 15.0000\n",
      "Test_Accuracy: 0.7358\n",
      "Train_Accuracy: 0.9357\n",
      "Precision: 0.6667\n",
      "Recall: 0.6667\n",
      "AUC_ROC: 0.8690\n",
      "F1_Score: 0.6667\n",
      "Training_Loss: 0.1234\n",
      "Time_Taken: 0.8359\n",
      "Epochs_Run: 20.0000\n",
      "Specificity: 0.7812\n",
      "MCC: 0.4479\n",
      "G_Mean: 0.7217\n",
      "Youdens_J: 0.4479\n",
      "Balanced_Accuracy: 0.7240\n",
      "Cohens_Kappa: 0.4479\n",
      "Model saved to sklearn_model_fold_12.joblib\n",
      "\n",
      "Fold 13/15\n",
      "Training data shape: (747, 228) (limited for CPU)\n",
      "Validation data shape: (53, 228)\n",
      "Training scikit-learn MLP model...\n",
      "Iteration 1, loss = 0.60052599\n",
      "Validation score: 0.720000\n",
      "Iteration 2, loss = 0.44608417\n",
      "Validation score: 0.693333\n",
      "Iteration 3, loss = 0.38447628\n",
      "Validation score: 0.733333\n",
      "Iteration 4, loss = 0.33898211\n",
      "Validation score: 0.680000\n",
      "Iteration 5, loss = 0.31109404\n",
      "Validation score: 0.720000\n",
      "Iteration 6, loss = 0.27240373\n",
      "Validation score: 0.733333\n",
      "Iteration 7, loss = 0.24699914\n",
      "Validation score: 0.693333\n",
      "Iteration 8, loss = 0.22593576\n",
      "Validation score: 0.693333\n",
      "Iteration 9, loss = 0.23268422\n",
      "Validation score: 0.760000\n",
      "Iteration 10, loss = 0.19994189\n",
      "Validation score: 0.746667\n",
      "Iteration 11, loss = 0.18121690\n",
      "Validation score: 0.680000\n",
      "Iteration 12, loss = 0.16318673\n",
      "Validation score: 0.720000\n",
      "Iteration 13, loss = 0.14825521\n",
      "Validation score: 0.653333\n",
      "Iteration 14, loss = 0.14252428\n",
      "Validation score: 0.693333\n",
      "Iteration 15, loss = 0.15516332\n",
      "Validation score: 0.706667\n",
      "Validation score did not improve more than tol=0.000100 for 5 consecutive epochs. Stopping.\n",
      "Training completed in 0.61 seconds\n",
      "\n",
      "Fold 13 Results:\n",
      "Fold: 13.0000\n",
      "K: 15.0000\n",
      "Test_Accuracy: 0.7547\n",
      "Train_Accuracy: 0.9143\n",
      "Precision: 0.7200\n",
      "Recall: 0.7500\n",
      "AUC_ROC: 0.8693\n",
      "F1_Score: 0.7347\n",
      "Training_Loss: 0.1552\n",
      "Time_Taken: 0.6063\n",
      "Epochs_Run: 15.0000\n",
      "Specificity: 0.7586\n",
      "MCC: 0.5072\n",
      "G_Mean: 0.7543\n",
      "Youdens_J: 0.5086\n",
      "Balanced_Accuracy: 0.7543\n",
      "Cohens_Kappa: 0.5068\n",
      "Model saved to sklearn_model_fold_13.joblib\n",
      "\n",
      "Fold 14/15\n",
      "Training data shape: (747, 228) (limited for CPU)\n",
      "Validation data shape: (53, 228)\n",
      "Training scikit-learn MLP model...\n",
      "Iteration 1, loss = 0.58241018\n",
      "Validation score: 0.773333\n",
      "Iteration 2, loss = 0.43123626\n",
      "Validation score: 0.680000\n",
      "Iteration 3, loss = 0.36914170\n",
      "Validation score: 0.706667\n",
      "Iteration 4, loss = 0.32996046\n",
      "Validation score: 0.693333\n",
      "Iteration 5, loss = 0.30329667\n",
      "Validation score: 0.693333\n",
      "Iteration 6, loss = 0.27477358\n",
      "Validation score: 0.680000\n",
      "Iteration 7, loss = 0.25642809\n",
      "Validation score: 0.666667\n",
      "Validation score did not improve more than tol=0.000100 for 5 consecutive epochs. Stopping.\n",
      "Training completed in 0.28 seconds\n",
      "\n",
      "Fold 14 Results:\n",
      "Fold: 14.0000\n",
      "K: 15.0000\n",
      "Test_Accuracy: 0.6981\n",
      "Train_Accuracy: 0.7925\n",
      "Precision: 0.7647\n",
      "Recall: 0.7647\n",
      "AUC_ROC: 0.7957\n",
      "F1_Score: 0.7647\n",
      "Training_Loss: 0.2564\n",
      "Time_Taken: 0.2760\n",
      "Epochs_Run: 7.0000\n",
      "Specificity: 0.5789\n",
      "MCC: 0.3437\n",
      "G_Mean: 0.6654\n",
      "Youdens_J: 0.3437\n",
      "Balanced_Accuracy: 0.6718\n",
      "Cohens_Kappa: 0.3437\n",
      "Model saved to sklearn_model_fold_14.joblib\n",
      "\n",
      "Fold 15/15\n",
      "Training data shape: (747, 228) (limited for CPU)\n",
      "Validation data shape: (53, 228)\n",
      "Training scikit-learn MLP model...\n",
      "Iteration 1, loss = 0.60577966\n",
      "Validation score: 0.760000\n",
      "Iteration 2, loss = 0.44072204\n",
      "Validation score: 0.773333\n",
      "Iteration 3, loss = 0.37951104\n",
      "Validation score: 0.746667\n",
      "Iteration 4, loss = 0.34360177\n",
      "Validation score: 0.760000\n",
      "Iteration 5, loss = 0.30635535\n",
      "Validation score: 0.800000\n",
      "Iteration 6, loss = 0.27227218\n",
      "Validation score: 0.840000\n",
      "Iteration 7, loss = 0.26460301\n",
      "Validation score: 0.826667\n",
      "Iteration 8, loss = 0.23735247\n",
      "Validation score: 0.786667\n",
      "Iteration 9, loss = 0.22290151\n",
      "Validation score: 0.773333\n",
      "Iteration 10, loss = 0.20820455\n",
      "Validation score: 0.773333\n",
      "Iteration 11, loss = 0.21611876\n",
      "Validation score: 0.706667\n",
      "Iteration 12, loss = 0.19152825\n",
      "Validation score: 0.786667\n",
      "Validation score did not improve more than tol=0.000100 for 5 consecutive epochs. Stopping.\n",
      "Training completed in 0.56 seconds\n",
      "\n",
      "Fold 15 Results:\n",
      "Fold: 15.0000\n",
      "K: 15.0000\n",
      "Test_Accuracy: 0.7736\n",
      "Train_Accuracy: 0.8996\n",
      "Precision: 0.7857\n",
      "Recall: 0.7857\n",
      "AUC_ROC: 0.8157\n",
      "F1_Score: 0.7857\n",
      "Training_Loss: 0.1915\n",
      "Time_Taken: 0.5568\n",
      "Epochs_Run: 12.0000\n",
      "Specificity: 0.7600\n",
      "MCC: 0.5457\n",
      "G_Mean: 0.7728\n",
      "Youdens_J: 0.5457\n",
      "Balanced_Accuracy: 0.7729\n",
      "Cohens_Kappa: 0.5457\n",
      "Model saved to sklearn_model_fold_15.joblib\n",
      "\n",
      "=== Starting 20-fold cross-validation ===\n",
      "\n",
      "Fold 1/20\n",
      "Training data shape: (760, 228) (limited for CPU)\n",
      "Validation data shape: (40, 228)\n",
      "Training scikit-learn MLP model...\n",
      "Iteration 1, loss = 0.58410345\n",
      "Validation score: 0.750000\n",
      "Iteration 2, loss = 0.43113757\n",
      "Validation score: 0.789474\n",
      "Iteration 3, loss = 0.37047713\n",
      "Validation score: 0.815789\n",
      "Iteration 4, loss = 0.32910893\n",
      "Validation score: 0.763158\n",
      "Iteration 5, loss = 0.30022708\n",
      "Validation score: 0.815789\n",
      "Iteration 6, loss = 0.26688228\n",
      "Validation score: 0.815789\n",
      "Iteration 7, loss = 0.24708209\n",
      "Validation score: 0.815789\n",
      "Iteration 8, loss = 0.23731479\n",
      "Validation score: 0.815789\n",
      "Iteration 9, loss = 0.23167595\n",
      "Validation score: 0.815789\n",
      "Validation score did not improve more than tol=0.000100 for 5 consecutive epochs. Stopping.\n",
      "Training completed in 0.42 seconds\n",
      "\n",
      "Fold 1 Results:\n",
      "Fold: 1.0000\n",
      "K: 20.0000\n",
      "Test_Accuracy: 0.7250\n",
      "Train_Accuracy: 0.8618\n",
      "Precision: 0.6957\n",
      "Recall: 0.8000\n",
      "AUC_ROC: 0.7800\n",
      "F1_Score: 0.7442\n",
      "Training_Loss: 0.2317\n",
      "Time_Taken: 0.4247\n",
      "Epochs_Run: 9.0000\n",
      "Specificity: 0.6500\n",
      "MCC: 0.4551\n",
      "G_Mean: 0.7211\n",
      "Youdens_J: 0.4500\n",
      "Balanced_Accuracy: 0.7250\n",
      "Cohens_Kappa: 0.4500\n",
      "Model saved to sklearn_model_fold_1.joblib\n",
      "\n",
      "Fold 2/20\n",
      "Training data shape: (760, 228) (limited for CPU)\n",
      "Validation data shape: (40, 228)\n",
      "Training scikit-learn MLP model...\n",
      "Iteration 1, loss = 0.58275243\n",
      "Validation score: 0.710526\n",
      "Iteration 2, loss = 0.43329569\n",
      "Validation score: 0.684211\n",
      "Iteration 3, loss = 0.36635233\n",
      "Validation score: 0.697368\n",
      "Iteration 4, loss = 0.32613376\n",
      "Validation score: 0.710526\n",
      "Iteration 5, loss = 0.29339803\n",
      "Validation score: 0.763158\n",
      "Iteration 6, loss = 0.27639536\n",
      "Validation score: 0.710526\n",
      "Iteration 7, loss = 0.26085299\n",
      "Validation score: 0.710526\n",
      "Iteration 8, loss = 0.24371102\n",
      "Validation score: 0.723684\n",
      "Iteration 9, loss = 0.22418088\n",
      "Validation score: 0.763158\n",
      "Iteration 10, loss = 0.21082271\n",
      "Validation score: 0.671053\n",
      "Iteration 11, loss = 0.19562473\n",
      "Validation score: 0.684211\n",
      "Validation score did not improve more than tol=0.000100 for 5 consecutive epochs. Stopping.\n",
      "Training completed in 0.52 seconds\n",
      "\n",
      "Fold 2 Results:\n",
      "Fold: 2.0000\n",
      "K: 20.0000\n",
      "Test_Accuracy: 0.7250\n",
      "Train_Accuracy: 0.8776\n",
      "Precision: 0.7500\n",
      "Recall: 0.6316\n",
      "AUC_ROC: 0.8421\n",
      "F1_Score: 0.6857\n",
      "Training_Loss: 0.1956\n",
      "Time_Taken: 0.5233\n",
      "Epochs_Run: 11.0000\n",
      "Specificity: 0.8095\n",
      "MCC: 0.4496\n",
      "G_Mean: 0.7150\n",
      "Youdens_J: 0.4411\n",
      "Balanced_Accuracy: 0.7206\n",
      "Cohens_Kappa: 0.4444\n",
      "Model saved to sklearn_model_fold_2.joblib\n",
      "\n",
      "Fold 3/20\n",
      "Training data shape: (760, 228) (limited for CPU)\n",
      "Validation data shape: (40, 228)\n",
      "Training scikit-learn MLP model...\n",
      "Iteration 1, loss = 0.61034398\n",
      "Validation score: 0.776316\n",
      "Iteration 2, loss = 0.44988401\n",
      "Validation score: 0.789474\n",
      "Iteration 3, loss = 0.38276339\n",
      "Validation score: 0.789474\n",
      "Iteration 4, loss = 0.34377164\n",
      "Validation score: 0.789474\n",
      "Iteration 5, loss = 0.29990464\n",
      "Validation score: 0.736842\n",
      "Iteration 6, loss = 0.28202470\n",
      "Validation score: 0.736842\n",
      "Iteration 7, loss = 0.26465788\n",
      "Validation score: 0.750000\n",
      "Iteration 8, loss = 0.25193059\n",
      "Validation score: 0.723684\n",
      "Validation score did not improve more than tol=0.000100 for 5 consecutive epochs. Stopping.\n",
      "Training completed in 0.72 seconds\n",
      "\n",
      "Fold 3 Results:\n",
      "Fold: 3.0000\n",
      "K: 20.0000\n",
      "Test_Accuracy: 0.7500\n",
      "Train_Accuracy: 0.8289\n",
      "Precision: 0.6957\n",
      "Recall: 0.8421\n",
      "AUC_ROC: 0.8271\n",
      "F1_Score: 0.7619\n",
      "Training_Loss: 0.2519\n",
      "Time_Taken: 0.7173\n",
      "Epochs_Run: 8.0000\n",
      "Specificity: 0.6667\n",
      "MCC: 0.5140\n",
      "G_Mean: 0.7493\n",
      "Youdens_J: 0.5088\n",
      "Balanced_Accuracy: 0.7544\n",
      "Cohens_Kappa: 0.5037\n",
      "Model saved to sklearn_model_fold_3.joblib\n",
      "\n",
      "Fold 4/20\n",
      "Training data shape: (760, 228) (limited for CPU)\n",
      "Validation data shape: (40, 228)\n",
      "Training scikit-learn MLP model...\n",
      "Iteration 1, loss = 0.61293316\n",
      "Validation score: 0.750000\n",
      "Iteration 2, loss = 0.45186057\n",
      "Validation score: 0.763158\n",
      "Iteration 3, loss = 0.38797088\n",
      "Validation score: 0.776316\n",
      "Iteration 4, loss = 0.34053806\n",
      "Validation score: 0.763158\n",
      "Iteration 5, loss = 0.31163509\n",
      "Validation score: 0.776316\n",
      "Iteration 6, loss = 0.29910310\n",
      "Validation score: 0.802632\n",
      "Iteration 7, loss = 0.26749093\n",
      "Validation score: 0.776316\n",
      "Iteration 8, loss = 0.23766468\n",
      "Validation score: 0.802632\n",
      "Iteration 9, loss = 0.21613084\n",
      "Validation score: 0.789474\n",
      "Iteration 10, loss = 0.19529886\n",
      "Validation score: 0.776316\n",
      "Iteration 11, loss = 0.19899470\n",
      "Validation score: 0.789474\n",
      "Iteration 12, loss = 0.17921765\n",
      "Validation score: 0.750000\n",
      "Validation score did not improve more than tol=0.000100 for 5 consecutive epochs. Stopping.\n",
      "Training completed in 0.49 seconds\n",
      "\n",
      "Fold 4 Results:\n",
      "Fold: 4.0000\n",
      "K: 20.0000\n",
      "Test_Accuracy: 0.8000\n",
      "Train_Accuracy: 0.8947\n",
      "Precision: 0.7619\n",
      "Recall: 0.8421\n",
      "AUC_ROC: 0.8847\n",
      "F1_Score: 0.8000\n",
      "Training_Loss: 0.1792\n",
      "Time_Taken: 0.4927\n",
      "Epochs_Run: 12.0000\n",
      "Specificity: 0.7619\n",
      "MCC: 0.6040\n",
      "G_Mean: 0.8010\n",
      "Youdens_J: 0.6040\n",
      "Balanced_Accuracy: 0.8020\n",
      "Cohens_Kappa: 0.6010\n",
      "Model saved to sklearn_model_fold_4.joblib\n",
      "\n",
      "Fold 5/20\n",
      "Training data shape: (760, 228) (limited for CPU)\n",
      "Validation data shape: (40, 228)\n",
      "Training scikit-learn MLP model...\n",
      "Iteration 1, loss = 0.58790776\n",
      "Validation score: 0.723684\n",
      "Iteration 2, loss = 0.43693203\n",
      "Validation score: 0.710526\n",
      "Iteration 3, loss = 0.37349216\n",
      "Validation score: 0.736842\n",
      "Iteration 4, loss = 0.33702346\n",
      "Validation score: 0.763158\n",
      "Iteration 5, loss = 0.30654108\n",
      "Validation score: 0.750000\n",
      "Iteration 6, loss = 0.28080006\n",
      "Validation score: 0.750000\n",
      "Iteration 7, loss = 0.26013697\n",
      "Validation score: 0.750000\n",
      "Iteration 8, loss = 0.24028970\n",
      "Validation score: 0.750000\n",
      "Iteration 9, loss = 0.23447862\n",
      "Validation score: 0.697368\n",
      "Iteration 10, loss = 0.21628026\n",
      "Validation score: 0.736842\n",
      "Validation score did not improve more than tol=0.000100 for 5 consecutive epochs. Stopping.\n",
      "Training completed in 0.45 seconds\n",
      "\n",
      "Fold 5 Results:\n",
      "Fold: 5.0000\n",
      "K: 20.0000\n",
      "Test_Accuracy: 0.7250\n",
      "Train_Accuracy: 0.8592\n",
      "Precision: 0.7143\n",
      "Recall: 0.5882\n",
      "AUC_ROC: 0.7698\n",
      "F1_Score: 0.6452\n",
      "Training_Loss: 0.2163\n",
      "Time_Taken: 0.4508\n",
      "Epochs_Run: 10.0000\n",
      "Specificity: 0.8261\n",
      "MCC: 0.4294\n",
      "G_Mean: 0.6971\n",
      "Youdens_J: 0.4143\n",
      "Balanced_Accuracy: 0.7072\n",
      "Cohens_Kappa: 0.4241\n",
      "Model saved to sklearn_model_fold_5.joblib\n",
      "\n",
      "Fold 6/20\n",
      "Training data shape: (760, 228) (limited for CPU)\n",
      "Validation data shape: (40, 228)\n",
      "Training scikit-learn MLP model...\n",
      "Iteration 1, loss = 0.61594994\n",
      "Validation score: 0.776316\n",
      "Iteration 2, loss = 0.44358323\n",
      "Validation score: 0.736842\n",
      "Iteration 3, loss = 0.38132018\n",
      "Validation score: 0.723684\n",
      "Iteration 4, loss = 0.35118215\n",
      "Validation score: 0.736842\n",
      "Iteration 5, loss = 0.31290391\n",
      "Validation score: 0.750000\n",
      "Iteration 6, loss = 0.28127521\n",
      "Validation score: 0.776316\n",
      "Iteration 7, loss = 0.26408121\n",
      "Validation score: 0.736842\n",
      "Validation score did not improve more than tol=0.000100 for 5 consecutive epochs. Stopping.\n",
      "Training completed in 0.29 seconds\n",
      "\n",
      "Fold 6 Results:\n",
      "Fold: 6.0000\n",
      "K: 20.0000\n",
      "Test_Accuracy: 0.7750\n",
      "Train_Accuracy: 0.7961\n",
      "Precision: 0.6818\n",
      "Recall: 0.8824\n",
      "AUC_ROC: 0.8721\n",
      "F1_Score: 0.7692\n",
      "Training_Loss: 0.2641\n",
      "Time_Taken: 0.2938\n",
      "Epochs_Run: 7.0000\n",
      "Specificity: 0.6957\n",
      "MCC: 0.5743\n",
      "G_Mean: 0.7835\n",
      "Youdens_J: 0.5780\n",
      "Balanced_Accuracy: 0.7890\n",
      "Cohens_Kappa: 0.5567\n",
      "Model saved to sklearn_model_fold_6.joblib\n",
      "\n",
      "Fold 7/20\n",
      "Training data shape: (760, 228) (limited for CPU)\n",
      "Validation data shape: (40, 228)\n",
      "Training scikit-learn MLP model...\n",
      "Iteration 1, loss = 0.58126510\n",
      "Validation score: 0.710526\n",
      "Iteration 2, loss = 0.43598704\n",
      "Validation score: 0.750000\n",
      "Iteration 3, loss = 0.37261594\n",
      "Validation score: 0.723684\n",
      "Iteration 4, loss = 0.33103572\n",
      "Validation score: 0.763158\n",
      "Iteration 5, loss = 0.30262777\n",
      "Validation score: 0.763158\n",
      "Iteration 6, loss = 0.27405623\n",
      "Validation score: 0.763158\n",
      "Iteration 7, loss = 0.25052952\n",
      "Validation score: 0.750000\n",
      "Iteration 8, loss = 0.23092637\n",
      "Validation score: 0.763158\n",
      "Iteration 9, loss = 0.22667156\n",
      "Validation score: 0.750000\n",
      "Iteration 10, loss = 0.20734895\n",
      "Validation score: 0.750000\n",
      "Validation score did not improve more than tol=0.000100 for 5 consecutive epochs. Stopping.\n",
      "Training completed in 0.49 seconds\n",
      "\n",
      "Fold 7 Results:\n",
      "Fold: 7.0000\n",
      "K: 20.0000\n",
      "Test_Accuracy: 0.7250\n",
      "Train_Accuracy: 0.8671\n",
      "Precision: 0.7895\n",
      "Recall: 0.6818\n",
      "AUC_ROC: 0.7904\n",
      "F1_Score: 0.7317\n",
      "Training_Loss: 0.2073\n",
      "Time_Taken: 0.4892\n",
      "Epochs_Run: 10.0000\n",
      "Specificity: 0.7778\n",
      "MCC: 0.4579\n",
      "G_Mean: 0.7282\n",
      "Youdens_J: 0.4596\n",
      "Balanced_Accuracy: 0.7298\n",
      "Cohens_Kappa: 0.4527\n",
      "Model saved to sklearn_model_fold_7.joblib\n",
      "\n",
      "Fold 8/20\n",
      "Training data shape: (760, 228) (limited for CPU)\n",
      "Validation data shape: (40, 228)\n",
      "Training scikit-learn MLP model...\n",
      "Iteration 1, loss = 0.59681690\n",
      "Validation score: 0.736842\n",
      "Iteration 2, loss = 0.43429614\n",
      "Validation score: 0.750000\n",
      "Iteration 3, loss = 0.36479790\n",
      "Validation score: 0.763158\n",
      "Iteration 4, loss = 0.32464597\n",
      "Validation score: 0.736842\n",
      "Iteration 5, loss = 0.30037151\n",
      "Validation score: 0.776316\n",
      "Iteration 6, loss = 0.27233907\n",
      "Validation score: 0.723684\n",
      "Iteration 7, loss = 0.24640956\n",
      "Validation score: 0.710526\n",
      "Iteration 8, loss = 0.23049655\n",
      "Validation score: 0.710526\n",
      "Iteration 9, loss = 0.20675468\n",
      "Validation score: 0.684211\n",
      "Iteration 10, loss = 0.21760782\n",
      "Validation score: 0.710526\n",
      "Iteration 11, loss = 0.19212531\n",
      "Validation score: 0.697368\n",
      "Validation score did not improve more than tol=0.000100 for 5 consecutive epochs. Stopping.\n",
      "Training completed in 0.46 seconds\n",
      "\n",
      "Fold 8 Results:\n",
      "Fold: 8.0000\n",
      "K: 20.0000\n",
      "Test_Accuracy: 0.6750\n",
      "Train_Accuracy: 0.8842\n",
      "Precision: 0.7500\n",
      "Recall: 0.6522\n",
      "AUC_ROC: 0.7468\n",
      "F1_Score: 0.6977\n",
      "Training_Loss: 0.1921\n",
      "Time_Taken: 0.4647\n",
      "Epochs_Run: 11.0000\n",
      "Specificity: 0.7059\n",
      "MCC: 0.3540\n",
      "G_Mean: 0.6785\n",
      "Youdens_J: 0.3581\n",
      "Balanced_Accuracy: 0.6790\n",
      "Cohens_Kappa: 0.3500\n",
      "Model saved to sklearn_model_fold_8.joblib\n",
      "\n",
      "Fold 9/20\n",
      "Training data shape: (760, 228) (limited for CPU)\n",
      "Validation data shape: (40, 228)\n",
      "Training scikit-learn MLP model...\n",
      "Iteration 1, loss = 0.61078717\n",
      "Validation score: 0.763158\n",
      "Iteration 2, loss = 0.44346334\n",
      "Validation score: 0.763158\n",
      "Iteration 3, loss = 0.39421597\n",
      "Validation score: 0.736842\n",
      "Iteration 4, loss = 0.34190008\n",
      "Validation score: 0.763158\n",
      "Iteration 5, loss = 0.31711505\n",
      "Validation score: 0.789474\n",
      "Iteration 6, loss = 0.28651113\n",
      "Validation score: 0.750000\n",
      "Iteration 7, loss = 0.26395069\n",
      "Validation score: 0.815789\n",
      "Iteration 8, loss = 0.24268055\n",
      "Validation score: 0.763158\n",
      "Iteration 9, loss = 0.22791621\n",
      "Validation score: 0.736842\n",
      "Iteration 10, loss = 0.21727690\n",
      "Validation score: 0.763158\n",
      "Iteration 11, loss = 0.19485115\n",
      "Validation score: 0.789474\n",
      "Iteration 12, loss = 0.18138854\n",
      "Validation score: 0.776316\n",
      "Iteration 13, loss = 0.16826473\n",
      "Validation score: 0.763158\n",
      "Validation score did not improve more than tol=0.000100 for 5 consecutive epochs. Stopping.\n",
      "Training completed in 0.91 seconds\n",
      "\n",
      "Fold 9 Results:\n",
      "Fold: 9.0000\n",
      "K: 20.0000\n",
      "Test_Accuracy: 0.8250\n",
      "Train_Accuracy: 0.9000\n",
      "Precision: 0.8182\n",
      "Recall: 0.8571\n",
      "AUC_ROC: 0.8446\n",
      "F1_Score: 0.8372\n",
      "Training_Loss: 0.1683\n",
      "Time_Taken: 0.9102\n",
      "Epochs_Run: 13.0000\n",
      "Specificity: 0.7895\n",
      "MCC: 0.6491\n",
      "G_Mean: 0.8226\n",
      "Youdens_J: 0.6466\n",
      "Balanced_Accuracy: 0.8233\n",
      "Cohens_Kappa: 0.6482\n",
      "Model saved to sklearn_model_fold_9.joblib\n",
      "\n",
      "Fold 10/20\n",
      "Training data shape: (760, 228) (limited for CPU)\n",
      "Validation data shape: (40, 228)\n",
      "Training scikit-learn MLP model...\n",
      "Iteration 1, loss = 0.60561373\n",
      "Validation score: 0.828947\n",
      "Iteration 2, loss = 0.44158490\n",
      "Validation score: 0.802632\n",
      "Iteration 3, loss = 0.38030576\n",
      "Validation score: 0.789474\n",
      "Iteration 4, loss = 0.34493454\n",
      "Validation score: 0.736842\n",
      "Iteration 5, loss = 0.30897512\n",
      "Validation score: 0.802632\n",
      "Iteration 6, loss = 0.29256732\n",
      "Validation score: 0.776316\n",
      "Iteration 7, loss = 0.26312769\n",
      "Validation score: 0.815789\n",
      "Validation score did not improve more than tol=0.000100 for 5 consecutive epochs. Stopping.\n",
      "Training completed in 0.41 seconds\n",
      "\n",
      "Fold 10 Results:\n",
      "Fold: 10.0000\n",
      "K: 20.0000\n",
      "Test_Accuracy: 0.7250\n",
      "Train_Accuracy: 0.8039\n",
      "Precision: 0.7222\n",
      "Recall: 0.6842\n",
      "AUC_ROC: 0.7168\n",
      "F1_Score: 0.7027\n",
      "Training_Loss: 0.2631\n",
      "Time_Taken: 0.4076\n",
      "Epochs_Run: 7.0000\n",
      "Specificity: 0.7619\n",
      "MCC: 0.4478\n",
      "G_Mean: 0.7220\n",
      "Youdens_J: 0.4461\n",
      "Balanced_Accuracy: 0.7231\n",
      "Cohens_Kappa: 0.4472\n",
      "Model saved to sklearn_model_fold_10.joblib\n",
      "\n",
      "Fold 11/20\n",
      "Training data shape: (760, 228) (limited for CPU)\n",
      "Validation data shape: (40, 228)\n",
      "Training scikit-learn MLP model...\n",
      "Iteration 1, loss = 0.60178483\n",
      "Validation score: 0.815789\n",
      "Iteration 2, loss = 0.44145411\n",
      "Validation score: 0.815789\n",
      "Iteration 3, loss = 0.37826004\n",
      "Validation score: 0.789474\n",
      "Iteration 4, loss = 0.33462962\n",
      "Validation score: 0.842105\n",
      "Iteration 5, loss = 0.30432111\n",
      "Validation score: 0.881579\n",
      "Iteration 6, loss = 0.29032151\n",
      "Validation score: 0.828947\n",
      "Iteration 7, loss = 0.26608387\n",
      "Validation score: 0.815789\n",
      "Iteration 8, loss = 0.25585481\n",
      "Validation score: 0.789474\n",
      "Iteration 9, loss = 0.22616122\n",
      "Validation score: 0.789474\n",
      "Iteration 10, loss = 0.20986714\n",
      "Validation score: 0.842105\n",
      "Iteration 11, loss = 0.19614612\n",
      "Validation score: 0.815789\n",
      "Validation score did not improve more than tol=0.000100 for 5 consecutive epochs. Stopping.\n",
      "Training completed in 1.45 seconds\n",
      "\n",
      "Fold 11 Results:\n",
      "Fold: 11.0000\n",
      "K: 20.0000\n",
      "Test_Accuracy: 0.7000\n",
      "Train_Accuracy: 0.8842\n",
      "Precision: 0.6667\n",
      "Recall: 0.6667\n",
      "AUC_ROC: 0.7071\n",
      "F1_Score: 0.6667\n",
      "Training_Loss: 0.1961\n",
      "Time_Taken: 1.4473\n",
      "Epochs_Run: 11.0000\n",
      "Specificity: 0.7273\n",
      "MCC: 0.3939\n",
      "G_Mean: 0.6963\n",
      "Youdens_J: 0.3939\n",
      "Balanced_Accuracy: 0.6970\n",
      "Cohens_Kappa: 0.3939\n",
      "Model saved to sklearn_model_fold_11.joblib\n",
      "\n",
      "Fold 12/20\n",
      "Training data shape: (760, 228) (limited for CPU)\n",
      "Validation data shape: (40, 228)\n",
      "Training scikit-learn MLP model...\n",
      "Iteration 1, loss = 0.60471995\n",
      "Validation score: 0.763158\n",
      "Iteration 2, loss = 0.44822433\n",
      "Validation score: 0.802632\n",
      "Iteration 3, loss = 0.38264316\n",
      "Validation score: 0.750000\n",
      "Iteration 4, loss = 0.33845382\n",
      "Validation score: 0.750000\n",
      "Iteration 5, loss = 0.31048398\n",
      "Validation score: 0.750000\n",
      "Iteration 6, loss = 0.29180854\n",
      "Validation score: 0.736842\n",
      "Iteration 7, loss = 0.26094142\n",
      "Validation score: 0.763158\n",
      "Iteration 8, loss = 0.24878485\n",
      "Validation score: 0.736842\n",
      "Validation score did not improve more than tol=0.000100 for 5 consecutive epochs. Stopping.\n",
      "Training completed in 0.34 seconds\n",
      "\n",
      "Fold 12 Results:\n",
      "Fold: 12.0000\n",
      "K: 20.0000\n",
      "Test_Accuracy: 0.7500\n",
      "Train_Accuracy: 0.8395\n",
      "Precision: 0.8333\n",
      "Recall: 0.6818\n",
      "AUC_ROC: 0.8131\n",
      "F1_Score: 0.7500\n",
      "Training_Loss: 0.2488\n",
      "Time_Taken: 0.3409\n",
      "Epochs_Run: 8.0000\n",
      "Specificity: 0.8333\n",
      "MCC: 0.5152\n",
      "G_Mean: 0.7538\n",
      "Youdens_J: 0.5152\n",
      "Balanced_Accuracy: 0.7576\n",
      "Cohens_Kappa: 0.5050\n",
      "Model saved to sklearn_model_fold_12.joblib\n",
      "\n",
      "Fold 13/20\n",
      "Training data shape: (760, 228) (limited for CPU)\n",
      "Validation data shape: (40, 228)\n",
      "Training scikit-learn MLP model...\n",
      "Iteration 1, loss = 0.60802208\n",
      "Validation score: 0.750000\n",
      "Iteration 2, loss = 0.44084412\n",
      "Validation score: 0.802632\n",
      "Iteration 3, loss = 0.37785771\n",
      "Validation score: 0.789474\n",
      "Iteration 4, loss = 0.33544578\n",
      "Validation score: 0.750000\n",
      "Iteration 5, loss = 0.29813423\n",
      "Validation score: 0.736842\n",
      "Iteration 6, loss = 0.27718346\n",
      "Validation score: 0.776316\n",
      "Iteration 7, loss = 0.26475540\n",
      "Validation score: 0.763158\n",
      "Iteration 8, loss = 0.24586386\n",
      "Validation score: 0.750000\n",
      "Validation score did not improve more than tol=0.000100 for 5 consecutive epochs. Stopping.\n",
      "Training completed in 0.34 seconds\n",
      "\n",
      "Fold 13 Results:\n",
      "Fold: 13.0000\n",
      "K: 20.0000\n",
      "Test_Accuracy: 0.8000\n",
      "Train_Accuracy: 0.8329\n",
      "Precision: 0.7778\n",
      "Recall: 0.7778\n",
      "AUC_ROC: 0.8990\n",
      "F1_Score: 0.7778\n",
      "Training_Loss: 0.2459\n",
      "Time_Taken: 0.3377\n",
      "Epochs_Run: 8.0000\n",
      "Specificity: 0.8182\n",
      "MCC: 0.5960\n",
      "G_Mean: 0.7977\n",
      "Youdens_J: 0.5960\n",
      "Balanced_Accuracy: 0.7980\n",
      "Cohens_Kappa: 0.5960\n",
      "Model saved to sklearn_model_fold_13.joblib\n",
      "\n",
      "Fold 14/20\n",
      "Training data shape: (760, 228) (limited for CPU)\n",
      "Validation data shape: (40, 228)\n",
      "Training scikit-learn MLP model...\n",
      "Iteration 1, loss = 0.61750732\n",
      "Validation score: 0.763158\n",
      "Iteration 2, loss = 0.44594497\n",
      "Validation score: 0.723684\n",
      "Iteration 3, loss = 0.37251850\n",
      "Validation score: 0.710526\n",
      "Iteration 4, loss = 0.33728506\n",
      "Validation score: 0.684211\n",
      "Iteration 5, loss = 0.29896518\n",
      "Validation score: 0.697368\n",
      "Iteration 6, loss = 0.28529323\n",
      "Validation score: 0.710526\n",
      "Iteration 7, loss = 0.25509068\n",
      "Validation score: 0.723684\n",
      "Validation score did not improve more than tol=0.000100 for 5 consecutive epochs. Stopping.\n",
      "Training completed in 0.63 seconds\n",
      "\n",
      "Fold 14 Results:\n",
      "Fold: 14.0000\n",
      "K: 20.0000\n",
      "Test_Accuracy: 0.7000\n",
      "Train_Accuracy: 0.7829\n",
      "Precision: 0.5909\n",
      "Recall: 0.8125\n",
      "AUC_ROC: 0.7734\n",
      "F1_Score: 0.6842\n",
      "Training_Loss: 0.2551\n",
      "Time_Taken: 0.6342\n",
      "Epochs_Run: 7.0000\n",
      "Specificity: 0.6250\n",
      "MCC: 0.4308\n",
      "G_Mean: 0.7126\n",
      "Youdens_J: 0.4375\n",
      "Balanced_Accuracy: 0.7188\n",
      "Cohens_Kappa: 0.4118\n",
      "Model saved to sklearn_model_fold_14.joblib\n",
      "\n",
      "Fold 15/20\n",
      "Training data shape: (760, 228) (limited for CPU)\n",
      "Validation data shape: (40, 228)\n",
      "Training scikit-learn MLP model...\n",
      "Iteration 1, loss = 0.60026182\n",
      "Validation score: 0.710526\n",
      "Iteration 2, loss = 0.44194251\n",
      "Validation score: 0.710526\n",
      "Iteration 3, loss = 0.37565771\n",
      "Validation score: 0.789474\n",
      "Iteration 4, loss = 0.32939572\n",
      "Validation score: 0.750000\n",
      "Iteration 5, loss = 0.30212752\n",
      "Validation score: 0.776316\n",
      "Iteration 6, loss = 0.26964578\n",
      "Validation score: 0.750000\n",
      "Iteration 7, loss = 0.24587411\n",
      "Validation score: 0.789474\n",
      "Iteration 8, loss = 0.22866833\n",
      "Validation score: 0.776316\n",
      "Iteration 9, loss = 0.21689359\n",
      "Validation score: 0.723684\n",
      "Validation score did not improve more than tol=0.000100 for 5 consecutive epochs. Stopping.\n",
      "Training completed in 0.46 seconds\n",
      "\n",
      "Fold 15 Results:\n",
      "Fold: 15.0000\n",
      "K: 20.0000\n",
      "Test_Accuracy: 0.8500\n",
      "Train_Accuracy: 0.8618\n",
      "Precision: 0.8800\n",
      "Recall: 0.8800\n",
      "AUC_ROC: 0.8880\n",
      "F1_Score: 0.8800\n",
      "Training_Loss: 0.2169\n",
      "Time_Taken: 0.4552\n",
      "Epochs_Run: 9.0000\n",
      "Specificity: 0.8000\n",
      "MCC: 0.6800\n",
      "G_Mean: 0.8390\n",
      "Youdens_J: 0.6800\n",
      "Balanced_Accuracy: 0.8400\n",
      "Cohens_Kappa: 0.6800\n",
      "Model saved to sklearn_model_fold_15.joblib\n",
      "\n",
      "Fold 16/20\n",
      "Training data shape: (760, 228) (limited for CPU)\n",
      "Validation data shape: (40, 228)\n",
      "Training scikit-learn MLP model...\n",
      "Iteration 1, loss = 0.62214043\n",
      "Validation score: 0.789474\n",
      "Iteration 2, loss = 0.45249724\n",
      "Validation score: 0.802632\n",
      "Iteration 3, loss = 0.38940642\n",
      "Validation score: 0.842105\n",
      "Iteration 4, loss = 0.35065910\n",
      "Validation score: 0.855263\n",
      "Iteration 5, loss = 0.31836527\n",
      "Validation score: 0.842105\n",
      "Iteration 6, loss = 0.29410787\n",
      "Validation score: 0.815789\n",
      "Iteration 7, loss = 0.27339333\n",
      "Validation score: 0.842105\n",
      "Iteration 8, loss = 0.25947285\n",
      "Validation score: 0.855263\n",
      "Iteration 9, loss = 0.23368553\n",
      "Validation score: 0.881579\n",
      "Iteration 10, loss = 0.20769373\n",
      "Validation score: 0.828947\n",
      "Iteration 11, loss = 0.20298198\n",
      "Validation score: 0.855263\n",
      "Iteration 12, loss = 0.18463605\n",
      "Validation score: 0.868421\n",
      "Iteration 13, loss = 0.17611285\n",
      "Validation score: 0.815789\n",
      "Iteration 14, loss = 0.16274256\n",
      "Validation score: 0.868421\n",
      "Iteration 15, loss = 0.16052163\n",
      "Validation score: 0.855263\n",
      "Validation score did not improve more than tol=0.000100 for 5 consecutive epochs. Stopping.\n",
      "Training completed in 0.70 seconds\n",
      "\n",
      "Fold 16 Results:\n",
      "Fold: 16.0000\n",
      "K: 20.0000\n",
      "Test_Accuracy: 0.7250\n",
      "Train_Accuracy: 0.9171\n",
      "Precision: 0.6250\n",
      "Recall: 0.6667\n",
      "AUC_ROC: 0.8080\n",
      "F1_Score: 0.6452\n",
      "Training_Loss: 0.1605\n",
      "Time_Taken: 0.7013\n",
      "Epochs_Run: 15.0000\n",
      "Specificity: 0.7600\n",
      "MCC: 0.4216\n",
      "G_Mean: 0.7118\n",
      "Youdens_J: 0.4267\n",
      "Balanced_Accuracy: 0.7133\n",
      "Cohens_Kappa: 0.4211\n",
      "Model saved to sklearn_model_fold_16.joblib\n",
      "\n",
      "Fold 17/20\n",
      "Training data shape: (760, 228) (limited for CPU)\n",
      "Validation data shape: (40, 228)\n",
      "Training scikit-learn MLP model...\n",
      "Iteration 1, loss = 0.60410752\n",
      "Validation score: 0.750000\n",
      "Iteration 2, loss = 0.43622070\n",
      "Validation score: 0.750000\n",
      "Iteration 3, loss = 0.38228193\n",
      "Validation score: 0.763158\n",
      "Iteration 4, loss = 0.33636114\n",
      "Validation score: 0.723684\n",
      "Iteration 5, loss = 0.31654936\n",
      "Validation score: 0.736842\n",
      "Iteration 6, loss = 0.28180325\n",
      "Validation score: 0.750000\n",
      "Iteration 7, loss = 0.25582508\n",
      "Validation score: 0.776316\n",
      "Iteration 8, loss = 0.24236781\n",
      "Validation score: 0.750000\n",
      "Iteration 9, loss = 0.23516582\n",
      "Validation score: 0.736842\n",
      "Iteration 10, loss = 0.21395522\n",
      "Validation score: 0.776316\n",
      "Iteration 11, loss = 0.18604152\n",
      "Validation score: 0.736842\n",
      "Iteration 12, loss = 0.18090962\n",
      "Validation score: 0.750000\n",
      "Iteration 13, loss = 0.18342156\n",
      "Validation score: 0.750000\n",
      "Validation score did not improve more than tol=0.000100 for 5 consecutive epochs. Stopping.\n",
      "Training completed in 0.54 seconds\n",
      "\n",
      "Fold 17 Results:\n",
      "Fold: 17.0000\n",
      "K: 20.0000\n",
      "Test_Accuracy: 0.7750\n",
      "Train_Accuracy: 0.8934\n",
      "Precision: 0.7273\n",
      "Recall: 0.8421\n",
      "AUC_ROC: 0.8421\n",
      "F1_Score: 0.7805\n",
      "Training_Loss: 0.1834\n",
      "Time_Taken: 0.5448\n",
      "Epochs_Run: 13.0000\n",
      "Specificity: 0.7143\n",
      "MCC: 0.5585\n",
      "G_Mean: 0.7756\n",
      "Youdens_J: 0.5564\n",
      "Balanced_Accuracy: 0.7782\n",
      "Cohens_Kappa: 0.5522\n",
      "Model saved to sklearn_model_fold_17.joblib\n",
      "\n",
      "Fold 18/20\n",
      "Training data shape: (760, 228) (limited for CPU)\n",
      "Validation data shape: (40, 228)\n",
      "Training scikit-learn MLP model...\n",
      "Iteration 1, loss = 0.60144002\n",
      "Validation score: 0.723684\n",
      "Iteration 2, loss = 0.43799226\n",
      "Validation score: 0.736842\n",
      "Iteration 3, loss = 0.37297322\n",
      "Validation score: 0.710526\n",
      "Iteration 4, loss = 0.33854807\n",
      "Validation score: 0.710526\n",
      "Iteration 5, loss = 0.30399362\n",
      "Validation score: 0.710526\n",
      "Iteration 6, loss = 0.29561706\n",
      "Validation score: 0.750000\n",
      "Iteration 7, loss = 0.27779994\n",
      "Validation score: 0.723684\n",
      "Iteration 8, loss = 0.24825875\n",
      "Validation score: 0.776316\n",
      "Iteration 9, loss = 0.23171968\n",
      "Validation score: 0.723684\n",
      "Iteration 10, loss = 0.21086413\n",
      "Validation score: 0.736842\n",
      "Iteration 11, loss = 0.19945313\n",
      "Validation score: 0.736842\n",
      "Iteration 12, loss = 0.20559827\n",
      "Validation score: 0.736842\n",
      "Iteration 13, loss = 0.18875887\n",
      "Validation score: 0.802632\n",
      "Iteration 14, loss = 0.16532457\n",
      "Validation score: 0.736842\n",
      "Iteration 15, loss = 0.16700605\n",
      "Validation score: 0.763158\n",
      "Iteration 16, loss = 0.15703676\n",
      "Validation score: 0.802632\n",
      "Iteration 17, loss = 0.17253160\n",
      "Validation score: 0.815789\n",
      "Iteration 18, loss = 0.13838072\n",
      "Validation score: 0.802632\n",
      "Iteration 19, loss = 0.13121993\n",
      "Validation score: 0.789474\n",
      "Iteration 20, loss = 0.12669099\n",
      "Validation score: 0.789474\n",
      "Iteration 21, loss = 0.11377047\n",
      "Validation score: 0.750000\n",
      "Iteration 22, loss = 0.10669423\n",
      "Validation score: 0.842105\n",
      "Iteration 23, loss = 0.09717934\n",
      "Validation score: 0.789474\n",
      "Iteration 24, loss = 0.14234356\n",
      "Validation score: 0.828947\n",
      "Iteration 25, loss = 0.12898466\n",
      "Validation score: 0.736842\n",
      "Iteration 26, loss = 0.15842752\n",
      "Validation score: 0.828947\n",
      "Iteration 27, loss = 0.13922979\n",
      "Validation score: 0.842105\n",
      "Iteration 28, loss = 0.10128700\n",
      "Validation score: 0.750000\n",
      "Validation score did not improve more than tol=0.000100 for 5 consecutive epochs. Stopping.\n",
      "Training completed in 1.56 seconds\n",
      "\n",
      "Fold 18 Results:\n",
      "Fold: 18.0000\n",
      "K: 20.0000\n",
      "Test_Accuracy: 0.7750\n",
      "Train_Accuracy: 0.9592\n",
      "Precision: 0.8235\n",
      "Recall: 0.7000\n",
      "AUC_ROC: 0.8250\n",
      "F1_Score: 0.7568\n",
      "Training_Loss: 0.1013\n",
      "Time_Taken: 1.5639\n",
      "Epochs_Run: 28.0000\n",
      "Specificity: 0.8500\n",
      "MCC: 0.5563\n",
      "G_Mean: 0.7714\n",
      "Youdens_J: 0.5500\n",
      "Balanced_Accuracy: 0.7750\n",
      "Cohens_Kappa: 0.5500\n",
      "Model saved to sklearn_model_fold_18.joblib\n",
      "\n",
      "Fold 19/20\n",
      "Training data shape: (760, 228) (limited for CPU)\n",
      "Validation data shape: (40, 228)\n",
      "Training scikit-learn MLP model...\n",
      "Iteration 1, loss = 0.60721919\n",
      "Validation score: 0.710526\n",
      "Iteration 2, loss = 0.43920000\n",
      "Validation score: 0.710526\n",
      "Iteration 3, loss = 0.38329856\n",
      "Validation score: 0.684211\n",
      "Iteration 4, loss = 0.34627228\n",
      "Validation score: 0.671053\n",
      "Iteration 5, loss = 0.30538459\n",
      "Validation score: 0.710526\n",
      "Iteration 6, loss = 0.28693940\n",
      "Validation score: 0.750000\n",
      "Iteration 7, loss = 0.26520849\n",
      "Validation score: 0.710526\n",
      "Iteration 8, loss = 0.24077908\n",
      "Validation score: 0.750000\n",
      "Iteration 9, loss = 0.22320606\n",
      "Validation score: 0.697368\n",
      "Iteration 10, loss = 0.20974564\n",
      "Validation score: 0.776316\n",
      "Iteration 11, loss = 0.20205799\n",
      "Validation score: 0.710526\n",
      "Iteration 12, loss = 0.18155781\n",
      "Validation score: 0.736842\n",
      "Iteration 13, loss = 0.17313519\n",
      "Validation score: 0.776316\n",
      "Iteration 14, loss = 0.16300099\n",
      "Validation score: 0.736842\n",
      "Iteration 15, loss = 0.16099595\n",
      "Validation score: 0.750000\n",
      "Iteration 16, loss = 0.14768269\n",
      "Validation score: 0.763158\n",
      "Validation score did not improve more than tol=0.000100 for 5 consecutive epochs. Stopping.\n",
      "Training completed in 0.72 seconds\n",
      "\n",
      "Fold 19 Results:\n",
      "Fold: 19.0000\n",
      "K: 20.0000\n",
      "Test_Accuracy: 0.6250\n",
      "Train_Accuracy: 0.9197\n",
      "Precision: 0.7917\n",
      "Recall: 0.6552\n",
      "AUC_ROC: 0.7429\n",
      "F1_Score: 0.7170\n",
      "Training_Loss: 0.1477\n",
      "Time_Taken: 0.7248\n",
      "Epochs_Run: 16.0000\n",
      "Specificity: 0.5455\n",
      "MCC: 0.1829\n",
      "G_Mean: 0.5978\n",
      "Youdens_J: 0.2006\n",
      "Balanced_Accuracy: 0.6003\n",
      "Cohens_Kappa: 0.1758\n",
      "Model saved to sklearn_model_fold_19.joblib\n",
      "\n",
      "Fold 20/20\n",
      "Training data shape: (760, 228) (limited for CPU)\n",
      "Validation data shape: (40, 228)\n",
      "Training scikit-learn MLP model...\n",
      "Iteration 1, loss = 0.60902700\n",
      "Validation score: 0.763158\n",
      "Iteration 2, loss = 0.44520136\n",
      "Validation score: 0.815789\n",
      "Iteration 3, loss = 0.37811558\n",
      "Validation score: 0.776316\n",
      "Iteration 4, loss = 0.34065804\n",
      "Validation score: 0.802632\n",
      "Iteration 5, loss = 0.30492205\n",
      "Validation score: 0.776316\n",
      "Iteration 6, loss = 0.27570005\n",
      "Validation score: 0.750000\n",
      "Iteration 7, loss = 0.25818120\n",
      "Validation score: 0.776316\n",
      "Iteration 8, loss = 0.23294146\n",
      "Validation score: 0.776316\n",
      "Validation score did not improve more than tol=0.000100 for 5 consecutive epochs. Stopping.\n",
      "Training completed in 0.36 seconds\n",
      "\n",
      "Fold 20 Results:\n",
      "Fold: 20.0000\n",
      "K: 20.0000\n",
      "Test_Accuracy: 0.8500\n",
      "Train_Accuracy: 0.8276\n",
      "Precision: 0.8333\n",
      "Recall: 0.8333\n",
      "AUC_ROC: 0.8510\n",
      "F1_Score: 0.8333\n",
      "Training_Loss: 0.2329\n",
      "Time_Taken: 0.3587\n",
      "Epochs_Run: 8.0000\n",
      "Specificity: 0.8636\n",
      "MCC: 0.6970\n",
      "G_Mean: 0.8483\n",
      "Youdens_J: 0.6970\n",
      "Balanced_Accuracy: 0.8485\n",
      "Cohens_Kappa: 0.6970\n",
      "Model saved to sklearn_model_fold_20.joblib\n",
      "\n",
      "=== Summary Statistics ===\n",
      "\n",
      "K=5 Cross-Validation Results:\n",
      "Average Test Accuracy: 0.7463 ± 0.0215\n",
      "Average F1 Score: 0.7474 ± 0.0248\n",
      "Average AUC-ROC: 0.8081 ± 0.0381\n",
      "Average Training Time: 0.38 seconds\n",
      "Average Epochs Run: 11.4\n",
      "\n",
      "K=10 Cross-Validation Results:\n",
      "Average Test Accuracy: 0.7687 ± 0.0476\n",
      "Average F1 Score: 0.7794 ± 0.0436\n",
      "Average AUC-ROC: 0.8220 ± 0.0493\n",
      "Average Training Time: 0.52 seconds\n",
      "Average Epochs Run: 10.2\n",
      "\n",
      "K=15 Cross-Validation Results:\n",
      "Average Test Accuracy: 0.7525 ± 0.0449\n",
      "Average F1 Score: 0.7510 ± 0.0449\n",
      "Average AUC-ROC: 0.8185 ± 0.0468\n",
      "Average Training Time: 0.58 seconds\n",
      "Average Epochs Run: 12.4\n",
      "\n",
      "K=20 Cross-Validation Results:\n",
      "Average Test Accuracy: 0.7500 ± 0.0574\n",
      "Average F1 Score: 0.7433 ± 0.0645\n",
      "Average AUC-ROC: 0.8112 ± 0.0566\n",
      "Average Training Time: 0.61 seconds\n",
      "Average Epochs Run: 11.1\n",
      "\n",
      "Total execution time: 31.18 seconds\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import (\n",
    "    f1_score, confusion_matrix, roc_auc_score,\n",
    "    matthews_corrcoef, balanced_accuracy_score, cohen_kappa_score\n",
    ")\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime\n",
    "import os\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Notice we don't import TensorFlow at all\n",
    "\n",
    "print(\"Using scikit-learn for CPU compatibility\")\n",
    "\n",
    "def preprocess_data(X):\n",
    "    # Downsample large images for CPU efficiency\n",
    "    if X.shape[1] > 112 or X.shape[2] > 112:  # More aggressive downsampling\n",
    "        from skimage.transform import resize\n",
    "        factor = max(1, min(X.shape[1], X.shape[2]) // 112)\n",
    "        new_shape = (X.shape[0], X.shape[1]//factor, X.shape[2]//factor)\n",
    "        X_small = np.zeros(new_shape + (X.shape[3],) if len(X.shape) > 3 else new_shape)\n",
    "        \n",
    "        for i in range(X.shape[0]):\n",
    "            if len(X.shape) > 3:\n",
    "                X_small[i] = resize(X[i], (new_shape[1], new_shape[2], X.shape[3]))\n",
    "            else:\n",
    "                X_small[i] = resize(X[i], (new_shape[1], new_shape[2]))\n",
    "        \n",
    "        print(f\"Downsampled data from {X.shape[1]}x{X.shape[2]} to {X_small.shape[1]}x{X_small.shape[2]}\")\n",
    "        X = X_small\n",
    "    \n",
    "    # Add channel dimension if it doesn't exist\n",
    "    if len(X.shape) == 3:\n",
    "        X = np.expand_dims(X, axis=-1)\n",
    "        X = np.repeat(X, 3, axis=-1)\n",
    "    return X\n",
    "\n",
    "def extract_features(X):\n",
    "    \"\"\"Extract simple features from images for use in sklearn models\"\"\"\n",
    "    # Reshape from (N, H, W, C) to (N, H*W*C)\n",
    "    # or simply flatten each image\n",
    "    n_samples = X.shape[0]\n",
    "    \n",
    "    # Simple feature extraction\n",
    "    features = []\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        img = X[i]\n",
    "        \n",
    "        # Average pooling to reduce dimensionality\n",
    "        h_pools = img.shape[0] // 8\n",
    "        w_pools = img.shape[1] // 8\n",
    "        pooled = np.zeros((8, 8, img.shape[2]))\n",
    "        \n",
    "        for h in range(8):\n",
    "            for w in range(8):\n",
    "                h_start = h * h_pools\n",
    "                h_end = (h + 1) * h_pools\n",
    "                w_start = w * w_pools\n",
    "                w_end = (w + 1) * w_pools\n",
    "                pooled[h, w] = np.mean(img[h_start:h_end, w_start:w_end], axis=(0, 1))\n",
    "        \n",
    "        # Extract more features\n",
    "        # 1. Mean of each channel\n",
    "        channel_means = np.mean(img, axis=(0, 1))\n",
    "        \n",
    "        # 2. Standard deviation of each channel\n",
    "        channel_stds = np.std(img, axis=(0, 1))\n",
    "        \n",
    "        # 3. Histogram features (simplified)\n",
    "        hist_features = []\n",
    "        for c in range(img.shape[2]):\n",
    "            hist, _ = np.histogram(img[:, :, c], bins=10, range=(0, 1))\n",
    "            hist_features.extend(hist / np.sum(hist))  # Normalize\n",
    "        \n",
    "        # Combine features\n",
    "        feature_vector = np.concatenate([\n",
    "            pooled.flatten(),\n",
    "            channel_means,\n",
    "            channel_stds,\n",
    "            np.array(hist_features)\n",
    "        ])\n",
    "        \n",
    "        features.append(feature_vector)\n",
    "    \n",
    "    return np.array(features)\n",
    "\n",
    "def perform_kfold_cv(X, y, k_folds=[5, 10], max_iter=100):\n",
    "    \"\"\"scikit-learn based k-fold cross-validation with multiple k values\"\"\"\n",
    "    # Preprocess and downsample\n",
    "    X = preprocess_data(X)\n",
    "    print(f\"Data shape after preprocessing: {X.shape}\")\n",
    "    \n",
    "    # Extract features for scikit-learn model\n",
    "    print(\"Extracting features...\")\n",
    "    X_features = extract_features(X)\n",
    "    print(f\"Feature shape: {X_features.shape}\")\n",
    "    \n",
    "    # Setup results storage\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    results_file = f'outputresults/mobilesam_results_{timestamp}111.csv'\n",
    "    all_results = []\n",
    "    \n",
    "    # Run for each k-fold setting\n",
    "    for k in k_folds:\n",
    "        print(f\"\\n=== Starting {k}-fold cross-validation ===\")\n",
    "        \n",
    "        # Setup k-fold for this k value\n",
    "        kfold = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "        \n",
    "        for fold, (train_idx, val_idx) in enumerate(kfold.split(X_features)):\n",
    "            print(f'\\nFold {fold + 1}/{k}')\n",
    "            \n",
    "            # Limit training set size for CPU efficiency\n",
    "            max_train_size = min(len(train_idx), 800)\n",
    "            max_val_size = min(len(val_idx), 200)\n",
    "            train_idx = train_idx[:max_train_size]\n",
    "            val_idx = val_idx[:max_val_size]\n",
    "            \n",
    "            # Get data for this fold\n",
    "            X_train_fold = X_features[train_idx].astype('float32')\n",
    "            X_val_fold = X_features[val_idx].astype('float32')\n",
    "            y_train_fold = y[train_idx].astype('float32')\n",
    "            y_val_fold = y[val_idx].astype('float32')\n",
    "            \n",
    "            print(f\"Training data shape: {X_train_fold.shape} (limited for CPU)\")\n",
    "            print(f\"Validation data shape: {X_val_fold.shape}\")\n",
    "            \n",
    "            # Scale features\n",
    "            scaler = StandardScaler()\n",
    "            X_train_scaled = scaler.fit_transform(X_train_fold)\n",
    "            X_val_scaled = scaler.transform(X_val_fold)\n",
    "            \n",
    "            # Create model - scikit-learn MLP classifier\n",
    "            model = MLPClassifier(\n",
    "                hidden_layer_sizes=(128, 64),  # Two hidden layers\n",
    "                activation='relu',\n",
    "                solver='adam',\n",
    "                alpha=0.0001,  # L2 penalty\n",
    "                batch_size=16,\n",
    "                learning_rate='adaptive',\n",
    "                learning_rate_init=0.001,\n",
    "                max_iter=max_iter,  # Number of epochs\n",
    "                early_stopping=True,  # Use early stopping\n",
    "                validation_fraction=0.1,  # 10% of training data for early stopping\n",
    "                n_iter_no_change=5,  # Patience for early stopping\n",
    "                verbose=True,\n",
    "                random_state=42\n",
    "            )\n",
    "            \n",
    "            print(f\"Training scikit-learn MLP model...\")\n",
    "            \n",
    "            # Train the model\n",
    "            start_time = time.time()\n",
    "            model.fit(X_train_scaled, y_train_fold.ravel())\n",
    "            training_time = time.time() - start_time\n",
    "            \n",
    "            print(f\"Training completed in {training_time:.2f} seconds\")\n",
    "            \n",
    "            # Get predictions\n",
    "            val_pred_proba = model.predict_proba(X_val_scaled)[:, 1]  # Get probability of class 1\n",
    "            val_pred_binary = model.predict(X_val_scaled)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            tn, fp, fn, tp = confusion_matrix(y_val_fold, val_pred_binary).ravel()\n",
    "            \n",
    "            # Basic metrics\n",
    "            accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "            precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "            recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "            f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "            \n",
    "            # Additional metrics\n",
    "            specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "            mcc = matthews_corrcoef(y_val_fold, val_pred_binary)\n",
    "            \n",
    "            # Calculate log loss\n",
    "            y_pred_clipped = np.clip(val_pred_proba, 1e-15, 1 - 1e-15)\n",
    "            log_loss_val = -np.mean(y_val_fold * np.log(y_pred_clipped) + \n",
    "                            (1 - y_val_fold) * np.log(1 - y_pred_clipped))\n",
    "            \n",
    "            # Other metrics\n",
    "            g_mean = np.sqrt(recall * specificity) if (recall * specificity) > 0 else 0\n",
    "            youdens_j = recall + specificity - 1\n",
    "            balanced_acc = balanced_accuracy_score(y_val_fold, val_pred_binary)\n",
    "            kappa = cohen_kappa_score(y_val_fold, val_pred_binary)\n",
    "            \n",
    "            # Calculate AUC if possible\n",
    "            try:\n",
    "                auc_roc = roc_auc_score(y_val_fold, val_pred_proba)\n",
    "            except:\n",
    "                auc_roc = 0.5\n",
    "            \n",
    "            # Store metrics\n",
    "            fold_metrics = {\n",
    "                'Fold': fold + 1,\n",
    "                'K': k,\n",
    "                'Test_Accuracy': accuracy,\n",
    "                'Train_Accuracy': model.score(X_train_scaled, y_train_fold),\n",
    "                'Precision': precision,\n",
    "                'Recall': recall,\n",
    "                'AUC_ROC': auc_roc,\n",
    "                'F1_Score': f1,\n",
    "                'Training_Loss': model.loss_,\n",
    "                'Testing_Loss': log_loss_val,\n",
    "                'Time_Taken': training_time,\n",
    "                'Epochs_Run': model.n_iter_,\n",
    "                'TP': tp,\n",
    "                'TN': tn,\n",
    "                'FP': fp,\n",
    "                'FN': fn,\n",
    "                'Specificity': specificity,\n",
    "                'MCC': mcc,\n",
    "                'Log_Loss': log_loss_val,\n",
    "                'G_Mean': g_mean,\n",
    "                'Youdens_J': youdens_j,\n",
    "                'Balanced_Accuracy': balanced_acc,\n",
    "                'Cohens_Kappa': kappa\n",
    "            }\n",
    "            \n",
    "            all_results.append(fold_metrics)\n",
    "            \n",
    "            # Print current fold results\n",
    "            print(f\"\\nFold {fold + 1} Results:\")\n",
    "            for metric, value in fold_metrics.items():\n",
    "                if isinstance(value, (int, float)):\n",
    "                    print(f\"{metric}: {value:.4f}\")\n",
    "            \n",
    "            # Try to save the model\n",
    "            try:\n",
    "                from joblib import dump\n",
    "                dump(model, f'sklearn_model_fold_{fold+1}.joblib')\n",
    "                print(f\"Model saved to sklearn_model_fold_{fold+1}.joblib\")\n",
    "            except:\n",
    "                print(\"Could not save model\")\n",
    "        \n",
    "    # Save all results\n",
    "    results_df = pd.DataFrame(all_results)\n",
    "    results_df.to_csv(results_file, index=False)\n",
    "    \n",
    "    # Calculate and print summary statistics for each k value\n",
    "    print(\"\\n=== Summary Statistics ===\")\n",
    "    for k in k_folds:\n",
    "        k_results = results_df[results_df['K'] == k]\n",
    "        print(f\"\\nK={k} Cross-Validation Results:\")\n",
    "        print(f\"Average Test Accuracy: {k_results['Test_Accuracy'].mean():.4f} ± {k_results['Test_Accuracy'].std():.4f}\")\n",
    "        print(f\"Average F1 Score: {k_results['F1_Score'].mean():.4f} ± {k_results['F1_Score'].std():.4f}\")\n",
    "        print(f\"Average AUC-ROC: {k_results['AUC_ROC'].mean():.4f} ± {k_results['AUC_ROC'].std():.4f}\")\n",
    "        print(f\"Average Training Time: {k_results['Time_Taken'].mean():.2f} seconds\")\n",
    "        print(f\"Average Epochs Run: {k_results['Epochs_Run'].mean():.1f}\")\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Import time for benchmarking\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Run with requested fold parameters\n",
    "    results = perform_kfold_cv(\n",
    "        X=X_train_full, \n",
    "        y=y_train_full,\n",
    "        k_folds=[5, 10,15,20],  # Run both 5-fold and 10-fold CV\n",
    "        max_iter=100      # Number of epochs/iterations\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nTotal execution time: {time.time() - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8790aa-c42a-487f-a039-4392b6c12658",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
